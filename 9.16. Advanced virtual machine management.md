## 9.16. Advanced virtual machine management




### 9.16.1. Working with resource quotas for virtual machines




Create and manage resource quotas for virtual machines.

#### 9.16.1.1. Setting resource quota limits for virtual machines




Resource quotas that only use requests automatically work with virtual machines (VMs). If your resource quota uses limits, you must manually set resource limits on VMs. Resource limits must be at least 100 MiB larger than resource requests.

 **Procedure** 

1. Set limits for a VM by editing the `    VirtualMachine` manifest. For example:
    
    
    ```
    apiVersion: kubevirt.io/v1    kind: VirtualMachine    metadata:      name: with-limits    spec:      running: false      template:        spec:          domain:    # ...            resources:              requests:                memory: 128Mi              limits:                memory: 256Mi<span id="CO19-1"><!--Empty--></span><span class="callout">1</span>
    ```
    
    
1. Save the `    VirtualMachine` manifest.


#### 9.16.1.2. Additional resources




-  [Resource quotas per project](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/building_applications/#quotas-setting-per-project) 
-  [Resource quotas across multiple projects](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/building_applications/#quotas-setting-across-multiple-projects) 


### 9.16.2. Specifying nodes for virtual machines




You can place virtual machines (VMs) on specific nodes by using node placement rules.

#### 9.16.2.1. About node placement for virtual machines




To ensure that virtual machines (VMs) run on appropriate nodes, you can configure node placement rules. You might want to do this if:

- You have several VMs. To ensure fault tolerance, you want them to run on different nodes.
- You have two chatty VMs. To avoid redundant inter-node routing, you want the VMs to run on the same node.
- Your VMs require specific hardware features that are not present on all available nodes.
- You have a pod that adds capabilities to a node, and you want to place a VM on that node so that it can use those capabilities.


Note
Virtual machine placement relies on any existing node placement rules for workloads. If workloads are excluded from specific nodes on the component level, virtual machines cannot be placed on those nodes.



You can use the following rule types in the `spec` field of a `VirtualMachine` manifest:

#### 9.16.2.2. Node placement examples




The following example YAML file snippets use `nodePlacement` , `affinity` , and `tolerations` fields to customize node placement for virtual machines.

##### 9.16.2.2.1. Example: VM node placement with nodeSelector




In this example, the virtual machine requires a node that has metadata containing both `example-key-1 = example-value-1` and `example-key-2 = example-value-2` labels.

Warning
If there are no nodes that fit this description, the virtual machine is not scheduled.



 **Example VM manifest** 

```
metadata:
  name: example-vm-node-selector
apiVersion: kubevirt.io/v1
kind: VirtualMachine
spec:
  template:
    spec:
      nodeSelector:
        example-key-1: example-value-1
        example-key-2: example-value-2
...
```


##### 9.16.2.2.2. Example: VM node placement with pod affinity and pod anti-affinity




In this example, the VM must be scheduled on a node that has a running pod with the label `example-key-1 = example-value-1` . If there is no such pod running on any node, the VM is not scheduled.

If possible, the VM is not scheduled on a node that has any pod with the label `example-key-2 = example-value-2` . However, if all candidate nodes have a pod with this label, the scheduler ignores this constraint.

 **Example VM manifest** 

```
metadata:
  name: example-vm-pod-affinity
apiVersion: kubevirt.io/v1
kind: VirtualMachine
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:<span id="CO20-1"><!--Empty--></span><span class="callout">1</span>- labelSelector:
          matchExpressions:
          - key: example-key-1
            operator: In
            values:
            - example-value-1
        topologyKey: kubernetes.io/hostname
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:<span id="CO20-2"><!--Empty--></span><span class="callout">2</span>- weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: example-key-2
              operator: In
              values:
              - example-value-2
          topologyKey: kubernetes.io/hostname
...
```


##### 9.16.2.2.3. Example: VM node placement with node affinity




In this example, the VM must be scheduled on a node that has the label `example.io/example-key = example-value-1` or the label `example.io/example-key = example-value-2` . The constraint is met if only one of the labels is present on the node. If neither label is present, the VM is not scheduled.

If possible, the scheduler avoids nodes that have the label `example-node-label-key = example-node-label-value` . However, if all candidate nodes have this label, the scheduler ignores this constraint.

 **Example VM manifest** 

```
metadata:
  name: example-vm-node-affinity
apiVersion: kubevirt.io/v1
kind: VirtualMachine
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:<span id="CO21-1"><!--Empty--></span><span class="callout">1</span>nodeSelectorTerms:
        - matchExpressions:
          - key: example.io/example-key
            operator: In
            values:
            - example-value-1
            - example-value-2
      preferredDuringSchedulingIgnoredDuringExecution:<span id="CO21-2"><!--Empty--></span><span class="callout">2</span>- weight: 1
        preference:
          matchExpressions:
          - key: example-node-label-key
            operator: In
            values:
            - example-node-label-value
...
```


##### 9.16.2.2.4. Example: VM node placement with tolerations




In this example, nodes that are reserved for virtual machines are already labeled with the `key=virtualization:NoSchedule` taint. Because this virtual machine has matching `tolerations` , it can schedule onto the tainted nodes.

Note
A virtual machine that tolerates a taint is not required to schedule onto a node with that taint.



 **Example VM manifest** 

```
metadata:
  name: example-vm-tolerations
apiVersion: kubevirt.io/v1
kind: VirtualMachine
spec:
  tolerations:
  - key: "key"
    operator: "Equal"
    value: "virtualization"
    effect: "NoSchedule"
...
```


#### 9.16.2.3. Additional resources




-  [Specifying nodes for virtualization components](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/virtualization/#virt-specifying-nodes-for-virtualization-components) 
-  [Placing pods on specific nodes using node selectors](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/nodes/#nodes-scheduler-node-selectors) 
-  [Controlling pod placement on nodes using node affinity rules](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/nodes/#nodes-scheduler-node-affinity) 
-  [Controlling pod placement using node taints](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/nodes/#nodes-scheduler-taints-tolerations) 


### 9.16.3. Configuring certificate rotation




Configure certificate rotation parameters to replace existing certificates.

#### 9.16.3.1. Configuring certificate rotation




You can do this during OpenShift Virtualization installation in the web console or after installation in the `HyperConverged` custom resource (CR).

 **Procedure** 

1. Open the `    HyperConverged` CR by running the following command:
    
    
    ```
    $ oc edit hco -n openshift-cnv kubevirt-hyperconverged
    ```
    
    
1. Edit the `    spec.certConfig` fields as shown in the following example. To avoid overloading the system, ensure that all values are greater than or equal to 10 minutes. Express all values as strings that comply with the [golang ParseDuration format](https://golang.org/pkg/time/#ParseDuration) .
    
    
    ```
    apiVersion: hco.kubevirt.io/v1beta1    kind: HyperConverged    metadata:     name: kubevirt-hyperconverged     namespace: openshift-cnv    spec:      certConfig:        ca:          duration: 48h0m0s          renewBefore: 24h0m0s<span id="CO22-1"><!--Empty--></span><span class="callout">1</span>server:          duration: 24h0m0s<span id="CO22-2"><!--Empty--></span><span class="callout">2</span>renewBefore: 12h0m0s<span id="CO22-3"><!--Empty--></span><span class="callout">3</span>
    ```
    
    
1. Apply the YAML file to your cluster.


#### 9.16.3.2. Troubleshooting certificate rotation parameters




Deleting one or more `certConfig` values causes them to revert to the default values, unless the default values conflict with one of the following conditions:

- The value of `    ca.renewBefore` must be less than or equal to the value of `    ca.duration` .
- The value of `    server.duration` must be less than or equal to the value of `    ca.duration` .
- The value of `    server.renewBefore` must be less than or equal to the value of `    server.duration` .


If the default values conflict with these conditions, you will receive an error.

If you remove the `server.duration` value in the following example, the default value of `24h0m0s` is greater than the value of `ca.duration` , conflicting with the specified conditions.

 **Example** 

```
certConfig:
   ca:
     duration: 4h0m0s
     renewBefore: 1h0m0s
   server:
     duration: 4h0m0s
     renewBefore: 4h0m0s
```


This results in the following error message:

```
error: hyperconvergeds.hco.kubevirt.io "kubevirt-hyperconverged" could not be patched: admission webhook "validate-hco.kubevirt.io" denied the request: spec.certConfig: ca.duration is smaller than server.duration
```

The error message only mentions the first conflict. Review all certConfig values before you proceed.

### 9.16.4. Using UEFI mode for virtual machines




You can boot a virtual machine (VM) in Unified Extensible Firmware Interface (UEFI) mode.

#### 9.16.4.1. About UEFI mode for virtual machines




Unified Extensible Firmware Interface (UEFI), like legacy BIOS, initializes hardware components and operating system image files when a computer starts. UEFI supports more modern features and customization options than BIOS, enabling faster boot times.

It stores all the information about initialization and startup in a file with a `.efi` extension, which is stored on a special partition called EFI System Partition (ESP). The ESP also contains the boot loader programs for the operating system that is installed on the computer.

#### 9.16.4.2. Booting virtual machines in UEFI mode




You can configure a virtual machine to boot in UEFI mode by editing the `VirtualMachine` manifest.

 **Prerequisites** 

- Install the OpenShift CLI ( `    oc` ).


 **Procedure** 

1. Edit or create a `    VirtualMachine` manifest file. Use the `    spec.firmware.bootloader` stanza to configure UEFI mode:
    
     **Booting in UEFI mode with secure boot active** 
    
    
    ```
    apiversion: kubevirt.io/v1    kind: VirtualMachine    metadata:      labels:        special: vm-secureboot      name: vm-secureboot    spec:      template:        metadata:          labels:            special: vm-secureboot        spec:          domain:            devices:              disks:              - disk:                  bus: virtio                name: containerdisk            features:              acpi: {}              smm:                enabled: true<span id="CO23-1"><!--Empty--></span><span class="callout">1</span>firmware:              bootloader:                efi:                  secureBoot: true<span id="CO23-2"><!--Empty--></span><span class="callout">2</span>...
    ```
    
    
    
1. Apply the manifest to your cluster by running the following command:
    
    
    ```
    $ oc create -f &lt;file_name&gt;.yaml
    ```
    
    


### 9.16.5. Configuring PXE booting for virtual machines




PXE booting, or network booting, is available in OpenShift Virtualization. Network booting allows a computer to boot and load an operating system or other program without requiring a locally attached storage device. For example, you can use it to choose your desired OS image from a PXE server when deploying a new host.

#### 9.16.5.1. Prerequisites




- A Linux bridge must be [connected](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/virtualization/#virt-attaching-vm-multiple-networks) .
- The PXE server must be connected to the same VLAN as the bridge.


#### 9.16.5.2. PXE booting with a specified MAC address




As an administrator, you can boot a client over the network by first creating a `NetworkAttachmentDefinition` object for your PXE network. Then, reference the network attachment definition in your virtual machine instance configuration file before you start the virtual machine instance. You can also specify a MAC address in the virtual machine instance configuration file, if required by the PXE server.

 **Prerequisites** 

- A Linux bridge must be connected.
- The PXE server must be connected to the same VLAN as the bridge.


 **Procedure** 

1. Configure a PXE network on the cluster:
    
    
    1. Create the network attachment definition file for PXE network `        pxe-net-conf` :
        
        
        ```
        apiVersion: "k8s.cni.cncf.io/v1"        kind: NetworkAttachmentDefinition        metadata:          name: pxe-net-conf        spec:          config: '{            "cniVersion": "0.3.1",            "name": "pxe-net-conf",            "plugins": [              {                "type": "cnv-bridge",                "bridge": "br1",                "vlan": 1<span id="CO24-1"><!--Empty--></span><span class="callout">1</span>},              {                "type": "cnv-tuning"<span id="CO24-2"><!--Empty--></span><span class="callout">2</span>}            ]          }'
        ```
        
        Note
        The virtual machine instance will be attached to the bridge `        br1` through an access port with the requested VLAN.
        
        
        
        
    
1. Create the network attachment definition by using the file you created in the previous step:
    
    
    ```
    $ oc create -f pxe-net-conf.yaml
    ```
    
    
1. Edit the virtual machine instance configuration file to include the details of the interface and network.
    
    
    1. Specify the network and MAC address, if required by the PXE server. If the MAC address is not specified, a value is assigned automatically.
        
        Ensure that `        bootOrder` is set to `        1` so that the interface boots first. In this example, the interface is connected to a network called `        &lt;pxe-net&gt;` :
        
        
        ```
        interfaces:        - masquerade: {}          name: default        - bridge: {}          name: pxe-net          macAddress: de:00:00:00:00:de          bootOrder: 1
        ```
        
        Note
        Boot order is global for interfaces and disks.
        
        
        
        
    1. Assign a boot device number to the disk to ensure proper booting after operating system provisioning.
        
        Set the disk `        bootOrder` value to `        2` :
        
        
        ```
        devices:          disks:          - disk:              bus: virtio            name: containerdisk            bootOrder: 2
        ```
        
        
    1. Specify that the network is connected to the previously created network attachment definition. In this scenario, `        &lt;pxe-net&gt;` is connected to the network attachment definition called `        &lt;pxe-net-conf&gt;` :
        
        
        ```
        networks:        - name: default          pod: {}        - name: pxe-net          multus:            networkName: pxe-net-conf
        ```
        
        
    
1. Create the virtual machine instance:
    
    
    ```
    $ oc create -f vmi-pxe-boot.yaml
    ```
    
    


 **Example output** 

```
virtualmachineinstance.kubevirt.io "vmi-pxe-boot" created
```


1. Wait for the virtual machine instance to run:
    
    
    ```
    $ oc get vmi vmi-pxe-boot -o yaml | grep -i phase      phase: Running
    ```
    
    
1. View the virtual machine instance using VNC:
    
    
    ```
    $ virtctl vnc vmi-pxe-boot
    ```
    
    
1. Watch the boot screen to verify that the PXE boot is successful.
1. Log in to the virtual machine instance:
    
    
    ```
    $ virtctl console vmi-pxe-boot
    ```
    
    
1. Verify the interfaces and MAC address on the virtual machine and that the interface connected to the bridge has the specified MAC address. In this case, we used `    eth1` for the PXE boot, without an IP address. The other interface, `    eth0` , got an IP address from OpenShift Container Platform.
    
    
    ```
    $ ip addr
    ```
    
    


 **Example output** 

```
...
3. eth1: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN group default qlen 1000
   link/ether de:00:00:00:00:de brd ff:ff:ff:ff:ff:ff
```


#### 9.16.5.3. OpenShift Virtualization networking glossary




OpenShift Virtualization provides advanced networking functionality by using custom resources and plugins.

The following terms are used throughout OpenShift Virtualization documentation:

### 9.16.6. Using huge pages with virtual machines




You can use huge pages as backing memory for virtual machines in your cluster.

#### 9.16.6.1. Prerequisites




- Nodes must have [pre-allocated huge pages configured](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/scalability_and_performance/#configuring-huge-pages_huge-pages) .


#### 9.16.6.2. What huge pages do




Memory is managed in blocks known as pages. On most systems, a page is 4Ki. 1Mi of memory is equal to 256 pages; 1Gi of memory is 256,000 pages, and so on. CPUs have a built-in memory management unit that manages a list of these pages in hardware. The Translation Lookaside Buffer (TLB) is a small hardware cache of virtual-to-physical page mappings. If the virtual address passed in a hardware instruction can be found in the TLB, the mapping can be determined quickly. If not, a TLB miss occurs, and the system falls back to slower, software-based address translation, resulting in performance issues. Since the size of the TLB is fixed, the only way to reduce the chance of a TLB miss is to increase the page size.

A huge page is a memory page that is larger than 4Ki. On x86_64 architectures, there are two common huge page sizes: 2Mi and 1Gi. Sizes vary on other architectures. To use huge pages, code must be written so that applications are aware of them. Transparent Huge Pages (THP) attempt to automate the management of huge pages without application knowledge, but they have limitations. In particular, they are limited to 2Mi page sizes. THP can lead to performance degradation on nodes with high memory utilization or fragmentation due to defragmenting efforts of THP, which can lock memory pages. For this reason, some applications may be designed to (or recommend) usage of pre-allocated huge pages instead of THP.

In OpenShift Virtualization, virtual machines can be configured to consume pre-allocated huge pages.

#### 9.16.6.3. Configuring huge pages for virtual machines




You can configure virtual machines to use pre-allocated huge pages by including the `memory.hugepages.pageSize` and `resources.requests.memory` parameters in your virtual machine configuration.

The memory request must be divisible by the page size. For example, you cannot request `500Mi` memory with a page size of `1Gi` .

Note
The memory layouts of the host and the guest OS are unrelated. Huge pages requested in the virtual machine manifest apply to QEMU. Huge pages inside the guest can only be configured based on the amount of available memory of the virtual machine instance.



If you edit a running virtual machine, the virtual machine must be rebooted for the changes to take effect.

 **Prerequisites** 

- Nodes must have pre-allocated huge pages configured.


 **Procedure** 

1. In your virtual machine configuration, add the `    resources.requests.memory` and `    memory.hugepages.pageSize` parameters to the `    spec.domain` . The following configuration snippet is for a virtual machine that requests a total of `    4Gi` memory with a page size of `    1Gi` :
    
    
    ```
    kind: VirtualMachine    ...    spec:      domain:        resources:          requests:            memory: "4Gi"<span id="CO25-1"><!--Empty--></span><span class="callout">1</span>memory:          hugepages:            pageSize: "1Gi"<span id="CO25-2"><!--Empty--></span><span class="callout">2</span>...
    ```
    
    
1. Apply the virtual machine configuration:
    
    
    ```
    $ oc apply -f &lt;virtual_machine&gt;.yaml
    ```
    
    


### 9.16.7. Enabling dedicated resources for virtual machines




To improve performance, you can dedicate node resources, such as CPU, to a virtual machine.

#### 9.16.7.1. About dedicated resources




When you enable dedicated resources for your virtual machine, your virtual machine’s workload is scheduled on CPUs that will not be used by other processes. By using dedicated resources, you can improve the performance of the virtual machine and the accuracy of latency predictions.

#### 9.16.7.2. Prerequisites




- The [CPU Manager](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/scalability_and_performance/#using-cpu-manager-and-topology-manager) must be configured on the node. Verify that the node has the `    cpumanager = true` label before scheduling virtual machine workloads.
- The virtual machine must be powered off.


#### 9.16.7.3. Enabling dedicated resources for a virtual machine




You enable dedicated resources for a virtual machine in the **Details** tab. Virtual machines that were created from a Red Hat template can be configured with dedicated resources.

 **Procedure** 

1. In the OpenShift Container Platform console, click **Virtualization** → **VirtualMachines** from the side menu.
1. Select a virtual machine to open the **VirtualMachine details** page.
1. On the **Scheduling** tab, click the pencil icon beside **Dedicated Resources** .
1. Select **Schedule this workload with dedicated resources (guaranteed policy)** .
1. Click **Save** .


### 9.16.8. Scheduling virtual machines




You can schedule a virtual machine (VM) on a node by ensuring that the VM’s CPU model and policy attribute are matched for compatibility with the CPU models and policy attributes supported by the node.

#### 9.16.8.1. Policy attributes




You can schedule a virtual machine (VM) by specifying a policy attribute and a CPU feature that is matched for compatibility when the VM is scheduled on a node. A policy attribute specified for a VM determines how that VM is scheduled on a node.

| Policy attribute | Description |
| --- | --- |
| force | The VM is forced to be scheduled on a node. This is true even if the host CPU does not support the VM’s CPU. |
| require | Default policy that applies to a VM if the VM is not configured with a specific CPU model and feature specification. If a node is not configured to support CPU node discovery with this default policy attribute or any one of the other policy attributes, VMs are not scheduled on that node. Either the host CPU must support the VM’s CPU or the hypervisor must be able to emulate the supported CPU model. |
| optional | The VM is added to a node if that VM is supported by the host’s physical machine CPU. |
| disable | The VM cannot be scheduled with CPU node discovery. |
| forbid | The VM is not scheduled even if the feature is supported by the host CPU and CPU node discovery is enabled. |


#### 9.16.8.2. Setting a policy attribute and CPU feature




You can set a policy attribute and CPU feature for each virtual machine (VM) to ensure that it is scheduled on a node according to policy and feature. The CPU feature that you set is verified to ensure that it is supported by the host CPU or emulated by the hypervisor.

 **Procedure** 

- Edit the `    domain` spec of your VM configuration file. The following example sets the CPU feature and the `    require` policy for a virtual machine (VM):
    
    
    ```
    apiVersion: kubevirt.io/v1    kind: VirtualMachine    metadata:      name: myvm    spec:      template:        spec:          domain:            cpu:              features:                - name: apic<span id="CO26-1"><!--Empty--></span><span class="callout">1</span>policy: require<span id="CO26-2"><!--Empty--></span><span class="callout">2</span>
    ```
    
    


#### 9.16.8.3. Scheduling virtual machines with the supported CPU model




You can configure a CPU model for a virtual machine (VM) to schedule it on a node where its CPU model is supported.

 **Procedure** 

- Edit the `    domain` spec of your virtual machine configuration file. The following example shows a specific CPU model defined for a VM:
    
    
    ```
    apiVersion: kubevirt.io/v1    kind: VirtualMachine    metadata:      name: myvm    spec:      template:        spec:          domain:            cpu:              model: Conroe<span id="CO27-1"><!--Empty--></span><span class="callout">1</span>
    ```
    
    


#### 9.16.8.4. Scheduling virtual machines with the host model




When the CPU model for a virtual machine (VM) is set to `host-model` , the VM inherits the CPU model of the node where it is scheduled.

 **Procedure** 

- Edit the `    domain` spec of your VM configuration file. The following example shows `    host-model` being specified for the virtual machine:
    
    
    ```
    apiVersion: kubevirt/v1alpha3    kind: VirtualMachine    metadata:      name: myvm    spec:      template:        spec:          domain:            cpu:              model: host-model<span id="CO28-1"><!--Empty--></span><span class="callout">1</span>
    ```
    
    


### 9.16.9. Configuring PCI passthrough




The Peripheral Component Interconnect (PCI) passthrough feature enables you to access and manage hardware devices from a virtual machine. When PCI passthrough is configured, the PCI devices function as if they were physically attached to the guest operating system.

Cluster administrators can expose and manage host devices that are permitted to be used in the cluster by using the `oc` command-line interface (CLI).

#### 9.16.9.1. About preparing a host device for PCI passthrough




To prepare a host device for PCI passthrough by using the CLI, create a `MachineConfig` object and add kernel arguments to enable the Input-Output Memory Management Unit (IOMMU). Bind the PCI device to the Virtual Function I/O (VFIO) driver and then expose it in the cluster by editing the `permittedHostDevices` field of the `HyperConverged` custom resource (CR). The `permittedHostDevices` list is empty when you first install the OpenShift Virtualization Operator.

To remove a PCI host device from the cluster by using the CLI, delete the PCI device information from the `HyperConverged` CR.

##### 9.16.9.1.1. Adding kernel arguments to enable the IOMMU driver




To enable the IOMMU (Input-Output Memory Management Unit) driver in the kernel, create the `MachineConfig` object and add the kernel arguments.

 **Prerequisites** 

- Administrative privilege to a working OpenShift Container Platform cluster.
- Intel or AMD CPU hardware.
- Intel Virtualization Technology for Directed I/O extensions or AMD IOMMU in the BIOS (Basic Input/Output System) is enabled.


 **Procedure** 

1. Create a `    MachineConfig` object that identifies the kernel argument. The following example shows a kernel argument for an Intel CPU.
    
    
    ```
    apiVersion: machineconfiguration.openshift.io/v1    kind: MachineConfig    metadata:      labels:        machineconfiguration.openshift.io/role: worker<span id="CO29-1"><!--Empty--></span><span class="callout">1</span>name: 100-worker-iommu<span id="CO29-2"><!--Empty--></span><span class="callout">2</span>spec:      config:        ignition:          version: 3.2.0      kernelArguments:          - intel_iommu=on<span id="CO29-3"><!--Empty--></span><span class="callout">3</span>...
    ```
    
    
1. Create the new `    MachineConfig` object:
    
    
    ```
    $ oc create -f 100-worker-kernel-arg-iommu.yaml
    ```
    
    


 **Verification** 

- Verify that the new `    MachineConfig` object was added.
    
    
    ```
    $ oc get MachineConfig
    ```
    
    


##### 9.16.9.1.2. Binding PCI devices to the VFIO driver




To bind PCI devices to the VFIO (Virtual Function I/O) driver, obtain the values for `vendor-ID` and `device-ID` from each device and create a list with the values. Add this list to the `MachineConfig` object. The `MachineConfig` Operator generates the `/etc/modprobe.d/vfio.conf` on the nodes with the PCI devices, and binds the PCI devices to the VFIO driver.

 **Prerequisites** 

- You added kernel arguments to enable IOMMU for the CPU.


 **Procedure** 

1. Run the `    lspci` command to obtain the `    vendor-ID` and the `    device-ID` for the PCI device.
    
    
    ```
    $ lspci -nnv | grep -i nvidia
    ```
    
     **Example output** 
    
    
    ```
    02:01.0 3D controller [0302]: NVIDIA Corporation GV100GL [Tesla V100 PCIe 32GB] [10de:1eb8] (rev a1)
    ```
    
    
    
1. Create a Butane config file, `    100-worker-vfiopci.bu` , binding the PCI device to the VFIO driver.
    
    Note
    See "Creating machine configs with Butane" for information about Butane.
    
    
    
     **Example** 
    
    
    ```
    variant: openshift    version: 4.11.0    metadata:      name: 100-worker-vfiopci      labels:        machineconfiguration.openshift.io/role: worker<span id="CO30-1"><!--Empty--></span><span class="callout">1</span>storage:      files:      - path: /etc/modprobe.d/vfio.conf        mode: 0644        overwrite: true        contents:          inline: |            options vfio-pci ids=10de:1eb8<span id="CO30-2"><!--Empty--></span><span class="callout">2</span>- path: /etc/modules-load.d/vfio-pci.conf<span id="CO30-3"><!--Empty--></span><span class="callout">3</span>mode: 0644        overwrite: true        contents:          inline: vfio-pci
    ```
    
    
    
1. Use Butane to generate a `    MachineConfig` object file, `    100-worker-vfiopci.yaml` , containing the configuration to be delivered to the worker nodes:
    
    
    ```
    $ butane 100-worker-vfiopci.bu -o 100-worker-vfiopci.yaml
    ```
    
    
1. Apply the `    MachineConfig` object to the worker nodes:
    
    
    ```
    $ oc apply -f 100-worker-vfiopci.yaml
    ```
    
    
1. Verify that the `    MachineConfig` object was added.
    
    
    ```
    $ oc get MachineConfig
    ```
    
     **Example output** 
    
    
    ```
    NAME                             GENERATEDBYCONTROLLER                      IGNITIONVERSION  AGE    00-master                        d3da910bfa9f4b599af4ed7f5ac270d55950a3a1   3.2.0            25h    00-worker                        d3da910bfa9f4b599af4ed7f5ac270d55950a3a1   3.2.0            25h    01-master-container-runtime      d3da910bfa9f4b599af4ed7f5ac270d55950a3a1   3.2.0            25h    01-master-kubelet                d3da910bfa9f4b599af4ed7f5ac270d55950a3a1   3.2.0            25h    01-worker-container-runtime      d3da910bfa9f4b599af4ed7f5ac270d55950a3a1   3.2.0            25h    01-worker-kubelet                d3da910bfa9f4b599af4ed7f5ac270d55950a3a1   3.2.0            25h    100-worker-iommu                                                            3.2.0            30s    100-worker-vfiopci-configuration                                            3.2.0            30s
    ```
    
    
    


 **Verification** 

- Verify that the VFIO driver is loaded.
    
    
    ```
    $ lspci -nnk -d 10de:
    ```
    
    The output confirms that the VFIO driver is being used.
    
     **Example output** 
    
    
    ```
    04:00.0 3D controller [0302]: NVIDIA Corporation GP102GL [Tesla P40] [10de:1eb8] (rev a1)            Subsystem: NVIDIA Corporation Device [10de:1eb8]            Kernel driver in use: vfio-pci            Kernel modules: nouveau
    ```
    
    
    


##### 9.16.9.1.3. Exposing PCI host devices in the cluster using the CLI




To expose PCI host devices in the cluster, add details about the PCI devices to the `spec.permittedHostDevices.pciHostDevices` array of the `HyperConverged` custom resource (CR).

 **Procedure** 

1. Edit the `    HyperConverged` CR in your default editor by running the following command:
    
    
    ```
    $ oc edit hyperconverged kubevirt-hyperconverged -n openshift-cnv
    ```
    
    
1. Add the PCI device information to the `    spec.permittedHostDevices.pciHostDevices` array. For example:
    
     **Example configuration file** 
    
    
    ```
    apiVersion: hco.kubevirt.io/v1    kind: HyperConverged    metadata:      name: kubevirt-hyperconverged      namespace: openshift-cnv    spec:      permittedHostDevices:<span id="CO31-1"><!--Empty--></span><span class="callout">1</span>pciHostDevices:<span id="CO31-2"><!--Empty--></span><span class="callout">2</span>- pciDeviceSelector: "10DE:1DB6"<span id="CO31-3"><!--Empty--></span><span class="callout">3</span>resourceName: "nvidia.com/GV100GL_Tesla_V100"<span id="CO31-4"><!--Empty--></span><span class="callout">4</span>- pciDeviceSelector: "10DE:1EB8"          resourceName: "nvidia.com/TU104GL_Tesla_T4"        - pciDeviceSelector: "8086:6F54"          resourceName: "intel.com/qat"          externalResourceProvider: true<span id="CO31-5"><!--Empty--></span><span class="callout">5</span>...
    ```
    
    
    Note
    The above example snippet shows two PCI host devices that are named `    nvidia.com/GV100GL_Tesla_V100` and `    nvidia.com/TU104GL_Tesla_T4` added to the list of permitted host devices in the `    HyperConverged` CR. These devices have been tested and verified to work with OpenShift Virtualization.
    
    
    
    
1. Save your changes and exit the editor.


 **Verification** 

- Verify that the PCI host devices were added to the node by running the following command. The example output shows that there is one device each associated with the `    nvidia.com/GV100GL_Tesla_V100` , `    nvidia.com/TU104GL_Tesla_T4` , and `    intel.com/qat` resource names.
    
    
    ```
    $ oc describe node &lt;node_name&gt;
    ```
    
     **Example output** 
    
    
    ```
    Capacity:      cpu:                            64      devices.kubevirt.io/kvm:        110      devices.kubevirt.io/tun:        110      devices.kubevirt.io/vhost-net:  110      ephemeral-storage:              915128Mi      hugepages-1Gi:                  0      hugepages-2Mi:                  0      memory:                         131395264Ki      nvidia.com/GV100GL_Tesla_V100   1      nvidia.com/TU104GL_Tesla_T4     1      intel.com/qat:                  1      pods:                           250    Allocatable:      cpu:                            63500m      devices.kubevirt.io/kvm:        110      devices.kubevirt.io/tun:        110      devices.kubevirt.io/vhost-net:  110      ephemeral-storage:              863623130526      hugepages-1Gi:                  0      hugepages-2Mi:                  0      memory:                         130244288Ki      nvidia.com/GV100GL_Tesla_V100   1      nvidia.com/TU104GL_Tesla_T4     1      intel.com/qat:                  1      pods:                           250
    ```
    
    
    


##### 9.16.9.1.4. Removing PCI host devices from the cluster using the CLI




To remove a PCI host device from the cluster, delete the information for that device from the `HyperConverged` custom resource (CR).

 **Procedure** 

1. Edit the `    HyperConverged` CR in your default editor by running the following command:
    
    
    ```
    $ oc edit hyperconverged kubevirt-hyperconverged -n openshift-cnv
    ```
    
    
1. Remove the PCI device information from the `    spec.permittedHostDevices.pciHostDevices` array by deleting the `    pciDeviceSelector` , `    resourceName` and `    externalResourceProvider` (if applicable) fields for the appropriate device. In this example, the `    intel.com/qat` resource has been deleted.
    
     **Example configuration file** 
    
    
    ```
    apiVersion: hco.kubevirt.io/v1    kind: HyperConverged    metadata:      name: kubevirt-hyperconverged      namespace: openshift-cnv    spec:      permittedHostDevices:        pciHostDevices:        - pciDeviceSelector: "10DE:1DB6"          resourceName: "nvidia.com/GV100GL_Tesla_V100"        - pciDeviceSelector: "10DE:1EB8"          resourceName: "nvidia.com/TU104GL_Tesla_T4"    ...
    ```
    
    
    
1. Save your changes and exit the editor.


 **Verification** 

- Verify that the PCI host device was removed from the node by running the following command. The example output shows that there are zero devices associated with the `    intel.com/qat` resource name.
    
    
    ```
    $ oc describe node &lt;node_name&gt;
    ```
    
     **Example output** 
    
    
    ```
    Capacity:      cpu:                            64      devices.kubevirt.io/kvm:        110      devices.kubevirt.io/tun:        110      devices.kubevirt.io/vhost-net:  110      ephemeral-storage:              915128Mi      hugepages-1Gi:                  0      hugepages-2Mi:                  0      memory:                         131395264Ki      nvidia.com/GV100GL_Tesla_V100   1      nvidia.com/TU104GL_Tesla_T4     1      intel.com/qat:                  0      pods:                           250    Allocatable:      cpu:                            63500m      devices.kubevirt.io/kvm:        110      devices.kubevirt.io/tun:        110      devices.kubevirt.io/vhost-net:  110      ephemeral-storage:              863623130526      hugepages-1Gi:                  0      hugepages-2Mi:                  0      memory:                         130244288Ki      nvidia.com/GV100GL_Tesla_V100   1      nvidia.com/TU104GL_Tesla_T4     1      intel.com/qat:                  0      pods:                           250
    ```
    
    
    


#### 9.16.9.2. Configuring virtual machines for PCI passthrough




After the PCI devices have been added to the cluster, you can assign them to virtual machines. The PCI devices are now available as if they are physically connected to the virtual machines.

##### 9.16.9.2.1. Assigning a PCI device to a virtual machine




When a PCI device is available in a cluster, you can assign it to a virtual machine and enable PCI passthrough.

 **Procedure** 

- Assign the PCI device to a virtual machine as a host device.
    
     **Example** 
    
    
    ```
    apiVersion: kubevirt.io/v1    kind: VirtualMachine    spec:      domain:        devices:          hostDevices:          - deviceName: nvidia.com/TU104GL_Tesla_T4<span id="CO32-1"><!--Empty--></span><span class="callout">1</span>name: hostdevices1
    ```
    
    
    


 **Verification** 

- Use the following command to verify that the host device is available from the virtual machine.
    
    
    ```
    $ lspci -nnk | grep NVIDIA
    ```
    
     **Example output** 
    
    
    ```
    $ 02:01.0 3D controller [0302]: NVIDIA Corporation GV100GL [Tesla V100 PCIe 32GB] [10de:1eb8] (rev a1)
    ```
    
    
    


#### 9.16.9.3. Additional resources




-  [Enabling Intel VT-X and AMD-V Virtualization Hardware Extensions in BIOS](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/virtualization_deployment_and_administration_guide/sect-troubleshooting-enabling_intel_vt_x_and_amd_v_virtualization_hardware_extensions_in_bios) 
-  [Managing file permissions](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_basic_system_settings/assembly_managing-file-permissions_configuring-basic-system-settings) 
-  [Post-installation machine configuration tasks](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/post-installation_configuration/#post-install-machine-configuration-tasks) 


### 9.16.10. Configuring vGPU passthrough




Your virtual machines can access a virtual GPU (vGPU) hardware. Assigning a vGPU to your virtual machine allows you do the following:

- Access a fraction of the underlying hardware’s GPU to achieve high performance benefits in your virtual machine.
- Streamline resource-intensive I/O operations.


Important
vGPU passthrough can only be assigned to devices that are connected to clusters running in a bare metal environment.



#### 9.16.10.1. Assigning vGPU passthrough devices to a virtual machine




Use the OpenShift Container Platform web console to assign vGPU passthrough devices to your virtual machine.

 **Prerequisites** 

- The virtual machine must be stopped.


 **Procedure** 

1. In the OpenShift Container Platform web console, click **Virtualization → VirtualMachines** from the side menu.
1. Select the virtual machine to which you want to assign the device.
1. On the **Details** tab, click **GPU devices** .
    
    If you add a vGPU device as a host device, you cannot access the device with the VNC console.
    
    
1. Click **Add GPU device** , enter the **Name** and select the device from the **Device name** list.
1. Click **Save** .
1. Click the **YAML** tab to verify that the new devices have been added to your cluster configuration in the `    hostDevices` section.


Note
You can add hardware devices to virtual machines created from customized templates or a YAML file. You cannot add devices to pre-supplied boot source templates for specific operating systems, such as Windows 10 or RHEL 7.

To display resources that are connected to your cluster, click **Compute** → **Hardware Devices** from the side menu.



#### 9.16.10.2. Additional resources




-  [Creating virtual machines](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/virtualization/#virt-create-vms) 
-  [Creating virtual machine templates](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/virtualization/#virt-creating-vm-template) 


### 9.16.11. Configuring mediated devices




OpenShift Virtualization automatically creates mediated devices, such as virtual GPUs (vGPUs), if you provide a list of devices in the `HyperConverged` custom resource (CR).

Important
Declarative configuration of mediated devices is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.

For more information about the support scope of Red Hat Technology Preview features, see [Technology Preview Features Support Scope](https://access.redhat.com/support/offerings/techpreview/) .



#### 9.16.11.1. About using the NVIDIA GPU Operator




The NVIDIA GPU Operator manages NVIDIA GPU resources in an OpenShift Container Platform cluster and automates tasks related to bootstrapping GPU nodes. Since the GPU is a special resource in the cluster, you must install some components before deploying application workloads onto the GPU. These components include the NVIDIA drivers which enables compute unified device architecture (CUDA), Kubernetes device plugin, container runtime and others such as automatic node labelling, monitoring and more.

Note
The NVIDIA GPU Operator is supported only by NVIDIA. For more information about obtaining support from NVIDIA, see [Obtaining Support from NVIDIA](https://access.redhat.com/solutions/5174941) .



There are two ways to enable GPUs with OpenShift Container Platform OpenShift Virtualization: the OpenShift Container Platform-native way described here and by using the NVIDIA GPU Operator.

The NVIDIA GPU Operator is a Kubernetes Operator that enables OpenShift Container Platform OpenShift Virtualization to expose GPUs to virtualized workloads running on OpenShift Container Platform. It allows users to easily provision and manage GPU-enabled virtual machines, providing them with the ability to run complex artificial intelligence/machine learning (AI/ML) workloads on the same platform as their other workloads. It also provides an easy way to scale the GPU capacity of their infrastructure, allowing for rapid growth of GPU-based workloads.

For more information about using the NVIDIA GPU Operator to provision worker nodes for running GPU-accelerated VMs, see [NVIDIA GPU Operator with OpenShift Virtualization](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/openshift/openshift-virtualization.html) .

#### 9.16.11.2. About using virtual GPUs with OpenShift Virtualization




Some graphics processing unit (GPU) cards support the creation of virtual GPUs (vGPUs). OpenShift Virtualization can automatically create vGPUs and other mediated devices if an administrator provides configuration details in the `HyperConverged` custom resource (CR). This automation is especially useful for large clusters.

Note
Refer to your hardware vendor’s documentation for functionality and support details.



##### 9.16.11.2.1. Prerequisites




- If your hardware vendor provides drivers, you installed them on the nodes where you want to create mediated devices.
    
    
    - If you use NVIDIA cards, you [installed the NVIDIA GRID driver](https://access.redhat.com/solutions/6738411) .
    


##### 9.16.11.2.2. Configuration overview




When configuring mediated devices, an administrator must complete the following tasks:

- Create the mediated devices.
- Expose the mediated devices to the cluster.


The `HyperConverged` CR includes APIs that accomplish both tasks.

 **Creating mediated devices** 

```
...
spec:
  mediatedDevicesConfiguration:
    mediatedDevicesTypes:<span id="CO33-1"><!--Empty--></span><span class="callout">1</span>- &lt;device_type&gt;
    nodeMediatedDeviceTypes:<span id="CO33-2"><!--Empty--></span><span class="callout">2</span>- mediatedDevicesTypes:<span id="CO33-3"><!--Empty--></span><span class="callout">3</span>- &lt;device_type&gt;
      nodeSelector:<span id="CO33-4"><!--Empty--></span><span class="callout">4</span>&lt;node_selector_key&gt;: &lt;node_selector_value&gt;
...
```


 **Exposing mediated devices to the cluster** 

```
...
  permittedHostDevices:
    mediatedDevices:
    - mdevNameSelector: GRID T4-2Q<span id="CO34-1"><!--Empty--></span><span class="callout">1</span>resourceName: nvidia.com/GRID_T4-2Q<span id="CO34-2"><!--Empty--></span><span class="callout">2</span>...
```


##### 9.16.11.2.3. How vGPUs are assigned to nodes




For each physical device, OpenShift Virtualization configures the following values:

- A single mdev type.
- The maximum number of instances of the selected `    mdev` type.


The cluster architecture affects how devices are created and assigned to nodes.

##### 9.16.11.2.4. About changing and removing mediated devices




The cluster’s mediated device configuration can be updated with OpenShift Virtualization by:

- Editing the `    HyperConverged` CR and change the contents of the `    mediatedDevicesTypes` stanza.
- Changing the node labels that match the `    nodeMediatedDeviceTypes` node selector.
- Removing the device information from the `    spec.mediatedDevicesConfiguration` and `    spec.permittedHostDevices` stanzas of the `    HyperConverged` CR.
    
    Note
    If you remove the device information from the `    spec.permittedHostDevices` stanza without also removing it from the `    spec.mediatedDevicesConfiguration` stanza, you cannot create a new mediated device type on the same node. To properly remove mediated devices, remove the device information from both stanzas.
    
    
    
    


Depending on the specific changes, these actions cause OpenShift Virtualization to reconfigure mediated devices or remove them from the cluster nodes.

##### 9.16.11.2.5. Preparing hosts for mediated devices




You must enable the Input-Output Memory Management Unit (IOMMU) driver before you can configure mediated devices.

###### 9.16.11.2.5.1. Adding kernel arguments to enable the IOMMU driver




To enable the IOMMU (Input-Output Memory Management Unit) driver in the kernel, create the `MachineConfig` object and add the kernel arguments.

 **Prerequisites** 

- Administrative privilege to a working OpenShift Container Platform cluster.
- Intel or AMD CPU hardware.
- Intel Virtualization Technology for Directed I/O extensions or AMD IOMMU in the BIOS (Basic Input/Output System) is enabled.


 **Procedure** 

1. Create a `    MachineConfig` object that identifies the kernel argument. The following example shows a kernel argument for an Intel CPU.
    
    
    ```
    apiVersion: machineconfiguration.openshift.io/v1    kind: MachineConfig    metadata:      labels:        machineconfiguration.openshift.io/role: worker<span id="CO35-1"><!--Empty--></span><span class="callout">1</span>name: 100-worker-iommu<span id="CO35-2"><!--Empty--></span><span class="callout">2</span>spec:      config:        ignition:          version: 3.2.0      kernelArguments:          - intel_iommu=on<span id="CO35-3"><!--Empty--></span><span class="callout">3</span>...
    ```
    
    
1. Create the new `    MachineConfig` object:
    
    
    ```
    $ oc create -f 100-worker-kernel-arg-iommu.yaml
    ```
    
    


 **Verification** 

- Verify that the new `    MachineConfig` object was added.
    
    
    ```
    $ oc get MachineConfig
    ```
    
    


##### 9.16.11.2.6. Adding and removing mediated devices




You can add or remove mediated devices.

###### 9.16.11.2.6.1. Creating and exposing mediated devices




You can expose and create mediated devices such as virtual GPUs (vGPUs) by editing the `HyperConverged` custom resource (CR).

 **Prerequisites** 

- You enabled the IOMMU (Input-Output Memory Management Unit) driver.


 **Procedure** 

1. Edit the `    HyperConverged` CR in your default editor by running the following command:
    
    
    ```
    $ oc edit hyperconverged kubevirt-hyperconverged -n openshift-cnv
    ```
    
    
1. Add the mediated device information to the `    HyperConverged` CR `    spec` , ensuring that you include the `    mediatedDevicesConfiguration` and `    permittedHostDevices` stanzas. For example:
    
     **Example configuration file** 
    
    
    ```
    apiVersion: hco.kubevirt.io/v1    kind: HyperConverged    metadata:      name: kubevirt-hyperconverged      namespace: openshift-cnv    spec:      mediatedDevicesConfiguration: &lt;.&gt;        mediatedDevicesTypes: &lt;.&gt;        - nvidia-231        nodeMediatedDeviceTypes: &lt;.&gt;        - mediatedDevicesTypes: &lt;.&gt;          - nvidia-233          nodeSelector:            kubernetes.io/hostname: node-11.redhat.com      permittedHostDevices: &lt;.&gt;        mediatedDevices:        - mdevNameSelector: GRID T4-2Q          resourceName: nvidia.com/GRID_T4-2Q        - mdevNameSelector: GRID T4-8Q          resourceName: nvidia.com/GRID_T4-8Q    ...
    ```
    
    
    <.> Creates mediated devices. <.> Required: Global `    mediatedDevicesTypes` configuration. <.> Optional: Overrides the global configuration for specific nodes. <.> Required if you use `    nodeMediatedDeviceTypes` . <.> Exposes mediated devices to the cluster.
    
    
1. Save your changes and exit the editor.


 **Verification** 

- You can verify that a device was added to a specific node by running the following command:
    
    
    ```
    $ oc describe node &lt;node_name&gt;
    ```
    
    


###### 9.16.11.2.6.2. Removing mediated devices from the cluster using the CLI




To remove a mediated device from the cluster, delete the information for that device from the `HyperConverged` custom resource (CR).

 **Procedure** 

1. Edit the `    HyperConverged` CR in your default editor by running the following command:
    
    
    ```
    $ oc edit hyperconverged kubevirt-hyperconverged -n openshift-cnv
    ```
    
    
1. Remove the device information from the `    spec.mediatedDevicesConfiguration` and `    spec.permittedHostDevices` stanzas of the `    HyperConverged` CR. Removing both entries ensures that you can later create a new mediated device type on the same node. For example:
    
     **Example configuration file** 
    
    
    ```
    apiVersion: hco.kubevirt.io/v1    kind: HyperConverged    metadata:      name: kubevirt-hyperconverged      namespace: openshift-cnv    spec:      mediatedDevicesConfiguration:        mediatedDevicesTypes:<span id="CO36-1"><!--Empty--></span><span class="callout">1</span>- nvidia-231      permittedHostDevices:        mediatedDevices:<span id="CO36-2"><!--Empty--></span><span class="callout">2</span>- mdevNameSelector: GRID T4-2Q          resourceName: nvidia.com/GRID_T4-2Q
    ```
    
    
    
1. Save your changes and exit the editor.


#### 9.16.11.3. Using mediated devices




A vGPU is a type of mediated device; the performance of the physical GPU is divided among the virtual devices. You can assign mediated devices to one or more virtual machines.

##### 9.16.11.3.1. Assigning a mediated device to a virtual machine




Assign mediated devices such as virtual GPUs (vGPUs) to virtual machines.

 **Prerequisites** 

- The mediated device is configured in the `    HyperConverged` custom resource.


 **Procedure** 

- Assign the mediated device to a virtual machine (VM) by editing the `    spec.domain.devices.gpus` stanza of the `    VirtualMachine` manifest:
    
     **Example virtual machine manifest** 
    
    
    ```
    apiVersion: kubevirt.io/v1    kind: VirtualMachine    spec:      domain:        devices:          gpus:          - deviceName: nvidia.com/TU104GL_Tesla_T4<span id="CO37-1"><!--Empty--></span><span class="callout">1</span>name: gpu1<span id="CO37-2"><!--Empty--></span><span class="callout">2</span>- deviceName: nvidia.com/GRID_T4-1Q            name: gpu2
    ```
    
    
    


 **Verification** 

- To verify that the device is available from the virtual machine, run the following command, substituting `    &lt;device_name&gt;` with the `    deviceName` value from the `    VirtualMachine` manifest:
    
    
    ```
    $ lspci -nnk | grep &lt;device_name&gt;
    ```
    
    


#### 9.16.11.4. Additional resources




-  [Enabling Intel VT-X and AMD-V Virtualization Hardware Extensions in BIOS](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/virtualization_deployment_and_administration_guide/sect-troubleshooting-enabling_intel_vt_x_and_amd_v_virtualization_hardware_extensions_in_bios) 


### 9.16.12. Configuring a watchdog




Expose a watchdog by configuring the virtual machine (VM) for a watchdog device, installing the watchdog, and starting the watchdog service.

#### 9.16.12.1. Prerequisites




- The virtual machine must have kernel support for an `    i6300esb` watchdog device. Red Hat Enterprise Linux (RHEL) images support `    i6300esb` .


#### 9.16.12.2. Defining a watchdog device




Define how the watchdog proceeds when the operating system (OS) no longer responds.


<span id="idm139667221728208"></span>
 **Table 9.5. Available actions** 

|  `poweroff` | The virtual machine (VM) powers down immediately. If `spec.running` is set to `true` , or `spec.runStrategy` is not set to `manual` , then the VM reboots. |
| --- | --- |
|  `reset` | The VM reboots in place and the guest OS cannot react. Because the length of time required for the guest OS to reboot can cause liveness probes to timeout, use of this option is discouraged. This timeout can extend the time it takes the VM to reboot if cluster-level protections notice the liveness probe failed and forcibly reschedule it. |
|  `shutdown` | The VM gracefully powers down by stopping all services. |




 **Procedure** 

1. Create a YAML file with the following contents:
    
    
    ```
    apiVersion: kubevirt.io/v1    kind: VirtualMachine    metadata:      labels:        kubevirt.io/vm: vm2-rhel84-watchdog      name: &lt;vm-name&gt;    spec:      running: false      template:        metadata:         labels:            kubevirt.io/vm: vm2-rhel84-watchdog        spec:          domain:            devices:              watchdog:                name: &lt;watchdog&gt;                i6300esb:                  action: "poweroff"<span id="CO38-1"><!--Empty--></span><span class="callout">1</span>...
    ```
    
    The example above configures the `    i6300esb` watchdog device on a RHEL8 VM with the poweroff action and exposes the device as `    /dev/watchdog` .
    
    This device can now be used by the watchdog binary.
    
    
1. Apply the YAML file to your cluster by running the following command:
    
    
    ```
    $ oc apply -f &lt;file_name&gt;.yaml
    ```
    
    


Important
This procedure is provided for testing watchdog functionality only and must not be run on production machines.



1. Run the following command to verify that the VM is connected to the watchdog device:
    
    
    ```
    $ lspci | grep watchdog -i
    ```
    
    
1. Run one of the following commands to confirm the watchdog is active:
    
    
    - Trigger a kernel panic:
        
        
        ```
        # echo c &gt; /proc/sysrq-trigger
        ```
        
        
    - Terminate the watchdog service:
        
        
        ```
        # pkill -9 watchdog
        ```
        
        
    


#### 9.16.12.3. Installing a watchdog device




Install the `watchdog` package on your virtual machine and start the watchdog service.

 **Procedure** 

1. As a root user, install the `    watchdog` package and dependencies:
    
    
    ```
    # yum install watchdog
    ```
    
    
1. Uncomment the following line in the `    /etc/watchdog.conf` file, and save the changes:
    
    
    ```
    #watchdog-device = /dev/watchdog
    ```
    
    
1. Enable the watchdog service to start on boot:
    
    
    ```
    # systemctl enable --now watchdog.service
    ```
    
    


#### 9.16.12.4. Additional resources




-  [Monitoring application health by using health checks](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/building_applications/#monitoring-application-health) 


### 9.16.13. Automatic importing and updating of pre-defined boot sources




You can use boot sources that are _system-defined_ and included with OpenShift Virtualization or _user-defined_ , which you create. System-defined boot source imports and updates are controlled by the product feature gate. You can enable, disable, or re-enable updates using the feature gate. User-defined boot sources are not controlled by the product feature gate and must be individually managed to opt in or opt out of automatic imports and updates.

Important
As of version 4.10, OpenShift Virtualization automatically imports and updates boot sources, unless you manually opt out or do not set a default storage class.

If you upgrade to version 4.10, you must manually enable automatic imports and updates for boot sources from version 4.9 or earlier.



#### 9.16.13.1. Enabling automatic boot source updates




If you have boot sources from OpenShift Virtualization 4.9 or earlier, you must manually turn on automatic updates for these boot sources. All boot sources in OpenShift Virtualization 4.10 and later are automatically updated by default.

To enable automatic boot source imports and updates, set the `cdi.kubevirt.io/dataImportCron` field to `true` for each boot source you want to update automatically.

 **Procedure** 

- To turn on automatic updates for a boot source, use the following command to apply the `    dataImportCron` label to the data source:
    
    
    ```
    $ oc label --overwrite DataSource rhel8 -n openshift-virtualization-os-images cdi.kubevirt.io/dataImportCron=true<span id="CO39-1"><!--Empty--></span><span class="callout">1</span>
    ```
    
    


#### 9.16.13.2. Disabling automatic boot source updates




Disabling automatic boot source imports and updates can be helpful to reduce the number of logs in disconnected environments or to reduce resource usage.

To disable automatic boot source imports and updates, set the `spec.featureGates.enableCommonBootImageImport` field in the `HyperConverged` custom resource (CR) to `false` .

Note
User-defined boot sources are not affected by this setting.



 **Procedure** 

- Use the following command to disable automatic boot source updates:
    
    
    ```
    $ oc patch hco kubevirt-hyperconverged -n openshift-cnv \     --type json -p '[{"op": "replace", "path": "/spec/featureGates/enableCommonBootImageImport", \     "value": false}]'
    ```
    
    


#### 9.16.13.3. Re-enabling automatic boot source updates




If you have previously disabled automatic boot source updates, you must manually re-enable the feature. Set the `spec.featureGates.enableCommonBootImageImport` field in the `HyperConverged` custom resource (CR) to `true` .

 **Procedure** 

- Use the following command to re-enable automatic updates:
    
    
    ```
    $ oc patch hco kubevirt-hyperconverged -n openshift-cnv --type json -p '[{"op": "replace", "path": "/spec/featureGates/enableCommonBootImageImport", "value": true}]'
    ```
    
    


#### 9.16.13.4. Configuring a storage class for user-defined boot source updates




You can configure a storage class that allows automatic importing and updating for user-defined boot sources.

 **Procedure** 

1. Define a new `    storageClassName` by editing the `    HyperConverged` custom resource (CR).
    
    
    ```
    apiVersion: hco.kubevirt.io/v1beta1    kind: HyperConverged    metadata:      name: kubevirt-hyperconverged    spec:      dataImportCronTemplates:      - metadata:          name: rhel8-image-cron        spec:          template:            spec:              storageClassName: &lt;appropriate_class_name&gt;    ...
    ```
    
    
1. Set the new default storage class by running the following commands:
    
    
    ```
    $ oc patch storageclass &lt;current_default_storage_class&gt; -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"false"}}}'
    ```
    
    
    ```
    $ oc patch storageclass &lt;appropriate_storage_class&gt; -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
    ```
    
    


#### 9.16.13.5. Enabling automatic updates for user-defined boot sources




OpenShift Virtualization automatically updates system-defined boot sources by default, but does not automatically update user-defined boot sources. You must manually enable automatic imports and updates on a user-defined boot sources by editing the `HyperConverged` custom resource (CR).

 **Procedure** 

1. Use the following command to open the `    HyperConverged` CR for editing:
    
    
    ```
    $ oc edit -n openshift-cnv HyperConverged
    ```
    
    
1. Edit the `    HyperConverged` CR, adding the appropriate template and boot source in the `    dataImportCronTemplates` section. For example:
    
     **Example in CentOS 7** 
    
    
    ```
    apiVersion: hco.kubevirt.io/v1beta1    kind: HyperConverged    metadata:      name: kubevirt-hyperconverged    spec:      dataImportCronTemplates:      - metadata:          name: centos7-image-cron          annotations:            cdi.kubevirt.io/storage.bind.immediate.requested: "true"<span id="CO40-1"><!--Empty--></span><span class="callout">1</span>spec:          schedule: "0 */12 * * *"<span id="CO40-2"><!--Empty--></span><span class="callout">2</span>template:            spec:              source:                registry:<span id="CO40-3"><!--Empty--></span><span class="callout">3</span>url: docker://quay.io/containerdisks/centos:7-2009              storage:                resources:                  requests:                    storage: 10Gi          managedDataSource: centos7<span id="CO40-4"><!--Empty--></span><span class="callout">4</span>retentionPolicy: "None"<span id="CO40-5"><!--Empty--></span><span class="callout">5</span>
    ```
    
    
    


#### 9.16.13.6. Disabling an automatic update for a system-defined or user-defined boot source




You can disable automatic imports and updates for a user-defined boot source and for a system-defined boot source.

Because system-defined boot sources are not listed by default in the `spec.dataImportCronTemplates` of the `HyperConverged` custom resource (CR), you must add the boot source and disable auto imports and updates.

 **Procedure** 

- To disable automatic imports and updates for a user-defined boot source, remove the boot source from the `    spec.dataImportCronTemplates` field in the custom resource list.
- To disable automatic imports and updates for a system-defined boot source:
    
    
    - Edit the `        HyperConverged` CR and add the boot source to `        spec.dataImportCronTemplates` .
    - Disable automatic imports and updates by setting the `        dataimportcrontemplate.kubevirt.io/enable` annotation to `        false` . For example:
        
        
        ```
        apiVersion: hco.kubevirt.io/v1beta1        kind: HyperConverged        metadata:          name: kubevirt-hyperconverged        spec:          dataImportCronTemplates:          - metadata:              annotations:                dataimportcrontemplate.kubevirt.io/enable: false              name: rhel8-image-cron        ...
        ```
        
        
    


#### 9.16.13.7. Verifying the status of a boot source




You can verify whether a boot source is system-defined or user-defined.

The `status` section of each boot source listed in the `status.dataImportChronTemplates` field of the `HyperConverged` CR indicates the type of boot source. For example, `commonTemplate: true` indicates a system-defined ( `commonTemplate` ) boot source and `status: {}` indicates a user-defined boot source.

 **Procedure** 

1. Use the `    oc get` command to list the `    dataImportChronTemplates` in the `    HyperConverged` CR.
1. Verify the status of the boot source.
    
     **Example output** 
    
    
    ```
    ...    apiVersion: hco.kubevirt.io/v1beta1    kind: HyperConverged    ...    spec:      ...    status:<span id="CO41-1"><!--Empty--></span><span class="callout">1</span>...      dataImportCronTemplates:<span id="CO41-2"><!--Empty--></span><span class="callout">2</span>- metadata:          annotations:            cdi.kubevirt.io/storage.bind.immediate.requested: "true"          name: centos-7-image-cron        spec:          garbageCollect: Outdated          managedDataSource: centos7          schedule: 55 8/12 * * *          template:            metadata: {}            spec:              source:                registry:                  url: docker://quay.io/containerdisks/centos:7-2009              storage:                resources:                  requests:                    storage: 30Gi            status: {}        status:          commonTemplate: true<span id="CO41-3"><!--Empty--></span><span class="callout">3</span>...      - metadata:          annotations:            cdi.kubevirt.io/storage.bind.immediate.requested: "true"          name: user-defined-dic        spec:          garbageCollect: Outdated          managedDataSource: user-defined-centos-stream8          schedule: 55 8/12 * * *          template:            metadata: {}            spec:              source:                registry:                  pullMethod: node                  url: docker://quay.io/containerdisks/centos-stream:8              storage:                resources:                  requests:                    storage: 30Gi            status: {}        status: {}<span id="CO41-4"><!--Empty--></span><span class="callout">4</span>...
    ```
    
    
    


### 9.16.14. Enabling descheduler evictions on virtual machines




You can use the descheduler to evict pods so that the pods can be rescheduled onto more appropriate nodes. If the pod is a virtual machine, the pod eviction causes the virtual machine to be live migrated to another node.

Important
Descheduler eviction for virtual machines is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.

For more information about the support scope of Red Hat Technology Preview features, see [Technology Preview Features Support Scope](https://access.redhat.com/support/offerings/techpreview/) .



#### 9.16.14.1. Descheduler profiles




Use the Technology Preview `DevPreviewLongLifecycle` profile to enable the descheduler on a virtual machine. This is the only descheduler profile currently available for OpenShift Virtualization. To ensure proper scheduling, create VMs with CPU and memory requests for the expected load.

#### 9.16.14.2. Installing the descheduler




The descheduler is not available by default. To enable the descheduler, you must install the Kube Descheduler Operator from OperatorHub and enable one or more descheduler profiles.

By default, the descheduler runs in predictive mode, which means that it only simulates pod evictions. You must change the mode to automatic for the descheduler to perform the pod evictions.

Important
If you have enabled hosted control planes in your cluster, set a custom priority threshold to lower the chance that pods in the hosted control plane namespaces are evicted. Set the priority threshold class name to `hypershift-control-plane` , because it has the lowest priority value ( `100000000` ) of the hosted control plane priority classes.



 **Prerequisites** 

- Cluster administrator privileges.
- Access to the OpenShift Container Platform web console.


 **Procedure** 

1. Log in to the OpenShift Container Platform web console.
1. Create the required namespace for the Kube Descheduler Operator.
    
    
    1. Navigate to **Administration** → **Namespaces** and click **Create Namespace** .
    1. Enter `        openshift-kube-descheduler-operator` in the **Name** field, enter `        openshift.io/cluster-monitoring=true` in the **Labels** field to enable descheduler metrics, and click **Create** .
    
1. Install the Kube Descheduler Operator.
    
    
    1. Navigate to **Operators** → **OperatorHub** .
    1. Type **Kube Descheduler Operator** into the filter box.
    1. Select the **Kube Descheduler Operator** and click **Install** .
    1. On the **Install Operator** page, select **A specific namespace on the cluster** . Select **openshift-kube-descheduler-operator** from the drop-down menu.
    1. Adjust the values for the **Update Channel** and **Approval Strategy** to the desired values.
    1. Click **Install** .
    
1. Create a descheduler instance.
    
    
    1. From the **Operators** → **Installed Operators** page, click the **Kube Descheduler Operator** .
    1. Select the **Kube Descheduler** tab and click **Create KubeDescheduler** .
    1. Edit the settings as necessary.
        
        
        1. To evict pods instead of simulating the evictions, change the **Mode** field to **Automatic** .
        1. Expand the **Profiles** section and select `            DevPreviewLongLifecycle` . The `            AffinityAndTaints` profile is enabled by default.
            
            Important
            The only profile currently available for OpenShift Virtualization is `            DevPreviewLongLifecycle` .
            
            
            
            
        
    


You can also configure the profiles and settings for the descheduler later using the OpenShift CLI ( `oc` ).

#### 9.16.14.3. Enabling descheduler evictions on a virtual machine (VM)




After the descheduler is installed, you can enable descheduler evictions on your VM by adding an annotation to the `VirtualMachine` custom resource (CR).

 **Prerequisites** 

- Install the descheduler in the OpenShift Container Platform web console or OpenShift CLI ( `    oc` ).
- Ensure that the VM is not running.


 **Procedure** 

1. Before starting the VM, add the `    descheduler.alpha.kubernetes.io/evict` annotation to the `    VirtualMachine` CR:
    
    
    ```
    apiVersion: kubevirt.io/v1    kind: VirtualMachine    spec:      template:        metadata:          annotations:            descheduler.alpha.kubernetes.io/evict: "true"
    ```
    
    
1. If you did not already set the `    DevPreviewLongLifecycle` profile in the web console during installation, specify the `    DevPreviewLongLifecycle` in the `    spec.profile` section of the `    KubeDescheduler` object:
    
    
    ```
    apiVersion: operator.openshift.io/v1    kind: KubeDescheduler    metadata:      name: cluster      namespace: openshift-kube-descheduler-operator    spec:      deschedulingIntervalSeconds: 3600      profiles:      - DevPreviewLongLifecycle      mode: Predictive<span id="CO42-1"><!--Empty--></span><span class="callout">1</span>
    ```
    
    


The descheduler is now enabled on the VM.

#### 9.16.14.4. Additional resources




-  [Evicting pods using the descheduler](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/nodes/#nodes-descheduler) 


