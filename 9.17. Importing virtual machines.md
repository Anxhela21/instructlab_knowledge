## 9.17. Importing virtual machines




### 9.17.1. TLS certificates for data volume imports




#### 9.17.1.1. Adding TLS certificates for authenticating data volume imports




TLS certificates for registry or HTTPS endpoints must be added to a config map to import data from these sources. This config map must be present in the namespace of the destination data volume.

Create the config map by referencing the relative file path for the TLS certificate.

 **Procedure** 

1. Ensure you are in the correct namespace. The config map can only be referenced by data volumes if it is in the same namespace.
    
    
    ```
    $ oc get ns
    ```
    
    
1. Create the config map:
    
    
    ```
    $ oc create configmap &lt;configmap-name&gt; --from-file=&lt;/path/to/file/ca.pem&gt;
    ```
    
    


#### 9.17.1.2. Example: Config map created from a TLS certificate




The following example is of a config map created from `ca.pem` TLS certificate.

```
apiVersion: v1
kind: ConfigMap
metadata:
  name: tls-certs
data:
  ca.pem: |
    -----BEGIN CERTIFICATE-----
    ... &lt;base64 encoded cert&gt; ...
    -----END CERTIFICATE-----
```

### 9.17.2. Importing virtual machine images with data volumes




Use the Containerized Data Importer (CDI) to import a virtual machine image into a persistent volume claim (PVC) by using a data volume. You can attach a data volume to a virtual machine for persistent storage.

The virtual machine image can be hosted at an HTTP or HTTPS endpoint, or built into a container disk and stored in a container registry.

Important
When you import a disk image into a PVC, the disk image is expanded to use the full storage capacity that is requested in the PVC. To use this space, the disk partitions and file system(s) in the virtual machine might need to be expanded.

The resizing procedure varies based on the operating system installed on the virtual machine. See the operating system documentation for details.



#### 9.17.2.1. Prerequisites




- If the endpoint requires a TLS certificate, the certificate must be [included in a config map](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/virtualization/#virt-adding-tls-certificates-for-authenticating-dv-imports_virt-tls-certificates-for-dv-imports) in the same namespace as the data volume and referenced in the data volume configuration.
- To import a container disk:
    
    
    - You might need to [prepare a container disk from a virtual machine image](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/virtualization/#virt-preparing-container-disk-for-vms_virt-using-container-disks-with-vms) and store it in your container registry before importing it.
    - If the container registry does not have TLS, you must [add the registry to the insecureRegistries field of the HyperConverged custom resource](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/virtualization/#virt-disabling-tls-for-registry_virt-using-container-disks-with-vms) before you can import a container disk from it.
    
- You might need to [define a storage class or prepare CDI scratch space](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/virtualization/#virt-defining-storageclass_virt-preparing-cdi-scratch-space) for this operation to complete successfully.


#### 9.17.2.2. CDI supported operations matrix




This matrix shows the supported CDI operations for content types against endpoints, and which of these operations requires scratch space.

| Content types | HTTP | HTTPS | HTTP basic auth | Registry | Upload |
| --- | --- | --- | --- | --- | --- |
| KubeVirt (QCOW2) | ✓ QCOW2    
✓ GZ*    
✓ XZ* | ✓ QCOW2**    
✓ GZ*    
✓ XZ* | ✓ QCOW2    
✓ GZ*    
✓ XZ* | ✓ QCOW2*    
□ GZ    
□ XZ | ✓ QCOW2*    
✓ GZ*    
✓ XZ* |
| KubeVirt (RAW) | ✓ RAW    
✓ GZ    
✓ XZ | ✓ RAW    
✓ GZ    
✓ XZ | ✓ RAW    
✓ GZ    
✓ XZ | ✓ RAW*    
□ GZ    
□ XZ | ✓ RAW*    
✓ GZ*    
✓ XZ* |


✓ Supported operation

□ Unsupported operation

* Requires scratch space

** Requires scratch space if a custom certificate authority is required

Note
CDI now uses the OpenShift Container Platform [cluster-wide proxy configuration](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/networking/#enable-cluster-wide-proxy) .



#### 9.17.2.3. About data volumes




 `DataVolume` objects are custom resources that are provided by the Containerized Data Importer (CDI) project. Data volumes orchestrate import, clone, and upload operations that are associated with an underlying persistent volume claim (PVC). Data volumes are integrated with OpenShift Virtualization, and they prevent a virtual machine from being started before the PVC has been prepared.

#### 9.17.2.4. Importing a virtual machine image into storage by using a data volume




You can import a virtual machine image into storage by using a data volume.

The virtual machine image can be hosted at an HTTP or HTTPS endpoint or the image can be built into a container disk and stored in a container registry.

You specify the data source for the image in a `VirtualMachine` configuration file. When the virtual machine is created, the data volume with the virtual machine image is imported into storage.

 **Prerequisites** 

- To import a virtual machine image you must have the following:
    
    
    - A virtual machine disk image in RAW, ISO, or QCOW2 format, optionally compressed by using `        xz` or `        gz` .
    - An HTTP or HTTPS endpoint where the image is hosted, along with any authentication credentials needed to access the data source.
    
- To import a container disk, you must have a virtual machine image built into a container disk and stored in a container registry, along with any authentication credentials needed to access the data source.
- If the virtual machine must communicate with servers that use self-signed certificates or certificates not signed by the system CA bundle, you must create a config map in the same namespace as the data volume.


 **Procedure** 

1. If your data source requires authentication, create a `    Secret` manifest, specifying the data source credentials, and save it as `    endpoint-secret.yaml` :
    
    
    ```
    apiVersion: v1    kind: Secret    metadata:      name: endpoint-secret<span id="CO43-1"><!--Empty--></span><span class="callout">1</span>labels:        app: containerized-data-importer    type: Opaque    data:      accessKeyId: ""<span id="CO43-2"><!--Empty--></span><span class="callout">2</span>secretKey:   ""<span id="CO43-3"><!--Empty--></span><span class="callout">3</span>
    ```
    
    
1. Apply the `    Secret` manifest:
    
    
    ```
    $ oc apply -f endpoint-secret.yaml
    ```
    
    
1. Edit the `    VirtualMachine` manifest, specifying the data source for the virtual machine image you want to import, and save it as `    vm-fedora-datavolume.yaml` :
    
    
    ```
    apiVersion: kubevirt.io/v1    kind: VirtualMachine    metadata:      creationTimestamp: null      labels:        kubevirt.io/vm: vm-fedora-datavolume      name: vm-fedora-datavolume<span id="CO44-1"><!--Empty--></span><span class="callout">1</span>spec:      dataVolumeTemplates:      - metadata:          creationTimestamp: null          name: fedora-dv<span id="CO44-2"><!--Empty--></span><span class="callout">2</span>spec:          storage:            resources:              requests:                storage: 10Gi            storageClassName: local          source:            http:<span id="CO44-3"><!--Empty--></span><span class="callout">3</span>url: "https://mirror.arizona.edu/fedora/linux/releases/35/Cloud/x86_64/images/Fedora-Cloud-Base-35-1.2.x86_64.qcow2"<span id="CO44-4"><!--Empty--></span><span class="callout">4</span>secretRef: endpoint-secret<span id="CO44-5"><!--Empty--></span><span class="callout">5</span>certConfigMap: ""<span id="CO44-6"><!--Empty--></span><span class="callout">6</span>status: {}      running: true      template:        metadata:          creationTimestamp: null          labels:            kubevirt.io/vm: vm-fedora-datavolume        spec:          domain:            devices:              disks:              - disk:                  bus: virtio                name: datavolumedisk1            machine:              type: ""            resources:              requests:                memory: 1.5Gi          terminationGracePeriodSeconds: 180          volumes:          - dataVolume:              name: fedora-dv            name: datavolumedisk1    status: {}
    ```
    
    
    
1. Create the virtual machine:
    
    
    ```
    $ oc create -f vm-fedora-datavolume.yaml
    ```
    
    Note
    The `    oc create` command creates the data volume and the virtual machine. The CDI controller creates an underlying PVC with the correct annotation and the import process begins. When the import is complete, the data volume status changes to `    Succeeded` . You can start the virtual machine.
    
    Data volume provisioning happens in the background, so there is no need to monitor the process.
    
    
    
    


 **Verification** 

1. The importer pod downloads the virtual machine image or container disk from the specified URL and stores it on the provisioned PV. View the status of the importer pod by running the following command:
    
    
    ```
    $ oc get pods
    ```
    
    
1. Monitor the data volume until its status is `    Succeeded` by running the following command:
    
    
    ```
    $ oc describe dv fedora-dv<span id="CO45-1"><!--Empty--></span><span class="callout">1</span>
    ```
    
    
1. Verify that provisioning is complete and that the virtual machine has started by accessing its serial console:
    
    
    ```
    $ virtctl console vm-fedora-datavolume
    ```
    
    


#### 9.17.2.5. Additional resources




-  [Configure preallocation mode](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/virtualization/#virt-using-preallocation-for-datavolumes) to improve write performance for data volume operations.


### 9.17.3. Importing virtual machine images into block storage with data volumes




You can import an existing virtual machine image into your OpenShift Container Platform cluster. OpenShift Virtualization uses data volumes to automate the import of data and the creation of an underlying persistent volume claim (PVC).

Important
When you import a disk image into a PVC, the disk image is expanded to use the full storage capacity that is requested in the PVC. To use this space, the disk partitions and file system(s) in the virtual machine might need to be expanded.

The resizing procedure varies based on the operating system that is installed on the virtual machine. See the operating system documentation for details.



#### 9.17.3.1. Prerequisites




- If you require scratch space according to the [CDI supported operations matrix](#virt-cdi-supported-operations-matrix_virt-importing-virtual-machine-images-datavolumes-block) , you must first [define a storage class or prepare CDI scratch space](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/virtualization/#virt-defining-storageclass_virt-preparing-cdi-scratch-space) for this operation to complete successfully.


#### 9.17.3.2. About data volumes




 `DataVolume` objects are custom resources that are provided by the Containerized Data Importer (CDI) project. Data volumes orchestrate import, clone, and upload operations that are associated with an underlying persistent volume claim (PVC). Data volumes are integrated with OpenShift Virtualization, and they prevent a virtual machine from being started before the PVC has been prepared.

#### 9.17.3.3. About block persistent volumes




A block persistent volume (PV) is a PV that is backed by a raw block device. These volumes do not have a file system and can provide performance benefits for virtual machines by reducing overhead.

Raw block volumes are provisioned by specifying `volumeMode: Block` in the PV and persistent volume claim (PVC) specification.

#### 9.17.3.4. Creating a local block persistent volume




Create a local block persistent volume (PV) on a node by populating a file and mounting it as a loop device. You can then reference this loop device in a PV manifest as a `Block` volume and use it as a block device for a virtual machine image.

 **Procedure** 

1. Log in as `    root` to the node on which to create the local PV. This procedure uses `    node01` for its examples.
1. Create a file and populate it with null characters so that it can be used as a block device. The following example creates a file `    loop10` with a size of 2Gb (20 100Mb blocks):
    
    
    ```
    $ dd if=/dev/zero of=&lt;loop10&gt; bs=100M count=20
    ```
    
    
1. Mount the `    loop10` file as a loop device.
    
    
    ```
    $ losetup &lt;/dev/loop10&gt;d3 &lt;loop10&gt;<span id="CO46-1"><!--Empty--></span><span class="callout">1</span><span id="CO46-2"><!--Empty--></span><span class="callout">2</span>
    ```
    
    
1. Create a `    PersistentVolume` manifest that references the mounted loop device.
    
    
    ```
    kind: PersistentVolume    apiVersion: v1    metadata:      name: &lt;local-block-pv10&gt;      annotations:    spec:      local:        path: &lt;/dev/loop10&gt;<span id="CO47-1"><!--Empty--></span><span class="callout">1</span>capacity:        storage: &lt;2Gi&gt;      volumeMode: Block<span id="CO47-2"><!--Empty--></span><span class="callout">2</span>storageClassName: local<span id="CO47-3"><!--Empty--></span><span class="callout">3</span>accessModes:        - ReadWriteOnce      persistentVolumeReclaimPolicy: Delete      nodeAffinity:        required:          nodeSelectorTerms:          - matchExpressions:            - key: kubernetes.io/hostname              operator: In              values:              - &lt;node01&gt;<span id="CO47-4"><!--Empty--></span><span class="callout">4</span>
    ```
    
    
1. Create the block PV.
    
    
    ```
    # oc create -f &lt;local-block-pv10.yaml&gt;<span id="CO48-1"><!--Empty--></span><span class="callout">1</span>
    ```
    
    


#### 9.17.3.5. Importing a virtual machine image into block storage by using a data volume




You can import a virtual machine image into block storage by using a data volume. You reference the data volume in a `VirtualMachine` manifest before you create a virtual machine.

 **Prerequisites** 

- A virtual machine disk image in RAW, ISO, or QCOW2 format, optionally compressed by using `    xz` or `    gz` .
- An HTTP or HTTPS endpoint where the image is hosted, along with any authentication credentials needed to access the data source.


 **Procedure** 

1. If your data source requires authentication, create a `    Secret` manifest, specifying the data source credentials, and save it as `    endpoint-secret.yaml` :
    
    
    ```
    apiVersion: v1    kind: Secret    metadata:      name: endpoint-secret<span id="CO49-1"><!--Empty--></span><span class="callout">1</span>labels:        app: containerized-data-importer    type: Opaque    data:      accessKeyId: ""<span id="CO49-2"><!--Empty--></span><span class="callout">2</span>secretKey:   ""<span id="CO49-3"><!--Empty--></span><span class="callout">3</span>
    ```
    
    
1. Apply the `    Secret` manifest:
    
    
    ```
    $ oc apply -f endpoint-secret.yaml
    ```
    
    
1. Create a `    DataVolume` manifest, specifying the data source for the virtual machine image and `    Block` for `    storage.volumeMode` .
    
    
    ```
    apiVersion: cdi.kubevirt.io/v1beta1    kind: DataVolume    metadata:      name: import-pv-datavolume<span id="CO50-1"><!--Empty--></span><span class="callout">1</span>spec:      storageClassName: local<span id="CO50-2"><!--Empty--></span><span class="callout">2</span>source:          http:            url: "https://mirror.arizona.edu/fedora/linux/releases/35/Cloud/x86_64/images/Fedora-Cloud-Base-35-1.2.x86_64.qcow2"<span id="CO50-3"><!--Empty--></span><span class="callout">3</span>secretRef: endpoint-secret<span id="CO50-4"><!--Empty--></span><span class="callout">4</span>storage:        volumeMode: Block<span id="CO50-5"><!--Empty--></span><span class="callout">5</span>resources:          requests:            storage: 10Gi
    ```
    
    
1. Create the data volume to import the virtual machine image:
    
    
    ```
    $ oc create -f import-pv-datavolume.yaml
    ```
    
    


You can reference this data volume in a `VirtualMachine` manifest before you create a virtual machine.

#### 9.17.3.6. CDI supported operations matrix




This matrix shows the supported CDI operations for content types against endpoints, and which of these operations requires scratch space.

| Content types | HTTP | HTTPS | HTTP basic auth | Registry | Upload |
| --- | --- | --- | --- | --- | --- |
| KubeVirt (QCOW2) | ✓ QCOW2    
✓ GZ*    
✓ XZ* | ✓ QCOW2**    
✓ GZ*    
✓ XZ* | ✓ QCOW2    
✓ GZ*    
✓ XZ* | ✓ QCOW2*    
□ GZ    
□ XZ | ✓ QCOW2*    
✓ GZ*    
✓ XZ* |
| KubeVirt (RAW) | ✓ RAW    
✓ GZ    
✓ XZ | ✓ RAW    
✓ GZ    
✓ XZ | ✓ RAW    
✓ GZ    
✓ XZ | ✓ RAW*    
□ GZ    
□ XZ | ✓ RAW*    
✓ GZ*    
✓ XZ* |


✓ Supported operation

□ Unsupported operation

* Requires scratch space

** Requires scratch space if a custom certificate authority is required

Note
CDI now uses the OpenShift Container Platform [cluster-wide proxy configuration](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/networking/#enable-cluster-wide-proxy) .



#### 9.17.3.7. Additional resources




-  [Configure preallocation mode](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/virtualization/#virt-using-preallocation-for-datavolumes) to improve write performance for data volume operations.


