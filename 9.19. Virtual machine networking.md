## 9.19. Virtual machine networking




### 9.19.1. Configuring the virtual machine for the default pod network




You can connect a virtual machine to the default internal pod network by configuring its network interface to use the `masquerade` binding mode

Note
Traffic on the virtual Network Interface Cards (vNICs) that are attached to the default pod network is interrupted during live migration.



#### 9.19.1.1. Configuring masquerade mode from the command line




You can use masquerade mode to hide a virtual machine’s outgoing traffic behind the pod IP address. Masquerade mode uses Network Address Translation (NAT) to connect virtual machines to the pod network backend through a Linux bridge.

Enable masquerade mode and allow traffic to enter the virtual machine by editing your virtual machine configuration file.

 **Prerequisites** 

- The virtual machine must be configured to use DHCP to acquire IPv4 addresses. The examples below are configured to use DHCP.


 **Procedure** 

1. Edit the `    interfaces` spec of your virtual machine configuration file:
    
    
    ```
    kind: VirtualMachine    spec:      domain:        devices:          interfaces:            - name: default              masquerade: {}<span id="CO61-1"><!--Empty--></span><span class="callout">1</span>ports:<span id="CO61-2"><!--Empty--></span><span class="callout">2</span>- port: 80      networks:      - name: default        pod: {}
    ```
    
    Note
    Ports 49152 and 49153 are reserved for use by the libvirt platform and all other incoming traffic to these ports is dropped.
    
    
    
    
1. Create the virtual machine:
    
    
    ```
    $ oc create -f &lt;vm-name&gt;.yaml
    ```
    
    


#### 9.19.1.2. Configuring masquerade mode with dual-stack (IPv4 and IPv6)




You can configure a new virtual machine (VM) to use both IPv6 and IPv4 on the default pod network by using cloud-init.

The `Network.pod.vmIPv6NetworkCIDR` field in the virtual machine instance configuration determines the static IPv6 address of the VM and the gateway IP address. These are used by the virt-launcher pod to route IPv6 traffic to the virtual machine and are not used externally. The `Network.pod.vmIPv6NetworkCIDR` field specifies an IPv6 address block in Classless Inter-Domain Routing (CIDR) notation. The default value is `fd10:0:2::2/120` . You can edit this value based on your network requirements.

When the virtual machine is running, incoming and outgoing traffic for the virtual machine is routed to both the IPv4 address and the unique IPv6 address of the virt-launcher pod. The virt-launcher pod then routes the IPv4 traffic to the DHCP address of the virtual machine, and the IPv6 traffic to the statically set IPv6 address of the virtual machine.

 **Prerequisites** 

- The OpenShift Container Platform cluster must use the OVN-Kubernetes Container Network Interface (CNI) network provider configured for dual-stack.


 **Procedure** 

1. In a new virtual machine configuration, include an interface with `    masquerade` and configure the IPv6 address and default gateway by using cloud-init.
    
    
    ```
    apiVersion: kubevirt.io/v1    kind: VirtualMachine    metadata:      name: example-vm-ipv6    ...              interfaces:                - name: default                  masquerade: {}<span id="CO62-1"><!--Empty--></span><span class="callout">1</span>ports:                    - port: 80<span id="CO62-2"><!--Empty--></span><span class="callout">2</span>networks:          - name: default            pod: {}          volumes:          - cloudInitNoCloud:              networkData: |                version: 2                ethernets:                  eth0:                    dhcp4: true                    addresses: [ fd10:0:2::2/120 ]<span id="CO62-3"><!--Empty--></span><span class="callout">3</span>gateway6: fd10:0:2::1<span id="CO62-4"><!--Empty--></span><span class="callout">4</span>
    ```
    
    
1. Create the virtual machine in the namespace:
    
    
    ```
    $ oc create -f example-vm-ipv6.yaml
    ```
    
    


 **Verification** 

- To verify that IPv6 has been configured, start the virtual machine and view the interface status of the virtual machine instance to ensure it has an IPv6 address:


```
$ oc get vmi &lt;vmi-name&gt; -o jsonpath="{.status.interfaces[*].ipAddresses}"
```

### 9.19.2. Creating a service to expose a virtual machine




You can expose a virtual machine within the cluster or outside the cluster by using a `Service` object.

#### 9.19.2.1. About services




A Kubernetes _service_ is an abstract way to expose an application running on a set of pods as a network service. Services allow your applications to receive traffic. Services can be exposed in different ways by specifying a `spec.type` in the `Service` object:

##### 9.19.2.1.1. Dual-stack support




If IPv4 and IPv6 dual-stack networking is enabled for your cluster, you can create a service that uses IPv4, IPv6, or both, by defining the `spec.ipFamilyPolicy` and the `spec.ipFamilies` fields in the `Service` object.

The `spec.ipFamilyPolicy` field can be set to one of the following values:

You can define which IP family to use for single-stack or define the order of IP families for dual-stack by setting the `spec.ipFamilies` field to one of the following array values:

-  `    [IPv4]` 
-  `    [IPv6]` 
-  `    [IPv4, IPv6]` 
-  `    [IPv6, IPv4]` 


#### 9.19.2.2. Exposing a virtual machine as a service




Create a `ClusterIP` , `NodePort` , or `LoadBalancer` service to connect to a running virtual machine (VM) from within or outside the cluster.

 **Procedure** 

1. Edit the `    VirtualMachine` manifest to add the label for service creation:
    
    
    ```
    apiVersion: kubevirt.io/v1    kind: VirtualMachine    metadata:      name: vm-ephemeral      namespace: example-namespace    spec:      running: false      template:        metadata:          labels:            special: key<span id="CO63-1"><!--Empty--></span><span class="callout">1</span># ...
    ```
    
    Note
    Labels on a virtual machine are passed through to the pod. The `    special: key` label must match the label in the `    spec.selector` attribute of the `    Service` manifest.
    
    
    
    
1. Save the `    VirtualMachine` manifest file to apply your changes.
1. Create a `    Service` manifest to expose the VM:
    
    
    ```
    apiVersion: v1    kind: Service    metadata:      name: vmservice<span id="CO64-1"><!--Empty--></span><span class="callout">1</span>namespace: example-namespace<span id="CO64-2"><!--Empty--></span><span class="callout">2</span>spec:      externalTrafficPolicy: Cluster<span id="CO64-3"><!--Empty--></span><span class="callout">3</span>ports:      - nodePort: 30000<span id="CO64-4"><!--Empty--></span><span class="callout">4</span>port: 27017        protocol: TCP        targetPort: 22<span id="CO64-5"><!--Empty--></span><span class="callout">5</span>selector:        special: key<span id="CO64-6"><!--Empty--></span><span class="callout">6</span>type: NodePort<span id="CO64-7"><!--Empty--></span><span class="callout">7</span>
    ```
    
    
1. Save the `    Service` manifest file.
1. Create the service by running the following command:
    
    
    ```
    $ oc create -f &lt;service_name&gt;.yaml
    ```
    
    
1. Start the VM. If the VM is already running, restart it.


 **Verification** 

1. Query the `    Service` object to verify that it is available:
    
    
    ```
    $ oc get service -n example-namespace
    ```
    
     **Example output for `    ClusterIP` service** 
    
    
    ```
    NAME        TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)     AGE    vmservice   ClusterIP   172.30.3.149   &lt;none&gt;        27017/TCP   2m
    ```
    
    
     **Example output for `    NodePort` service** 
    
    
    ```
    NAME        TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)            AGE    vmservice   NodePort    172.30.232.73   &lt;none&gt;       27017:30000/TCP    5m
    ```
    
    
     **Example output for `    LoadBalancer` service** 
    
    
    ```
    NAME        TYPE            CLUSTER-IP     EXTERNAL-IP                    PORT(S)           AGE    vmservice   LoadBalancer    172.30.27.5   172.29.10.235,172.29.10.235     27017:31829/TCP   5s
    ```
    
    
    
1. Choose the appropriate method to connect to the virtual machine:
    
    
    - For a `        ClusterIP` service, connect to the VM from within the cluster by using the service IP address and the service port. For example:
        
        
        ```
        $ ssh fedora@172.30.3.149 -p 27017
        ```
        
        
    - For a `        NodePort` service, connect to the VM by specifying the node IP address and the node port outside the cluster network. For example:
        
        
        ```
        $ ssh fedora@$NODE_IP -p 30000
        ```
        
        
    - For a `        LoadBalancer` service, use the `        vinagre` client to connect to your virtual machine by using the public IP address and port. External ports are dynamically allocated.
    


#### 9.19.2.3. Additional resources




-  [Configuring ingress cluster traffic using a NodePort](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/networking/#configuring-ingress-cluster-traffic-nodeport) 
-  [Configuring ingress cluster traffic using a load balancer](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/networking/#configuring-ingress-cluster-traffic-load-balancer) 


### 9.19.3. Connecting a virtual machine to a Linux bridge network




By default, OpenShift Virtualization is installed with a single, internal pod network.

You must create a Linux bridge network attachment definition (NAD) in order to connect to additional networks.

To attach a virtual machine to an additional network:

1. Create a Linux bridge node network configuration policy.
1. Create a Linux bridge network attachment definition.
1. Configure the virtual machine, enabling the virtual machine to recognize the network attachment definition.


For more information about scheduling, interface types, and other node networking activities, see the [node networking](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/networking/#k8s-nmstate-updating-node-network-config) section.

#### 9.19.3.1. Connecting to the network through the network attachment definition




##### 9.19.3.1.1. Creating a Linux bridge node network configuration policy




Use a `NodeNetworkConfigurationPolicy` manifest YAML file to create the Linux bridge.

 **Prerequisites** 

- You have installed the Kubernetes NMState Operator.


 **Procedure** 

- Create the `    NodeNetworkConfigurationPolicy` manifest. This example includes sample values that you must replace with your own information.
    
    
    ```
    apiVersion: nmstate.io/v1    kind: NodeNetworkConfigurationPolicy    metadata:      name: br1-eth1-policy<span id="CO65-1"><!--Empty--></span><span class="callout">1</span>spec:      desiredState:        interfaces:          - name: br1<span id="CO65-2"><!--Empty--></span><span class="callout">2</span>description: Linux bridge with eth1 as a port<span id="CO65-3"><!--Empty--></span><span class="callout">3</span>type: linux-bridge<span id="CO65-4"><!--Empty--></span><span class="callout">4</span>state: up<span id="CO65-5"><!--Empty--></span><span class="callout">5</span>ipv4:              enabled: false<span id="CO65-6"><!--Empty--></span><span class="callout">6</span>bridge:              options:                stp:                  enabled: false<span id="CO65-7"><!--Empty--></span><span class="callout">7</span>port:                - name: eth1<span id="CO65-8"><!--Empty--></span><span class="callout">8</span>
    ```
    
    


#### 9.19.3.2. Creating a Linux bridge network attachment definition




Warning
Configuring IP address management (IPAM) in a network attachment definition for virtual machines is not supported.



##### 9.19.3.2.1. Creating a Linux bridge network attachment definition in the web console




Network administrators can create network attachment definitions to provide layer-2 networking to pods and virtual machines.

 **Procedure** 

1. In the web console, click **Networking** → **Network Attachment Definitions** .
1. Click **Create Network Attachment Definition** .
    
    Note
    The network attachment definition must be in the same namespace as the pod or virtual machine.
    
    
    
    
1. Enter a unique **Name** and optional **Description** .
1. Click the **Network Type** list and select **CNV Linux bridge** .
1. Enter the name of the bridge in the **Bridge Name** field.
1. Optional: If the resource has VLAN IDs configured, enter the ID numbers in the **VLAN Tag Number** field.
1. Optional: Select **MAC Spoof Check** to enable MAC spoof filtering. This feature provides security against a MAC spoofing attack by allowing only a single MAC address to exit the pod.
1. Click **Create** .
    
    Note
    A Linux bridge network attachment definition is the most efficient method for connecting a virtual machine to a VLAN.
    
    
    
    


##### 9.19.3.2.2. Creating a Linux bridge network attachment definition in the CLI




As a network administrator, you can configure a network attachment definition of type `cnv-bridge` to provide layer-2 networking to pods and virtual machines.

 **Prerequisites** 

- The node must support nftables and the `    nft` binary must be deployed to enable MAC spoof check.


 **Procedure** 

1. Create a network attachment definition in the same namespace as the virtual machine.
1. Add the virtual machine to the network attachment definition, as in the following example:
    
    
    ```
    apiVersion: "k8s.cni.cncf.io/v1"    kind: NetworkAttachmentDefinition    metadata:      name: &lt;bridge-network&gt;<span id="CO66-1"><!--Empty--></span><span class="callout">1</span>annotations:        k8s.v1.cni.cncf.io/resourceName: bridge.network.kubevirt.io/&lt;bridge-interface&gt;<span id="CO66-2"><!--Empty--></span><span class="callout">2</span>spec:      config: '{        "cniVersion": "0.3.1",        "name": "&lt;bridge-network&gt;",<span id="CO66-3"><!--Empty--></span><span class="callout">3</span>"type": "cnv-bridge",<span id="CO66-4"><!--Empty--></span><span class="callout">4</span>"bridge": "&lt;bridge-interface&gt;",<span id="CO66-5"><!--Empty--></span><span class="callout">5</span>"macspoofchk": true,<span id="CO66-6"><!--Empty--></span><span class="callout">6</span>"vlan": 1<span id="CO66-7"><!--Empty--></span><span class="callout">7</span>}'
    ```
    
    Note
    A Linux bridge network attachment definition is the most efficient method for connecting a virtual machine to a VLAN.
    
    
    
    
1. Create the network attachment definition:
    
    
    ```
    $ oc create -f &lt;network-attachment-definition.yaml&gt;<span id="CO67-1"><!--Empty--></span><span class="callout">1</span>
    ```
    
    


 **Verification** 

- Verify that the network attachment definition was created by running the following command:
    
    
    ```
    $ oc get network-attachment-definition &lt;bridge-network&gt;
    ```
    
    


#### 9.19.3.3. Configuring the virtual machine for a Linux bridge network




##### 9.19.3.3.1. Creating a NIC for a virtual machine in the web console




Create and attach additional NICs to a virtual machine from the web console.

 **Prerequisites** 

- A network attachment definition must be available.


 **Procedure** 

1. In the correct project in the OpenShift Container Platform console, click **Virtualization** → **VirtualMachines** from the side menu.
1. Select a virtual machine to open the **VirtualMachine details** page.
1. Click the **Network Interfaces** tab to view the NICs already attached to the virtual machine.
1. Click **Add Network Interface** to create a new slot in the list.
1. Select a network attachment definition from the **Network** list for the additional network.
1. Fill in the **Name** , **Model** , **Type** , and **MAC Address** for the new NIC.
1. Click **Save** to save and attach the NIC to the virtual machine.


##### 9.19.3.3.2. Networking fields




| Name | Description |
| --- | --- |
| Name | Name for the network interface controller. |
| Model | Indicates the model of the network interface controller. Supported values are **e1000e** and **virtio** . |
| Network | List of available network attachment definitions. |
| Type | List of available binding methods. Select the binding method suitable for the network interface:

- Default pod network: `    masquerade` 
- Linux bridge network: `    bridge` 
- SR-IOV network: `    SR-IOV` |
| MAC Address | MAC address for the network interface controller. If a MAC address is not specified, one is assigned automatically. |


##### 9.19.3.3.3. Attaching a virtual machine to an additional network in the CLI




Attach a virtual machine to an additional network by adding a bridge interface and specifying a network attachment definition in the virtual machine configuration.

This procedure uses a YAML file to demonstrate editing the configuration and applying the updated file to the cluster. You can alternatively use the `oc edit &lt;object&gt; &lt;name&gt;` command to edit an existing virtual machine.

 **Prerequisites** 

- Shut down the virtual machine before editing the configuration. If you edit a running virtual machine, you must restart the virtual machine for the changes to take effect.


 **Procedure** 

1. Create or edit a configuration of a virtual machine that you want to connect to the bridge network.
1. Add the bridge interface to the `    spec.template.spec.domain.devices.interfaces` list and the network attachment definition to the `    spec.template.spec.networks` list. This example adds a bridge interface called `    bridge-net` that connects to the `    a-bridge-network` network attachment definition:
    
    
    ```
    apiVersion: kubevirt.io/v1    kind: VirtualMachine    metadata:        name: &lt;example-vm&gt;    spec:      template:        spec:          domain:            devices:              interfaces:                - masquerade: {}                  name: &lt;default&gt;                - bridge: {}                  name: &lt;bridge-net&gt;<span id="CO68-1"><!--Empty--></span><span class="callout">1</span>...          networks:            - name: &lt;default&gt;              pod: {}            - name: &lt;bridge-net&gt;<span id="CO68-2"><!--Empty--></span><span class="callout">2</span>multus:                networkName: &lt;network-namespace&gt;/&lt;a-bridge-network&gt;<span id="CO68-3"><!--Empty--></span><span class="callout">3</span>...
    ```
    
    
1. Apply the configuration:
    
    
    ```
    $ oc apply -f &lt;example-vm.yaml&gt;
    ```
    
    
1. Optional: If you edited a running virtual machine, you must restart it for the changes to take effect.


### 9.19.4. Connecting a virtual machine to an SR-IOV network




You can connect a virtual machine (VM) to a Single Root I/O Virtualization (SR-IOV) network by performing the following steps:

1. Configure an SR-IOV network device.
1. Configure an SR-IOV network.
1. Connect the VM to the SR-IOV network.


#### 9.19.4.1. Prerequisites




- You must have [enabled global SR-IOV and VT-d settings in the firmware for the host](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/scalability_and_performance/#ztp-du-configuring-host-firmware-requirements_sno-configure-for-vdu) .
- You must have [installed the SR-IOV Network Operator](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/networking/#installing-sriov-operator) .


#### 9.19.4.2. Configuring SR-IOV network devices




The SR-IOV Network Operator adds the `SriovNetworkNodePolicy.sriovnetwork.openshift.io` CustomResourceDefinition to OpenShift Container Platform. You can configure an SR-IOV network device by creating a SriovNetworkNodePolicy custom resource (CR).

Note
When applying the configuration specified in a `SriovNetworkNodePolicy` object, the SR-IOV Operator might drain the nodes, and in some cases, reboot nodes.

It might take several minutes for a configuration change to apply.



 **Prerequisites** 

- You installed the OpenShift CLI ( `    oc` ).
- You have access to the cluster as a user with the `    cluster-admin` role.
- You have installed the SR-IOV Network Operator.
- You have enough available nodes in your cluster to handle the evicted workload from drained nodes.
- You have not selected any control plane nodes for SR-IOV network device configuration.


 **Procedure** 

1. Create an `    SriovNetworkNodePolicy` object, and then save the YAML in the `    &lt;name&gt;-sriov-node-network.yaml` file. Replace `    &lt;name&gt;` with the name for this configuration.
    
    
    ```
    apiVersion: sriovnetwork.openshift.io/v1    kind: SriovNetworkNodePolicy    metadata:      name: &lt;name&gt;<span id="CO69-1"><!--Empty--></span><span class="callout">1</span>namespace: openshift-sriov-network-operator<span id="CO69-2"><!--Empty--></span><span class="callout">2</span>spec:      resourceName: &lt;sriov_resource_name&gt;<span id="CO69-3"><!--Empty--></span><span class="callout">3</span>nodeSelector:        feature.node.kubernetes.io/network-sriov.capable: "true"<span id="CO69-4"><!--Empty--></span><span class="callout">4</span>priority: &lt;priority&gt;<span id="CO69-5"><!--Empty--></span><span class="callout">5</span>mtu: &lt;mtu&gt;<span id="CO69-6"><!--Empty--></span><span class="callout">6</span>numVfs: &lt;num&gt;<span id="CO69-7"><!--Empty--></span><span class="callout">7</span>nicSelector:<span id="CO69-8"><!--Empty--></span><span class="callout">8</span>vendor: "&lt;vendor_code&gt;"<span id="CO69-9"><!--Empty--></span><span class="callout">9</span>deviceID: "&lt;device_id&gt;"<span id="CO69-10"><!--Empty--></span><span class="callout">10</span>pfNames: ["&lt;pf_name&gt;", ...]<span id="CO69-11"><!--Empty--></span><span class="callout">11</span>rootDevices: ["&lt;pci_bus_id&gt;", "..."]<span id="CO69-12"><!--Empty--></span><span class="callout">12</span>deviceType: vfio-pci<span id="CO69-13"><!--Empty--></span><span class="callout">13</span>isRdma: false<span id="CO69-14"><!--Empty--></span><span class="callout">14</span>
    ```
    
    Note
    If `    isRDMA` flag is set to `    true` , you can continue to use the RDMA enabled VF as a normal network device. A device can be used in either mode.
    
    
    
    
1. Optional: Label the SR-IOV capable cluster nodes with `    SriovNetworkNodePolicy.Spec.NodeSelector` if they are not already labeled. For more information about labeling nodes, see "Understanding how to update labels on nodes".
1. Create the `    SriovNetworkNodePolicy` object:
    
    
    ```
    $ oc create -f &lt;name&gt;-sriov-node-network.yaml
    ```
    
    where `    &lt;name&gt;` specifies the name for this configuration.
    
    After applying the configuration update, all the pods in `    sriov-network-operator` namespace transition to the `    Running` status.
    
    
1. To verify that the SR-IOV network device is configured, enter the following command. Replace `    &lt;node_name&gt;` with the name of a node with the SR-IOV network device that you just configured.
    
    
    ```
    $ oc get sriovnetworknodestates -n openshift-sriov-network-operator &lt;node_name&gt; -o jsonpath='{.status.syncStatus}'
    ```
    
    


#### 9.19.4.3. Configuring SR-IOV additional network




You can configure an additional network that uses SR-IOV hardware by creating an `SriovNetwork` object.

When you create an `SriovNetwork` object, the SR-IOV Network Operator automatically creates a `NetworkAttachmentDefinition` object.

Note
Do not modify or delete an `SriovNetwork` object if it is attached to pods or virtual machines in a `running` state.



 **Prerequisites** 

- Install the OpenShift CLI ( `    oc` ).
- Log in as a user with `    cluster-admin` privileges.


 **Procedure** 

1. Create the following `    SriovNetwork` object, and then save the YAML in the `    &lt;name&gt;-sriov-network.yaml` file. Replace `    &lt;name&gt;` with a name for this additional network.


```
apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetwork
metadata:
  name: &lt;name&gt;<span id="CO70-1"><!--Empty--></span><span class="callout">1</span>namespace: openshift-sriov-network-operator<span id="CO70-2"><!--Empty--></span><span class="callout">2</span>spec:
  resourceName: &lt;sriov_resource_name&gt;<span id="CO70-3"><!--Empty--></span><span class="callout">3</span>networkNamespace: &lt;target_namespace&gt;<span id="CO70-4"><!--Empty--></span><span class="callout">4</span>vlan: &lt;vlan&gt;<span id="CO70-5"><!--Empty--></span><span class="callout">5</span>spoofChk: "&lt;spoof_check&gt;"<span id="CO70-6"><!--Empty--></span><span class="callout">6</span>linkState: &lt;link_state&gt;<span id="CO70-7"><!--Empty--></span><span class="callout">7</span>maxTxRate: &lt;max_tx_rate&gt;<span id="CO70-8"><!--Empty--></span><span class="callout">8</span>minTxRate: &lt;min_rx_rate&gt;<span id="CO70-9"><!--Empty--></span><span class="callout">9</span>vlanQoS: &lt;vlan_qos&gt;<span id="CO70-10"><!--Empty--></span><span class="callout">10</span>trust: "&lt;trust_vf&gt;"<span id="CO70-11"><!--Empty--></span><span class="callout">11</span>capabilities: &lt;capabilities&gt;<span id="CO70-12"><!--Empty--></span><span class="callout">12</span>
```

1. To create the object, enter the following command. Replace `    &lt;name&gt;` with a name for this additional network.
    
    
    ```
    $ oc create -f &lt;name&gt;-sriov-network.yaml
    ```
    
    
1. Optional: To confirm that the `    NetworkAttachmentDefinition` object associated with the `    SriovNetwork` object that you created in the previous step exists, enter the following command. Replace `    &lt;namespace&gt;` with the namespace you specified in the `    SriovNetwork` object.
    
    
    ```
    $ oc get net-attach-def -n &lt;namespace&gt;
    ```
    
    


#### 9.19.4.4. Connecting a virtual machine to an SR-IOV network




You can connect the virtual machine (VM) to the SR-IOV network by including the network details in the VM configuration.

 **Procedure** 

1. Include the SR-IOV network details in the `    spec.domain.devices.interfaces` and `    spec.networks` of the VM configuration:
    
    
    ```
    kind: VirtualMachine    ...    spec:      domain:        devices:          interfaces:          - name: &lt;default&gt;<span id="CO71-1"><!--Empty--></span><span class="callout">1</span>masquerade: {}<span id="CO71-2"><!--Empty--></span><span class="callout">2</span>- name: &lt;nic1&gt;<span id="CO71-3"><!--Empty--></span><span class="callout">3</span>sriov: {}      networks:      - name: &lt;default&gt;<span id="CO71-4"><!--Empty--></span><span class="callout">4</span>pod: {}      - name: &lt;nic1&gt;<span id="CO71-5"><!--Empty--></span><span class="callout">5</span>multus:            networkName: &lt;sriov-network&gt;<span id="CO71-6"><!--Empty--></span><span class="callout">6</span>...
    ```
    
    
1. Apply the virtual machine configuration:
    
    
    ```
    $ oc apply -f &lt;vm-sriov.yaml&gt;<span id="CO72-1"><!--Empty--></span><span class="callout">1</span>
    ```
    
    


### 9.19.5. Connecting a virtual machine to a service mesh




OpenShift Virtualization is now integrated with OpenShift Service Mesh. You can monitor, visualize, and control traffic between pods that run virtual machine workloads on the default pod network with IPv4.

#### 9.19.5.1. Prerequisites




- You must have [installed the Service Mesh Operator](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/service_mesh/#installing-ossm) and [deployed the service mesh control plane](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/service_mesh/#ossm-create-smcp) .
- You must have added the namespace where the virtual machine is created to the [service mesh member roll](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/service_mesh/#ossm-create-mesh) .
- You must use the `    masquerade` binding method for the default pod network.


#### 9.19.5.2. Configuring a virtual machine for the service mesh




To add a virtual machine (VM) workload to a service mesh, enable automatic sidecar injection in the VM configuration file by setting the `sidecar.istio.io/inject` annotation to `true` . Then expose your VM as a service to view your application in the mesh.

 **Prerequisites** 

- To avoid port conflicts, do not use ports used by the Istio sidecar proxy. These include ports 15000, 15001, 15006, 15008, 15020, 15021, and 15090.


 **Procedure** 

1. Edit the VM configuration file to add the `    sidecar.istio.io/inject: "true"` annotation.
    
     **Example configuration file** 
    
    
    ```
    apiVersion: kubevirt.io/v1    kind: VirtualMachine    metadata:      labels:        kubevirt.io/vm: vm-istio      name: vm-istio    spec:      runStrategy: Always      template:        metadata:          labels:            kubevirt.io/vm: vm-istio            app: vm-istio<span id="CO73-1"><!--Empty--></span><span class="callout">1</span>annotations:            sidecar.istio.io/inject: "true"<span id="CO73-2"><!--Empty--></span><span class="callout">2</span>spec:          domain:            devices:              interfaces:              - name: default                masquerade: {}<span id="CO73-3"><!--Empty--></span><span class="callout">3</span>disks:              - disk:                  bus: virtio                name: containerdisk              - disk:                  bus: virtio                name: cloudinitdisk            resources:              requests:                memory: 1024M          networks:          - name: default            pod: {}          terminationGracePeriodSeconds: 180          volumes:          - containerDisk:              image: registry:5000/kubevirt/fedora-cloud-container-disk-demo:devel            name: containerdisk
    ```
    
    
    
1. Apply the VM configuration:
    
    
    ```
    $ oc apply -f &lt;vm_name&gt;.yaml<span id="CO74-1"><!--Empty--></span><span class="callout">1</span>
    ```
    
    
1. Create a `    Service` object to expose your VM to the service mesh.
    
    
    ```
    apiVersion: v1      kind: Service      metadata:        name: vm-istio      spec:        selector:          app: vm-istio<span id="CO75-1"><!--Empty--></span><span class="callout">1</span>ports:          - port: 8080            name: http            protocol: TCP
    ```
    
    
1. Create the service:
    
    
    ```
    $ oc create -f &lt;service_name&gt;.yaml<span id="CO76-1"><!--Empty--></span><span class="callout">1</span>
    ```
    
    


### 9.19.6. Configuring IP addresses for virtual machines




You can configure either dynamically or statically provisioned IP addresses for virtual machines.

 **Prerequisites** 

- The virtual machine must connect to an [external network](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/virtualization/#virt-attaching-vm-multiple-networks) .
- You must have a DHCP server available on the additional network to configure a dynamic IP for the virtual machine.


#### 9.19.6.1. Configuring an IP address for a new virtual machine using cloud-init




You can use cloud-init to configure an IP address when you create a virtual machine. The IP address can be dynamically or statically provisioned.

 **Procedure** 

- Create a virtual machine configuration and include the cloud-init network details in the `    spec.volumes.cloudInitNoCloud.networkData` field of the virtual machine configuration:
    
    
    1. To configure a dynamic IP, specify the interface name and the `        dhcp4` boolean:
        
        
        ```
        kind: VirtualMachine        spec:        ...          volumes:          - cloudInitNoCloud:              networkData: |                version: 2                ethernets:                  eth1:<span id="CO77-1"><!--Empty--></span><span class="callout">1</span>dhcp4: true<span id="CO77-2"><!--Empty--></span><span class="callout">2</span>
        ```
        
        
    1. To configure a static IP, specify the interface name and the IP address:
        
        
        ```
        kind: VirtualMachine        spec:        ...          volumes:          - cloudInitNoCloud:              networkData: |                version: 2                ethernets:                  eth1:<span id="CO78-1"><!--Empty--></span><span class="callout">1</span>addresses:                    - 10.10.10.14/24<span id="CO78-2"><!--Empty--></span><span class="callout">2</span>
        ```
        
        
    


### 9.19.7. Viewing the IP address of NICs on a virtual machine




You can view the IP address for a network interface controller (NIC) by using the web console or the `oc` client. The [QEMU guest agent](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/virtualization/#virt-installing-qemu-guest-agent) displays additional information about the virtual machine’s secondary networks.

#### 9.19.7.1. Prerequisites




- Install the QEMU guest agent on the virtual machine.


#### 9.19.7.2. Viewing the IP address of a virtual machine interface in the CLI




The network interface configuration is included in the `oc describe vmi &lt;vmi_name&gt;` command.

You can also view the IP address information by running `ip addr` on the virtual machine, or by running `oc get vmi &lt;vmi_name&gt; -o yaml` .

 **Procedure** 

- Use the `    oc describe` command to display the virtual machine interface configuration:
    
    
    ```
    $ oc describe vmi &lt;vmi_name&gt;
    ```
    
     **Example output** 
    
    
    ```
    ...    Interfaces:       Interface Name:  eth0       Ip Address:      10.244.0.37/24       Ip Addresses:         10.244.0.37/24         fe80::858:aff:fef4:25/64       Mac:             0a:58:0a:f4:00:25       Name:            default       Interface Name:  v2       Ip Address:      1.1.1.7/24       Ip Addresses:         1.1.1.7/24         fe80::f4d9:70ff:fe13:9089/64       Mac:             f6:d9:70:13:90:89       Interface Name:  v1       Ip Address:      1.1.1.1/24       Ip Addresses:         1.1.1.1/24         1.1.1.2/24         1.1.1.4/24         2001:de7:0:f101::1/64         2001:db8:0:f101::1/64         fe80::1420:84ff:fe10:17aa/64       Mac:             16:20:84:10:17:aa
    ```
    
    
    


#### 9.19.7.3. Viewing the IP address of a virtual machine interface in the web console




The IP information is displayed on the **VirtualMachine details** page for the virtual machine.

 **Procedure** 

1. In the OpenShift Container Platform console, click **Virtualization** → **VirtualMachines** from the side menu.
1. Select a virtual machine name to open the **VirtualMachine details** page.


The information for each attached NIC is displayed under **IP Address** on the **Details** tab.

### 9.19.8. Using a MAC address pool for virtual machines




The _KubeMacPool_ component provides a MAC address pool service for virtual machine NICs in a namespace.

#### 9.19.8.1. About KubeMacPool




KubeMacPool provides a MAC address pool per namespace and allocates MAC addresses for virtual machine NICs from the pool. This ensures that the NIC is assigned a unique MAC address that does not conflict with the MAC address of another virtual machine.

Virtual machine instances created from that virtual machine retain the assigned MAC address across reboots.

Note
KubeMacPool does not handle virtual machine instances created independently from a virtual machine.



KubeMacPool is enabled by default when you install OpenShift Virtualization. You can disable a MAC address pool for a namespace by adding the `mutatevirtualmachines.kubemacpool.io=ignore` label to the namespace. Re-enable KubeMacPool for the namespace by removing the label.

#### 9.19.8.2. Disabling a MAC address pool for a namespace in the CLI




Disable a MAC address pool for virtual machines in a namespace by adding the `mutatevirtualmachines.kubemacpool.io=ignore` label to the namespace.

 **Procedure** 

- Add the `    mutatevirtualmachines.kubemacpool.io=ignore` label to the namespace. The following example disables KubeMacPool for two namespaces, `    &lt;namespace1&gt;` and `    &lt;namespace2&gt;` :
    
    
    ```
    $ oc label namespace &lt;namespace1&gt; &lt;namespace2&gt; mutatevirtualmachines.kubemacpool.io=ignore
    ```
    
    


#### 9.19.8.3. Re-enabling a MAC address pool for a namespace in the CLI




If you disabled KubeMacPool for a namespace and want to re-enable it, remove the `mutatevirtualmachines.kubemacpool.io=ignore` label from the namespace.

Note
Earlier versions of OpenShift Virtualization used the label `mutatevirtualmachines.kubemacpool.io=allocate` to enable KubeMacPool for a namespace. This is still supported but redundant as KubeMacPool is now enabled by default.



 **Procedure** 

- Remove the KubeMacPool label from the namespace. The following example re-enables KubeMacPool for two namespaces, `    &lt;namespace1&gt;` and `    &lt;namespace2&gt;` :
    
    
    ```
    $ oc label namespace &lt;namespace1&gt; &lt;namespace2&gt; mutatevirtualmachines.kubemacpool.io-
    ```
    
    


