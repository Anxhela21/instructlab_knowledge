## 9.20. Virtual machine disks




### 9.20.1. Storage features




Use the following table to determine feature availability for local and shared persistent storage in OpenShift Virtualization.

#### 9.20.1.1. OpenShift Virtualization storage feature matrix





<span id="idm139667221081968"></span>
 **Table 9.6. OpenShift Virtualization storage feature matrix** 

|  | Virtual machine live migration | Host-assisted virtual machine disk cloning | Storage-assisted virtual machine disk cloning | Virtual machine snapshots |
| --- | --- | --- | --- | --- |
| OpenShift Data Foundation: RBD block-mode volumes | Yes | Yes | Yes | Yes |
| OpenShift Virtualization hostpath provisioner | No | Yes | No | No |
| Other multi-node writable storage | Yes | Yes | Yes | Yes |
| Other single-node writable storage | No | Yes | Yes | Yes |




1. PVCs must request a ReadWriteMany access mode.
1. Storage provider must support both Kubernetes and CSI snapshot APIs


Note
You cannot live migrate virtual machines that use:

- A storage class with ReadWriteOnce (RWO) access mode
- Passthrough features such as GPUs


Do not set the `evictionStrategy` field to `LiveMigrate` for these virtual machines.



### 9.20.2. Configuring local storage for virtual machines




You can configure local storage for virtual machines by using the hostpath provisioner (HPP).

When you install the OpenShift Virtualization Operator, the Hostpath Provisioner (HPP) Operator is automatically installed. The HPP is a local storage provisioner designed for OpenShift Virtualization that is created by the Hostpath Provisioner Operator. To use the HPP, you must create an HPP custom resource (CR).

#### 9.20.2.1. Creating a hostpath provisioner with a basic storage pool




You configure a hostpath provisioner (HPP) with a basic storage pool by creating an HPP custom resource (CR) with a `storagePools` stanza. The storage pool specifies the name and path used by the CSI driver.

 **Prerequisites** 

- The directories specified in `    spec.storagePools.path` must have read/write access.
- The storage pools must not be in the same partition as the operating system. Otherwise, the operating system partition might become filled to capacity, which will impact performance or cause the node to become unstable or unusable.


 **Procedure** 

1. Create an `    hpp_cr.yaml` file with a `    storagePools` stanza as in the following example:
    
    
    ```
    apiVersion: hostpathprovisioner.kubevirt.io/v1beta1    kind: HostPathProvisioner    metadata:      name: hostpath-provisioner    spec:      imagePullPolicy: IfNotPresent      storagePools:<span id="CO79-1"><!--Empty--></span><span class="callout">1</span>- name: any_name        path: "/var/myvolumes"<span id="CO79-2"><!--Empty--></span><span class="callout">2</span>workload:      nodeSelector:        kubernetes.io/os: linux
    ```
    
    
1. Save the file and exit.
1. Create the HPP by running the following command:
    
    
    ```
    $ oc create -f hpp_cr.yaml
    ```
    
    


##### 9.20.2.1.1. About creating storage classes




When you create a storage class, you set parameters that affect the dynamic provisioning of persistent volumes (PVs) that belong to that storage class. You cannot update a `StorageClass` object’s parameters after you create it.

In order to use the hostpath provisioner (HPP) you must create an associated storage class for the CSI driver with the `storagePools` stanza.

Note
Virtual machines use data volumes that are based on local PVs. Local PVs are bound to specific nodes. While the disk image is prepared for consumption by the virtual machine, it is possible that the virtual machine cannot be scheduled to the node where the local storage PV was previously pinned.

To solve this problem, use the Kubernetes pod scheduler to bind the persistent volume claim (PVC) to a PV on the correct node. By using the `StorageClass` value with `volumeBindingMode` parameter set to `WaitForFirstConsumer` , the binding and provisioning of the PV is delayed until a pod is created using the PVC.



##### 9.20.2.1.2. Creating a storage class for the CSI driver with the storagePools stanza




You create a storage class custom resource (CR) for the hostpath provisioner (HPP) CSI driver.

 **Procedure** 

1. Create a `    storageclass_csi.yaml` file to define the storage class:
    
    
    ```
    apiVersion: storage.k8s.io/v1    kind: StorageClass    metadata:      name: hostpath-csi    provisioner: kubevirt.io.hostpath-provisioner    reclaimPolicy: Delete<span id="CO80-1"><!--Empty--></span><span class="callout">1</span>volumeBindingMode: WaitForFirstConsumer<span id="CO80-2"><!--Empty--></span><span class="callout">2</span>parameters:      storagePool: my-storage-pool<span id="CO80-3"><!--Empty--></span><span class="callout">3</span>
    ```
    
    


#### 9.20.2.2. About storage pools created with PVC templates




If you have a single, large persistent volume (PV), you can create a storage pool by defining a PVC template in the hostpath provisioner (HPP) custom resource (CR).

A storage pool created with a PVC template can contain multiple HPP volumes. Splitting a PV into smaller volumes provides greater flexibility for data allocation.

The PVC template is based on the `spec` stanza of the `PersistentVolumeClaim` object:

 **Example `PersistentVolumeClaim` object** 

```
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: iso-pvc
spec:
  volumeMode: Block<span id="CO81-1"><!--Empty--></span><span class="callout">1</span>storageClassName: my-storage-class
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
```


You define a storage pool using a `pvcTemplate` specification in the HPP CR. The Operator creates a PVC from the `pvcTemplate` specification for each node containing the HPP CSI driver. The PVC created from the PVC template consumes the single large PV, allowing the HPP to create smaller dynamic volumes.

You can combine basic storage pools with storage pools created from PVC templates.

##### 9.20.2.2.1. Creating a storage pool with a PVC template




You can create a storage pool for multiple hostpath provisioner (HPP) volumes by specifying a PVC template in the HPP custom resource (CR).

 **Prerequisites** 

- The directories specified in `    spec.storagePools.path` must have read/write access.
- The storage pools must not be in the same partition as the operating system. Otherwise, the operating system partition might become filled to capacity, which will impact performance or cause the node to become unstable or unusable.


 **Procedure** 

1. Create an `    hpp_pvc_template_pool.yaml` file for the HPP CR that specifies a persistent volume (PVC) template in the `    storagePools` stanza according to the following example:
    
    
    ```
    apiVersion: hostpathprovisioner.kubevirt.io/v1beta1    kind: HostPathProvisioner    metadata:      name: hostpath-provisioner    spec:      imagePullPolicy: IfNotPresent      storagePools:<span id="CO82-1"><!--Empty--></span><span class="callout">1</span>- name: my-storage-pool        path: "/var/myvolumes"<span id="CO82-2"><!--Empty--></span><span class="callout">2</span>pvcTemplate:          volumeMode: Block<span id="CO82-3"><!--Empty--></span><span class="callout">3</span>storageClassName: my-storage-class<span id="CO82-4"><!--Empty--></span><span class="callout">4</span>accessModes:          - ReadWriteOnce          resources:            requests:              storage: 5Gi<span id="CO82-5"><!--Empty--></span><span class="callout">5</span>workload:        nodeSelector:          kubernetes.io/os: linux
    ```
    
    
1. Save the file and exit.
1. Create the HPP with a storage pool by running the following command:
    
    
    ```
    $ oc create -f hpp_pvc_template_pool.yaml
    ```
    
    


 **Additional resources** 

-  [Customizing the storage profile](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/virtualization/#virt-customizing-storage-profile_virt-creating-data-volumes) 


### 9.20.3. Creating data volumes




When you create a data volume, the Containerized Data Importer (CDI) creates a persistent volume claim (PVC) and populates the PVC with your data. You can create a data volume as either a standalone resource or by using a `dataVolumeTemplate` resource in a virtual machine specification. You create a data volume by using either the PVC API or storage APIs.

Important
When using OpenShift Virtualization with OpenShift Container Platform Container Storage, specify RBD block mode persistent volume claims (PVCs) when creating virtual machine disks. With virtual machine disks, RBD block mode volumes are more efficient and provide better performance than Ceph FS or RBD filesystem-mode PVCs.

To specify RBD block mode PVCs, use the 'ocs-storagecluster-ceph-rbd' storage class and `VolumeMode: Block` .



Tip
Whenever possible, use the storage API to optimize space allocation and maximize performance.



A _storage profile_ is a custom resource that the CDI manages. It provides recommended storage settings based on the associated storage class. A storage profile is allocated for each storage class.

Storage profiles enable you to create data volumes quickly while reducing coding and minimizing potential errors.

For recognized storage types, the CDI provides values that optimize the creation of PVCs. However, you can configure automatic settings for a storage class if you customize the storage profile.

#### 9.20.3.1. Creating data volumes using the storage API




When you create a data volume using the storage API, the Containerized Data Interface (CDI) optimizes your persistent volume claim (PVC) allocation based on the type of storage supported by your selected storage class. You only have to specify the data volume name, namespace, and the amount of storage that you want to allocate.

For example:

- When using Ceph RBD, `    accessModes` is automatically set to `    ReadWriteMany` , which enables live migration. `    volumeMode` is set to `    Block` to maximize performance.
- When you are using `    volumeMode: Filesystem` , more space will automatically be requested by the CDI, if required to accommodate file system overhead.


In the following YAML, using the storage API requests a data volume with two gigabytes of usable space. The user does not need to know the `volumeMode` in order to correctly estimate the required persistent volume claim (PVC) size. The CDI chooses the optimal combination of `accessModes` and `volumeMode` attributes automatically. These optimal values are based on the type of storage or the defaults that you define in your storage profile. If you want to provide custom values, they override the system-calculated values.

 **Example DataVolume definition** 

```
apiVersion: cdi.kubevirt.io/v1beta1
kind: DataVolume
metadata:
  name: &lt;datavolume&gt;<span id="CO83-1"><!--Empty--></span><span class="callout">1</span>spec:
  source:
    pvc:<span id="CO83-2"><!--Empty--></span><span class="callout">2</span>namespace: "&lt;source_namespace&gt;"<span id="CO83-3"><!--Empty--></span><span class="callout">3</span>name: "&lt;my_vm_disk&gt;"<span id="CO83-4"><!--Empty--></span><span class="callout">4</span>storage:<span id="CO83-5"><!--Empty--></span><span class="callout">5</span>resources:
      requests:
        storage: 2Gi<span id="CO83-6"><!--Empty--></span><span class="callout">6</span>storageClassName: &lt;storage_class&gt;<span id="CO83-7"><!--Empty--></span><span class="callout">7</span>
```


#### 9.20.3.2. Creating data volumes using the PVC API




When you create a data volume using the PVC API, the Containerized Data Interface (CDI) creates the data volume based on what you specify for the following fields:

-  `    accessModes` ( `    ReadWriteOnce` , `    ReadWriteMany` , or `    ReadOnlyMany` )
-  `    volumeMode` ( `    Filesystem` or `    Block` )
-  `    capacity` of `    storage` ( `    5Gi` , for example)


In the following YAML, using the PVC API allocates a data volume with a storage capacity of two gigabytes. You specify an access mode of `ReadWriteMany` to enable live migration. Because you know the values your system can support, you specify `Block` storage instead of the default, `Filesystem` .

 **Example DataVolume definition** 

```
apiVersion: cdi.kubevirt.io/v1beta1
kind: DataVolume
metadata:
  name: &lt;datavolume&gt;<span id="CO84-1"><!--Empty--></span><span class="callout">1</span>spec:
  source:
    pvc:<span id="CO84-2"><!--Empty--></span><span class="callout">2</span>namespace: "&lt;source_namespace&gt;"<span id="CO84-3"><!--Empty--></span><span class="callout">3</span>name: "&lt;my_vm_disk&gt;"<span id="CO84-4"><!--Empty--></span><span class="callout">4</span>pvc:<span id="CO84-5"><!--Empty--></span><span class="callout">5</span>accessModes:<span id="CO84-6"><!--Empty--></span><span class="callout">6</span>- ReadWriteMany
    resources:
      requests:
        storage: 2Gi<span id="CO84-7"><!--Empty--></span><span class="callout">7</span>volumeMode: Block<span id="CO84-8"><!--Empty--></span><span class="callout">8</span>storageClassName: &lt;storage_class&gt;<span id="CO84-9"><!--Empty--></span><span class="callout">9</span>
```


Important
When you explicitly allocate a data volume by using the PVC API and you are not using `volumeMode: Block` , consider file system overhead.

File system overhead is the amount of space required by the file system to maintain its metadata. The amount of space required for file system metadata is file system dependent. Failing to account for file system overhead in your storage capacity request can result in an underlying persistent volume claim (PVC) that is not large enough to accommodate your virtual machine disk.

If you use the storage API, the CDI will factor in file system overhead and request a larger persistent volume claim (PVC) to ensure that your allocation request is successful.



#### 9.20.3.3. Customizing the storage profile




You can specify default parameters by editing the `StorageProfile` object for the provisioner’s storage class. These default parameters only apply to the persistent volume claim (PVC) if they are not configured in the `DataVolume` object.

 **Prerequisites** 

- Ensure that your planned configuration is supported by the storage class and its provider. Specifying an incompatible configuration in a storage profile causes volume provisioning to fail.


Note
An empty `status` section in a storage profile indicates that a storage provisioner is not recognized by the Containerized Data Interface (CDI). Customizing a storage profile is necessary if you have a storage provisioner that is not recognized by the CDI. In this case, the administrator sets appropriate values in the storage profile to ensure successful allocations.



Warning
If you create a data volume and omit YAML attributes and these attributes are not defined in the storage profile, then the requested storage will not be allocated and the underlying persistent volume claim (PVC) will not be created.



 **Procedure** 

1. Edit the storage profile. In this example, the provisioner is not recognized by CDI:
    
    
    ```
    $ oc edit -n openshift-cnv storageprofile &lt;storage_class&gt;
    ```
    
     **Example storage profile** 
    
    
    ```
    apiVersion: cdi.kubevirt.io/v1beta1    kind: StorageProfile    metadata:      name: &lt;unknown_provisioner_class&gt;    #   ...    spec: {}    status:      provisioner: &lt;unknown_provisioner&gt;      storageClass: &lt;unknown_provisioner_class&gt;
    ```
    
    
    
1. Provide the needed attribute values in the storage profile:
    
     **Example storage profile** 
    
    
    ```
    apiVersion: cdi.kubevirt.io/v1beta1    kind: StorageProfile    metadata:      name: &lt;unknown_provisioner_class&gt;    #   ...    spec:      claimPropertySets:      - accessModes:        - ReadWriteOnce<span id="CO85-1"><!--Empty--></span><span class="callout">1</span>volumeMode:          Filesystem<span id="CO85-2"><!--Empty--></span><span class="callout">2</span>status:      provisioner: &lt;unknown_provisioner&gt;      storageClass: &lt;unknown_provisioner_class&gt;
    ```
    
    
    After you save your changes, the selected values appear in the storage profile `    status` element.
    
    


##### 9.20.3.3.1. Setting a default cloning strategy using a storage profile




You can use storage profiles to set a default cloning method for a storage class, creating a _cloning strategy_ . Setting cloning strategies can be helpful, for example, if your storage vendor only supports certain cloning methods. It also allows you to select a method that limits resource usage or maximizes performance.

Cloning strategies can be specified by setting the `cloneStrategy` attribute in a storage profile to one of these values:

-  `    snapshot` - This method is used by default when snapshots are configured. This cloning strategy uses a temporary volume snapshot to clone the volume. The storage provisioner must support CSI snapshots.
-  `    copy` - This method uses a source pod and a target pod to copy data from the source volume to the target volume. Host-assisted cloning is the least efficient method of cloning.
-  `    csi-clone` - This method uses the CSI clone API to efficiently clone an existing volume without using an interim volume snapshot. Unlike `    snapshot` or `    copy` , which are used by default if no storage profile is defined, CSI volume cloning is only used when you specify it in the `    StorageProfile` object for the provisioner’s storage class.


Note
You can also set clone strategies using the CLI without modifying the default `claimPropertySets` in your YAML `spec` section.



 **Example storage profile** 

```
apiVersion: cdi.kubevirt.io/v1beta1
kind: StorageProfile
metadata:
  name: &lt;provisioner_class&gt;
#   ...
spec:
  claimPropertySets:
  - accessModes:
    - ReadWriteOnce<span id="CO86-1"><!--Empty--></span><span class="callout">1</span>volumeMode:
      Filesystem<span id="CO86-2"><!--Empty--></span><span class="callout">2</span>cloneStrategy:
  csi-clone<span id="CO86-3"><!--Empty--></span><span class="callout">3</span>status:
  provisioner: &lt;provisioner&gt;
  storageClass: &lt;provisioner_class&gt;
```


#### 9.20.3.4. Additional resources




-  [About creating storage classes](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/virtualization/#virt-about-creating-storage-classes_virt-configuring-local-storage-for-vms) 
-  [Overriding the default file system overhead value](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/virtualization/#virt-overriding-default-fs-overhead-value_virt-reserving-pvc-space-fs-overhead) 
-  [Cloning a data volume using smart cloning](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/virtualization/#virt-cloning-a-datavolume-using-smart-cloning) 


### 9.20.4. Reserving PVC space for file system overhead




By default, the OpenShift Virtualization reserves space for file system overhead data in persistent volume claims (PVCs) that use the `Filesystem` volume mode. You can set the percentage to reserve space for this purpose globally and for specific storage classes.

#### 9.20.4.1. How file system overhead affects space for virtual machine disks




When you add a virtual machine disk to a persistent volume claim (PVC) that uses the `Filesystem` volume mode, you must ensure that there is enough space on the PVC for:

- The virtual machine disk.
- The space reserved for file system overhead, such as metadata


By default, OpenShift Virtualization reserves 5.5% of the PVC space for overhead, reducing the space available for virtual machine disks by that amount.

You can configure a different overhead value by editing the `HCO` object. You can change the value globally and you can specify values for specific storage classes.

#### 9.20.4.2. Overriding the default file system overhead value




Change the amount of persistent volume claim (PVC) space that the OpenShift Virtualization reserves for file system overhead by editing the `spec.filesystemOverhead` attribute of the `HCO` object.

 **Prerequisites** 

- Install the OpenShift CLI ( `    oc` ).


 **Procedure** 

1. Open the `    HCO` object for editing by running the following command:
    
    
    ```
    $ oc edit hco -n openshift-cnv kubevirt-hyperconverged
    ```
    
    
1. Edit the `    spec.filesystemOverhead` fields, populating them with your chosen values:
    
    
    ```
    ...    spec:      filesystemOverhead:        global: "&lt;new_global_value&gt;"<span id="CO87-1"><!--Empty--></span><span class="callout">1</span>storageClass:          &lt;storage_class_name&gt;: "&lt;new_value_for_this_storage_class&gt;"<span id="CO87-2"><!--Empty--></span><span class="callout">2</span>
    ```
    
    
1. Save and exit the editor to update the `    HCO` object.


 **Verification** 

- View the `    CDIConfig` status and verify your changes by running one of the following commands:
    
    To generally verify changes to `    CDIConfig` :
    
    
    ```
    $ oc get cdiconfig -o yaml
    ```
    
    To view your specific changes to `    CDIConfig` :
    
    
    ```
    $ oc get cdiconfig -o jsonpath='{.items..status.filesystemOverhead}'
    ```
    
    


### 9.20.5. Configuring CDI to work with namespaces that have a compute resource quota




You can use the Containerized Data Importer (CDI) to import, upload, and clone virtual machine disks into namespaces that are subject to CPU and memory resource restrictions.

#### 9.20.5.1. About CPU and memory quotas in a namespace




A _resource quota_ , defined by the `ResourceQuota` object, imposes restrictions on a namespace that limit the total amount of compute resources that can be consumed by resources within that namespace.

The `HyperConverged` custom resource (CR) defines the user configuration for the Containerized Data Importer (CDI). The CPU and memory request and limit values are set to a default value of `0` . This ensures that pods created by CDI that do not specify compute resource requirements are given the default values and are allowed to run in a namespace that is restricted with a quota.

#### 9.20.5.2. Overriding CPU and memory defaults




Modify the default settings for CPU and memory requests and limits for your use case by adding the `spec.resourceRequirements.storageWorkloads` stanza to the `HyperConverged` custom resource (CR).

 **Prerequisites** 

- Install the OpenShift CLI ( `    oc` ).


 **Procedure** 

1. Edit the `    HyperConverged` CR by running the following command:
    
    
    ```
    $ oc edit hco -n openshift-cnv kubevirt-hyperconverged
    ```
    
    
1. Add the `    spec.resourceRequirements.storageWorkloads` stanza to the CR, setting the values based on your use case. For example:
    
    
    ```
    apiVersion: hco.kubevirt.io/v1beta1    kind: HyperConverged    metadata:      name: kubevirt-hyperconverged    spec:      resourceRequirements:        storageWorkloads:          limits:            cpu: "500m"            memory: "2Gi"          requests:            cpu: "250m"            memory: "1Gi"
    ```
    
    
1. Save and exit the editor to update the `    HyperConverged` CR.


#### 9.20.5.3. Additional resources




-  [Resource quotas per project](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/building_applications/#quotas-setting-per-project) 


### 9.20.6. Managing data volume annotations




Data volume (DV) annotations allow you to manage pod behavior. You can add one or more annotations to a data volume, which then propagates to the created importer pods.

#### 9.20.6.1. Example: Data volume annotations




This example shows how you can configure data volume (DV) annotations to control which network the importer pod uses. The `v1.multus-cni.io/default-network: bridge-network` annotation causes the pod to use the multus network named `bridge-network` as its default network. If you want the importer pod to use both the default network from the cluster and the secondary multus network, use the `k8s.v1.cni.cncf.io/networks: &lt;network_name&gt;` annotation.

 **Multus network annotation example** 

```
apiVersion: cdi.kubevirt.io/v1beta1
kind: DataVolume
metadata:
  name: dv-ann
  annotations:
      v1.multus-cni.io/default-network: bridge-network<span id="CO88-1"><!--Empty--></span><span class="callout">1</span>spec:
  source:
      http:
         url: "example.exampleurl.com"
  pvc:
    accessModes:
      - ReadWriteOnce
    resources:
      requests:
        storage: 1Gi
```


### 9.20.7. Using preallocation for data volumes




The Containerized Data Importer can preallocate disk space to improve write performance when creating data volumes.

You can enable preallocation for specific data volumes.

#### 9.20.7.1. About preallocation




The Containerized Data Importer (CDI) can use the QEMU preallocate mode for data volumes to improve write performance. You can use preallocation mode for importing and uploading operations and when creating blank data volumes.

If preallocation is enabled, CDI uses the better preallocation method depending on the underlying file system and device type:

#### 9.20.7.2. Enabling preallocation for a data volume




You can enable preallocation for specific data volumes by including the `spec.preallocation` field in the data volume manifest. You can enable preallocation mode in either the web console or by using the OpenShift CLI ( `oc` ).

Preallocation mode is supported for all CDI source types.

 **Procedure** 

- Specify the `    spec.preallocation` field in the data volume manifest:
    
    
    ```
    apiVersion: cdi.kubevirt.io/v1beta1    kind: DataVolume    metadata:      name: preallocated-datavolume    spec:      source:<span id="CO89-1"><!--Empty--></span><span class="callout">1</span>...      pvc:        ...      preallocation: true<span id="CO89-2"><!--Empty--></span><span class="callout">2</span>
    ```
    
    


### 9.20.8. Uploading local disk images by using the web console




You can upload a locally stored disk image file by using the web console.

#### 9.20.8.1. Prerequisites




- You must have a virtual machine image file in IMG, ISO, or QCOW2 format.
- If you require scratch space according to the [CDI supported operations matrix](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/virtualization/#virt-cdi-supported-operations-matrix_virt-uploading-local-disk-images-web) , you must first [define a storage class or prepare CDI scratch space](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/virtualization/#virt-defining-storageclass_virt-preparing-cdi-scratch-space) for this operation to complete successfully.


#### 9.20.8.2. CDI supported operations matrix




This matrix shows the supported CDI operations for content types against endpoints, and which of these operations requires scratch space.

| Content types | HTTP | HTTPS | HTTP basic auth | Registry | Upload |
| --- | --- | --- | --- | --- | --- |
| KubeVirt (QCOW2) | ✓ QCOW2    
✓ GZ*    
✓ XZ* | ✓ QCOW2**    
✓ GZ*    
✓ XZ* | ✓ QCOW2    
✓ GZ*    
✓ XZ* | ✓ QCOW2*    
□ GZ    
□ XZ | ✓ QCOW2*    
✓ GZ*    
✓ XZ* |
| KubeVirt (RAW) | ✓ RAW    
✓ GZ    
✓ XZ | ✓ RAW    
✓ GZ    
✓ XZ | ✓ RAW    
✓ GZ    
✓ XZ | ✓ RAW*    
□ GZ    
□ XZ | ✓ RAW*    
✓ GZ*    
✓ XZ* |


✓ Supported operation

□ Unsupported operation

* Requires scratch space

** Requires scratch space if a custom certificate authority is required

#### 9.20.8.3. Uploading an image file using the web console




Use the web console to upload an image file to a new persistent volume claim (PVC). You can later use this PVC to attach the image to new virtual machines.

 **Prerequisites** 

- You must have one of the following:
    
    
    - A raw virtual machine image file in either ISO or IMG format.
    - A virtual machine image file in QCOW2 format.
    
- For best results, compress your image file according to the following guidelines before you upload it:
    
    
    - Compress a raw image file by using `        xz` or `        gzip` .
        
        Note
        Using a compressed raw image file results in the most efficient upload.
        
        
        
        
    - Compress a QCOW2 image file by using the method that is recommended for your client:
        
        
        - If you use a Linux client, _sparsify_ the QCOW2 file by using the [virt-sparsify](https://libguestfs.org/virt-sparsify.1.html) tool.
        - If you use a Windows client, compress the QCOW2 file by using `            xz` or `            gzip` .
        
    


 **Procedure** 

1. From the side menu of the web console, click **Storage** → **Persistent Volume Claims** .
1. Click the **Create Persistent Volume Claim** drop-down list to expand it.
1. Click **With Data Upload Form** to open the **Upload Data to Persistent Volume Claim** page.
1. Click **Browse** to open the file manager and select the image that you want to upload, or drag the file into the **Drag a file here or browse to upload** field.
1. Optional: Set this image as the default image for a specific operating system.
    
    
    1. Select the **Attach this data to a virtual machine operating system** check box.
    1. Select an operating system from the list.
    
1. The **Persistent Volume Claim Name** field is automatically filled with a unique name and cannot be edited. Take note of the name assigned to the PVC so that you can identify it later, if necessary.
1. Select a storage class from the **Storage Class** list.
1. In the **Size** field, enter the size value for the PVC. Select the corresponding unit of measurement from the drop-down list.
    
    Warning
    The PVC size must be larger than the size of the uncompressed virtual disk.
    
    
    
    
1. Select an **Access Mode** that matches the storage class that you selected.
1. Click **Upload** .


#### 9.20.8.4. Additional resources




-  [Configure preallocation mode](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/virtualization/#virt-using-preallocation-for-datavolumes) to improve write performance for data volume operations.


### 9.20.9. Uploading local disk images by using the virtctl tool




You can upload a locally stored disk image to a new or existing data volume by using the `virtctl` command-line utility.

#### 9.20.9.1. Prerequisites




-  [Enable](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/virtualization/#virt-enabling-virtctl) the `    kubevirt-virtctl` package.
- If you require scratch space according to the [CDI supported operations matrix](#virt-cdi-supported-operations-matrix_virt-cloning-vm-disk-into-new-datavolume-block) , you must first [define a storage class or prepare CDI scratch space](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/virtualization/#virt-defining-storageclass_virt-preparing-cdi-scratch-space) for this operation to complete successfully.


#### 9.20.9.2. About data volumes




 `DataVolume` objects are custom resources that are provided by the Containerized Data Importer (CDI) project. Data volumes orchestrate import, clone, and upload operations that are associated with an underlying persistent volume claim (PVC). Data volumes are integrated with OpenShift Virtualization, and they prevent a virtual machine from being started before the PVC has been prepared.

#### 9.20.9.3. Creating an upload data volume




You can manually create a data volume with an `upload` data source to use for uploading local disk images.

 **Procedure** 

1. Create a data volume configuration that specifies `    spec: source: upload{}` :
    
    
    ```
    apiVersion: cdi.kubevirt.io/v1beta1    kind: DataVolume    metadata:      name: &lt;upload-datavolume&gt;<span id="CO90-1"><!--Empty--></span><span class="callout">1</span>spec:      source:          upload: {}      pvc:        accessModes:          - ReadWriteOnce        resources:          requests:            storage: &lt;2Gi&gt;<span id="CO90-2"><!--Empty--></span><span class="callout">2</span>
    ```
    
    
1. Create the data volume by running the following command:
    
    
    ```
    $ oc create -f &lt;upload-datavolume&gt;.yaml
    ```
    
    


#### 9.20.9.4. Uploading a local disk image to a data volume




You can use the `virtctl` CLI utility to upload a local disk image from a client machine to a data volume (DV) in your cluster. You can use a DV that already exists in your cluster or create a new DV during this procedure.

Note
After you upload a local disk image, you can add it to a virtual machine.



 **Prerequisites** 

- You must have one of the following:
    
    
    - A raw virtual machine image file in either ISO or IMG format.
    - A virtual machine image file in QCOW2 format.
    
- For best results, compress your image file according to the following guidelines before you upload it:
    
    
    - Compress a raw image file by using `        xz` or `        gzip` .
        
        Note
        Using a compressed raw image file results in the most efficient upload.
        
        
        
        
    - Compress a QCOW2 image file by using the method that is recommended for your client:
        
        
        - If you use a Linux client, _sparsify_ the QCOW2 file by using the [virt-sparsify](https://libguestfs.org/virt-sparsify.1.html) tool.
        - If you use a Windows client, compress the QCOW2 file by using `            xz` or `            gzip` .
        
    
- The `    kubevirt-virtctl` package must be installed on the client machine.
- The client machine must be configured to trust the OpenShift Container Platform router’s certificate.


 **Procedure** 

1. Identify the following items:
    
    
    - The name of the upload data volume that you want to use. If this data volume does not exist, it is created automatically.
    - The size of the data volume, if you want it to be created during the upload procedure. The size must be greater than or equal to the size of the disk image.
    - The file location of the virtual machine disk image that you want to upload.
    
1. Upload the disk image by running the `    virtctl image-upload` command. Specify the parameters that you identified in the previous step. For example:
    
    
    ```
    $ virtctl image-upload dv &lt;datavolume_name&gt; \<span id="CO91-1"><!--Empty--></span><span class="callout">1</span>--size=&lt;datavolume_size&gt; \<span id="CO91-2"><!--Empty--></span><span class="callout">2</span>--image-path=&lt;/path/to/image&gt; \<span id="CO91-3"><!--Empty--></span><span class="callout">3</span>
    ```
    
    Note
    
    - If you do not want to create a new data volume, omit the `        --size` parameter and include the `        --no-create` flag.
    - When uploading a disk image to a PVC, the PVC size must be larger than the size of the uncompressed virtual disk.
    - To allow insecure server connections when using HTTPS, use the `        --insecure` parameter. Be aware that when you use the `        --insecure` flag, the authenticity of the upload endpoint is **not** verified.
    
    
    
1. Optional. To verify that a data volume was created, view all data volumes by running the following command:
    
    
    ```
    $ oc get dvs
    ```
    
    


#### 9.20.9.5. CDI supported operations matrix




This matrix shows the supported CDI operations for content types against endpoints, and which of these operations requires scratch space.

| Content types | HTTP | HTTPS | HTTP basic auth | Registry | Upload |
| --- | --- | --- | --- | --- | --- |
| KubeVirt (QCOW2) | ✓ QCOW2    
✓ GZ*    
✓ XZ* | ✓ QCOW2**    
✓ GZ*    
✓ XZ* | ✓ QCOW2    
✓ GZ*    
✓ XZ* | ✓ QCOW2*    
□ GZ    
□ XZ | ✓ QCOW2*    
✓ GZ*    
✓ XZ* |
| KubeVirt (RAW) | ✓ RAW    
✓ GZ    
✓ XZ | ✓ RAW    
✓ GZ    
✓ XZ | ✓ RAW    
✓ GZ    
✓ XZ | ✓ RAW*    
□ GZ    
□ XZ | ✓ RAW*    
✓ GZ*    
✓ XZ* |


✓ Supported operation

□ Unsupported operation

* Requires scratch space

** Requires scratch space if a custom certificate authority is required

#### 9.20.9.6. Additional resources




-  [Configure preallocation mode](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/virtualization/#virt-using-preallocation-for-datavolumes) to improve write performance for data volume operations.


### 9.20.10. Uploading a local disk image to a block storage data volume




You can upload a local disk image into a block data volume by using the `virtctl` command-line utility.

In this workflow, you create a local block device to use as a persistent volume, associate this block volume with an `upload` data volume, and use `virtctl` to upload the local disk image into the data volume.

#### 9.20.10.1. Prerequisites




-  [Enable](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/virtualization/#virt-enabling-virtctl) the `    kubevirt-virtctl` package.
- If you require scratch space according to the [CDI supported operations matrix](#virt-cdi-supported-operations-matrix_virt-uploading-local-disk-images-block) , you must first [define a storage class or prepare CDI scratch space](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/virtualization/#virt-defining-storageclass_virt-preparing-cdi-scratch-space) for this operation to complete successfully.


#### 9.20.10.2. About data volumes




 `DataVolume` objects are custom resources that are provided by the Containerized Data Importer (CDI) project. Data volumes orchestrate import, clone, and upload operations that are associated with an underlying persistent volume claim (PVC). Data volumes are integrated with OpenShift Virtualization, and they prevent a virtual machine from being started before the PVC has been prepared.

#### 9.20.10.3. About block persistent volumes




A block persistent volume (PV) is a PV that is backed by a raw block device. These volumes do not have a file system and can provide performance benefits for virtual machines by reducing overhead.

Raw block volumes are provisioned by specifying `volumeMode: Block` in the PV and persistent volume claim (PVC) specification.

#### 9.20.10.4. Creating a local block persistent volume




Create a local block persistent volume (PV) on a node by populating a file and mounting it as a loop device. You can then reference this loop device in a PV manifest as a `Block` volume and use it as a block device for a virtual machine image.

 **Procedure** 

1. Log in as `    root` to the node on which to create the local PV. This procedure uses `    node01` for its examples.
1. Create a file and populate it with null characters so that it can be used as a block device. The following example creates a file `    loop10` with a size of 2Gb (20 100Mb blocks):
    
    
    ```
    $ dd if=/dev/zero of=&lt;loop10&gt; bs=100M count=20
    ```
    
    
1. Mount the `    loop10` file as a loop device.
    
    
    ```
    $ losetup &lt;/dev/loop10&gt;d3 &lt;loop10&gt;<span id="CO92-1"><!--Empty--></span><span class="callout">1</span><span id="CO92-2"><!--Empty--></span><span class="callout">2</span>
    ```
    
    
1. Create a `    PersistentVolume` manifest that references the mounted loop device.
    
    
    ```
    kind: PersistentVolume    apiVersion: v1    metadata:      name: &lt;local-block-pv10&gt;      annotations:    spec:      local:        path: &lt;/dev/loop10&gt;<span id="CO93-1"><!--Empty--></span><span class="callout">1</span>capacity:        storage: &lt;2Gi&gt;      volumeMode: Block<span id="CO93-2"><!--Empty--></span><span class="callout">2</span>storageClassName: local<span id="CO93-3"><!--Empty--></span><span class="callout">3</span>accessModes:        - ReadWriteOnce      persistentVolumeReclaimPolicy: Delete      nodeAffinity:        required:          nodeSelectorTerms:          - matchExpressions:            - key: kubernetes.io/hostname              operator: In              values:              - &lt;node01&gt;<span id="CO93-4"><!--Empty--></span><span class="callout">4</span>
    ```
    
    
1. Create the block PV.
    
    
    ```
    # oc create -f &lt;local-block-pv10.yaml&gt;<span id="CO94-1"><!--Empty--></span><span class="callout">1</span>
    ```
    
    


#### 9.20.10.5. Creating an upload data volume




You can manually create a data volume with an `upload` data source to use for uploading local disk images.

 **Procedure** 

1. Create a data volume configuration that specifies `    spec: source: upload{}` :
    
    
    ```
    apiVersion: cdi.kubevirt.io/v1beta1    kind: DataVolume    metadata:      name: &lt;upload-datavolume&gt;<span id="CO95-1"><!--Empty--></span><span class="callout">1</span>spec:      source:          upload: {}      pvc:        accessModes:          - ReadWriteOnce        resources:          requests:            storage: &lt;2Gi&gt;<span id="CO95-2"><!--Empty--></span><span class="callout">2</span>
    ```
    
    
1. Create the data volume by running the following command:
    
    
    ```
    $ oc create -f &lt;upload-datavolume&gt;.yaml
    ```
    
    


#### 9.20.10.6. Uploading a local disk image to a data volume




You can use the `virtctl` CLI utility to upload a local disk image from a client machine to a data volume (DV) in your cluster. You can use a DV that already exists in your cluster or create a new DV during this procedure.

Note
After you upload a local disk image, you can add it to a virtual machine.



 **Prerequisites** 

- You must have one of the following:
    
    
    - A raw virtual machine image file in either ISO or IMG format.
    - A virtual machine image file in QCOW2 format.
    
- For best results, compress your image file according to the following guidelines before you upload it:
    
    
    - Compress a raw image file by using `        xz` or `        gzip` .
        
        Note
        Using a compressed raw image file results in the most efficient upload.
        
        
        
        
    - Compress a QCOW2 image file by using the method that is recommended for your client:
        
        
        - If you use a Linux client, _sparsify_ the QCOW2 file by using the [virt-sparsify](https://libguestfs.org/virt-sparsify.1.html) tool.
        - If you use a Windows client, compress the QCOW2 file by using `            xz` or `            gzip` .
        
    
- The `    kubevirt-virtctl` package must be installed on the client machine.
- The client machine must be configured to trust the OpenShift Container Platform router’s certificate.


 **Procedure** 

1. Identify the following items:
    
    
    - The name of the upload data volume that you want to use. If this data volume does not exist, it is created automatically.
    - The size of the data volume, if you want it to be created during the upload procedure. The size must be greater than or equal to the size of the disk image.
    - The file location of the virtual machine disk image that you want to upload.
    
1. Upload the disk image by running the `    virtctl image-upload` command. Specify the parameters that you identified in the previous step. For example:
    
    
    ```
    $ virtctl image-upload dv &lt;datavolume_name&gt; \<span id="CO96-1"><!--Empty--></span><span class="callout">1</span>--size=&lt;datavolume_size&gt; \<span id="CO96-2"><!--Empty--></span><span class="callout">2</span>--image-path=&lt;/path/to/image&gt; \<span id="CO96-3"><!--Empty--></span><span class="callout">3</span>
    ```
    
    Note
    
    - If you do not want to create a new data volume, omit the `        --size` parameter and include the `        --no-create` flag.
    - When uploading a disk image to a PVC, the PVC size must be larger than the size of the uncompressed virtual disk.
    - To allow insecure server connections when using HTTPS, use the `        --insecure` parameter. Be aware that when you use the `        --insecure` flag, the authenticity of the upload endpoint is **not** verified.
    
    
    
1. Optional. To verify that a data volume was created, view all data volumes by running the following command:
    
    
    ```
    $ oc get dvs
    ```
    
    


#### 9.20.10.7. CDI supported operations matrix




This matrix shows the supported CDI operations for content types against endpoints, and which of these operations requires scratch space.

| Content types | HTTP | HTTPS | HTTP basic auth | Registry | Upload |
| --- | --- | --- | --- | --- | --- |
| KubeVirt (QCOW2) | ✓ QCOW2    
✓ GZ*    
✓ XZ* | ✓ QCOW2**    
✓ GZ*    
✓ XZ* | ✓ QCOW2    
✓ GZ*    
✓ XZ* | ✓ QCOW2*    
□ GZ    
□ XZ | ✓ QCOW2*    
✓ GZ*    
✓ XZ* |
| KubeVirt (RAW) | ✓ RAW    
✓ GZ    
✓ XZ | ✓ RAW    
✓ GZ    
✓ XZ | ✓ RAW    
✓ GZ    
✓ XZ | ✓ RAW*    
□ GZ    
□ XZ | ✓ RAW*    
✓ GZ*    
✓ XZ* |


✓ Supported operation

□ Unsupported operation

* Requires scratch space

** Requires scratch space if a custom certificate authority is required

#### 9.20.10.8. Additional resources




-  [Configure preallocation mode](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/virtualization/#virt-using-preallocation-for-datavolumes) to improve write performance for data volume operations.


### 9.20.11. Managing virtual machine snapshots




You can create and delete virtual machine (VM) snapshots for VMs, whether the VMs are powered off (offline) or on (online). You can only restore to a powered off (offline) VM. OpenShift Virtualization supports VM snapshots on the following:

- Red Hat OpenShift Data Foundation
- Any other cloud storage provider with the Container Storage Interface (CSI) driver that supports the Kubernetes Volume Snapshot API


Online snapshots have a default time deadline of five minutes ( `5m` ) that can be changed, if needed.

Important
Online snapshots are supported for virtual machines that have hot-plugged virtual disks. However, hot-plugged disks that are not in the virtual machine specification are not included in the snapshot.



Note
To create snapshots of an online (Running state) VM with the highest integrity, install the QEMU guest agent.

The QEMU guest agent takes a consistent snapshot by attempting to quiesce the VM file system as much as possible, depending on the system workload. This ensures that in-flight I/O is written to the disk before the snapshot is taken. If the guest agent is not present, quiescing is not possible and a best-effort snapshot is taken. The conditions under which the snapshot was taken are reflected in the snapshot indications that are displayed in the web console or CLI.



#### 9.20.11.1. About virtual machine snapshots




A _snapshot_ represents the state and data of a virtual machine (VM) at a specific point in time. You can use a snapshot to restore an existing VM to a previous state (represented by the snapshot) for backup and disaster recovery or to rapidly roll back to a previous development version.

A VM snapshot is created from a VM that is powered off (Stopped state) or powered on (Running state).

When taking a snapshot of a running VM, the controller checks that the QEMU guest agent is installed and running. If so, it freezes the VM file system before taking the snapshot, and thaws the file system after the snapshot is taken.

The snapshot stores a copy of each Container Storage Interface (CSI) volume attached to the VM and a copy of the VM specification and metadata. Snapshots cannot be changed after creation.

With the VM snapshots feature, cluster administrators and application developers can:

- Create a new snapshot
- List all snapshots attached to a specific VM
- Restore a VM from a snapshot
- Delete an existing VM snapshot


##### 9.20.11.1.1. Virtual machine snapshot controller and custom resource definitions (CRDs)




The VM snapshot feature introduces three new API objects defined as CRDs for managing snapshots:

-  `    VirtualMachineSnapshot` : Represents a user request to create a snapshot. It contains information about the current state of the VM.
-  `    VirtualMachineSnapshotContent` : Represents a provisioned resource on the cluster (a snapshot). It is created by the VM snapshot controller and contains references to all resources required to restore the VM.
-  `    VirtualMachineRestore` : Represents a user request to restore a VM from a snapshot.


The VM snapshot controller binds a `VirtualMachineSnapshotContent` object with the `VirtualMachineSnapshot` object for which it was created, with a one-to-one mapping.

#### 9.20.11.2. Installing QEMU guest agent on a Linux virtual machine




The `qemu-guest-agent` is widely available and available by default in Red Hat virtual machines. Install the agent and start the service.

To check if your virtual machine (VM) has the QEMU guest agent installed and running, verify that `AgentConnected` is listed in the VM spec.

Note
To create snapshots of an online (Running state) VM with the highest integrity, install the QEMU guest agent.

The QEMU guest agent takes a consistent snapshot by attempting to quiesce the VM’s file system as much as possible, depending on the system workload. This ensures that in-flight I/O is written to the disk before the snapshot is taken. If the guest agent is not present, quiescing is not possible and a best-effort snapshot is taken. The conditions under which the snapshot was taken are reflected in the snapshot indications that are displayed in the web console or CLI.



 **Procedure** 

1. Access the virtual machine command line through one of the consoles or by SSH.
1. Install the QEMU guest agent on the virtual machine:
    
    
    ```
    $ yum install -y qemu-guest-agent
    ```
    
    
1. Ensure the service is persistent and start it:
    
    
    ```
    $ systemctl enable --now qemu-guest-agent
    ```
    
    


#### 9.20.11.3. Installing QEMU guest agent on a Windows virtual machine




For Windows virtual machines, the QEMU guest agent is included in the VirtIO drivers. Install the drivers on an existing or a new Windows installation.

To check if your virtual machine (VM) has the QEMU guest agent installed and running, verify that `AgentConnected` is listed in the VM spec.

Note
To create snapshots of an online (Running state) VM with the highest integrity, install the QEMU guest agent.

The QEMU guest agent takes a consistent snapshot by attempting to quiesce the VM’s file system as much as possible, depending on the system workload. This ensures that in-flight I/O is written to the disk before the snapshot is taken. If the guest agent is not present, quiescing is not possible and a best-effort snapshot is taken. The conditions under which the snapshot was taken are reflected in the snapshot indications that are displayed in the web console or CLI.



##### 9.20.11.3.1. Installing VirtIO drivers on an existing Windows virtual machine




Install the VirtIO drivers from the attached SATA CD drive to an existing Windows virtual machine.

Note
This procedure uses a generic approach to adding drivers to Windows. The process might differ slightly between versions of Windows. See the installation documentation for your version of Windows for specific installation steps.



 **Procedure** 

1. Start the virtual machine and connect to a graphical console.
1. Log in to a Windows user session.
1. Open **Device Manager** and expand **Other devices** to list any **Unknown device** .
    
    
    1. Open the `        Device Properties` to identify the unknown device. Right-click the device and select **Properties** .
    1. Click the **Details** tab and select **Hardware Ids** in the **Property** list.
    1. Compare the **Value** for the **Hardware Ids** with the supported VirtIO drivers.
    
1. Right-click the device and select **Update Driver Software** .
1. Click **Browse my computer for driver software** and browse to the attached SATA CD drive, where the VirtIO drivers are located. The drivers are arranged hierarchically according to their driver type, operating system, and CPU architecture.
1. Click **Next** to install the driver.
1. Repeat this process for all the necessary VirtIO drivers.
1. After the driver installs, click **Close** to close the window.
1. Reboot the virtual machine to complete the driver installation.


##### 9.20.11.3.2. Installing VirtIO drivers during Windows installation




Install the VirtIO drivers from the attached SATA CD driver during Windows installation.

Note
This procedure uses a generic approach to the Windows installation and the installation method might differ between versions of Windows. See the documentation for the version of Windows that you are installing.



 **Procedure** 

1. Start the virtual machine and connect to a graphical console.
1. Begin the Windows installation process.
1. Select the **Advanced** installation.
1. The storage destination will not be recognized until the driver is loaded. Click `    Load driver` .
1. The drivers are attached as a SATA CD drive. Click **OK** and browse the CD drive for the storage driver to load. The drivers are arranged hierarchically according to their driver type, operating system, and CPU architecture.
1. Repeat the previous two steps for all required drivers.
1. Complete the Windows installation.


#### 9.20.11.4. Creating a virtual machine snapshot in the web console




You can create a virtual machine (VM) snapshot by using the web console.

Note
To create snapshots of an online (Running state) VM with the highest integrity, install the QEMU guest agent.

The QEMU guest agent takes a consistent snapshot by attempting to quiesce the VM’s file system as much as possible, depending on the system workload. This ensures that in-flight I/O is written to the disk before the snapshot is taken. If the guest agent is not present, quiescing is not possible and a best-effort snapshot is taken. The conditions under which the snapshot was taken are reflected in the snapshot indications that are displayed in the web console or CLI.



The VM snapshot only includes disks that meet the following requirements:

- Must be either a data volume or persistent volume claim
- Belong to a storage class that supports Container Storage Interface (CSI) volume snapshots


 **Procedure** 

1. Click **Virtualization** → **VirtualMachines** from the side menu.
1. Select a virtual machine to open the **VirtualMachine details** page.
1. If the virtual machine is running, click **Actions** → **Stop** to power it down.
1. Click the **Snapshots** tab and then click **Take Snapshot** .
1. Fill in the **Snapshot Name** and optional **Description** fields.
1. Expand **Disks included in this Snapshot** to see the storage volumes to be included in the snapshot.
1. If your VM has disks that cannot be included in the snapshot and you still wish to proceed, select the **I am aware of this warning and wish to proceed** checkbox.
1. Click **Save** .


#### 9.20.11.5. Creating a virtual machine snapshot in the CLI




You can create a virtual machine (VM) snapshot for an offline or online VM by creating a `VirtualMachineSnapshot` object. Kubevirt will coordinate with the QEMU guest agent to create a snapshot of the online VM.

Note
To create snapshots of an online (Running state) VM with the highest integrity, install the QEMU guest agent.

The QEMU guest agent takes a consistent snapshot by attempting to quiesce the VM’s file system as much as possible, depending on the system workload. This ensures that in-flight I/O is written to the disk before the snapshot is taken. If the guest agent is not present, quiescing is not possible and a best-effort snapshot is taken. The conditions under which the snapshot was taken are reflected in the snapshot indications that are displayed in the web console or CLI.



 **Prerequisites** 

- Ensure that the persistent volume claims (PVCs) are in a storage class that supports Container Storage Interface (CSI) volume snapshots.
- Install the OpenShift CLI ( `    oc` ).
- Optional: Power down the VM for which you want to create a snapshot.


 **Procedure** 

1. Create a YAML file to define a `    VirtualMachineSnapshot` object that specifies the name of the new `    VirtualMachineSnapshot` and the name of the source VM.
    
    For example:
    
    
    ```
    apiVersion: snapshot.kubevirt.io/v1alpha1    kind: VirtualMachineSnapshot    metadata:      name: my-vmsnapshot<span id="CO97-1"><!--Empty--></span><span class="callout">1</span>spec:      source:        apiGroup: kubevirt.io        kind: VirtualMachine        name: my-vm<span id="CO97-2"><!--Empty--></span><span class="callout">2</span>
    ```
    
    
1. Create the `    VirtualMachineSnapshot` resource. The snapshot controller creates a `    VirtualMachineSnapshotContent` object, binds it to the `    VirtualMachineSnapshot` and updates the `    status` and `    readyToUse` fields of the `    VirtualMachineSnapshot` object.
    
    
    ```
    $ oc create -f &lt;my-vmsnapshot&gt;.yaml
    ```
    
    
1. Optional: If you are taking an online snapshot, you can use the `    wait` command and monitor the status of the snapshot:
    
    
    1. Enter the following command:
        
        
        ```
        $ oc wait my-vm my-vmsnapshot --for condition=Ready
        ```
        
        
    1. Verify the status of the snapshot:
        
        
        -  `            InProgress` - The online snapshot operation is still in progress.
        -  `            Succeeded` - The online snapshot operation completed successfully.
        -  `            Failed` - The online snapshot operaton failed.
            
            Note
            Online snapshots have a default time deadline of five minutes ( `            5m` ). If the snapshot does not complete successfully in five minutes, the status is set to `            failed` . Afterwards, the file system will be thawed and the VM unfrozen but the status remains `            failed` until you delete the failed snapshot image.
            
            To change the default time deadline, add the `            FailureDeadline` attribute to the VM snapshot spec with the time designated in minutes ( `            m` ) or in seconds ( `            s` ) that you want to specify before the snapshot operation times out.
            
            To set no deadline, you can specify `            0` , though this is generally not recommended, as it can result in an unresponsive VM.
            
            If you do not specify a unit of time such as `            m` or `            s` , the default is seconds ( `            s` ).
            
            
            
            
        
    


 **Verification** 

1. Verify that the `    VirtualMachineSnapshot` object is created and bound with `    VirtualMachineSnapshotContent` . The `    readyToUse` flag must be set to `    true` .
    
    
    ```
    $ oc describe vmsnapshot &lt;my-vmsnapshot&gt;
    ```
    
     **Example output** 
    
    
    ```
    apiVersion: snapshot.kubevirt.io/v1alpha1    kind: VirtualMachineSnapshot    metadata:      creationTimestamp: "2020-09-30T14:41:51Z"      finalizers:      - snapshot.kubevirt.io/vmsnapshot-protection      generation: 5      name: mysnap      namespace: default      resourceVersion: "3897"      selfLink: /apis/snapshot.kubevirt.io/v1alpha1/namespaces/default/virtualmachinesnapshots/my-vmsnapshot      uid: 28eedf08-5d6a-42c1-969c-2eda58e2a78d    spec:      source:        apiGroup: kubevirt.io        kind: VirtualMachine        name: my-vm    status:      conditions:      - lastProbeTime: null        lastTransitionTime: "2020-09-30T14:42:03Z"        reason: Operation complete        status: "False"<span id="CO98-1"><!--Empty--></span><span class="callout">1</span>type: Progressing      - lastProbeTime: null        lastTransitionTime: "2020-09-30T14:42:03Z"        reason: Operation complete        status: "True"<span id="CO98-2"><!--Empty--></span><span class="callout">2</span>type: Ready      creationTime: "2020-09-30T14:42:03Z"      readyToUse: true<span id="CO98-3"><!--Empty--></span><span class="callout">3</span>sourceUID: 355897f3-73a0-4ec4-83d3-3c2df9486f4f      virtualMachineSnapshotContentName: vmsnapshot-content-28eedf08-5d6a-42c1-969c-2eda58e2a78d<span id="CO98-4"><!--Empty--></span><span class="callout">4</span>
    ```
    
    
    
1. Check the `    spec:volumeBackups` property of the `    VirtualMachineSnapshotContent` resource to verify that the expected PVCs are included in the snapshot.


#### 9.20.11.6. Verifying online snapshot creation with snapshot indications




Snapshot indications are contextual information about online virtual machine (VM) snapshot operations. Indications are not available for offline virtual machine (VM) snapshot operations. Indications are helpful in describing details about the online snapshot creation.

 **Prerequisites** 

- To view indications, you must have attempted to create an online VM snapshot using the CLI or the web console.


 **Procedure** 

1. Display the output from the snapshot indications by doing one of the following:
    
    
    - For snapshots created with the CLI, view indicator output in the `        VirtualMachineSnapshot` object YAML, in the **status** field.
    - For snapshots created using the web console, click **VirtualMachineSnapshot > Status** in the **Snapshot details** screen.
    
1. Verify the status of your online VM snapshot:
    
    
    -  `        Online` indicates that the VM was running during online snapshot creation.
    -  `        NoGuestAgent` indicates that the QEMU guest agent was not running during online snapshot creation. The QEMU guest agent could not be used to freeze and thaw the file system, either because the QEMU guest agent was not installed or running or due to another error.
    


#### 9.20.11.7. Restoring a virtual machine from a snapshot in the web console




You can restore a virtual machine (VM) to a previous configuration represented by a snapshot in the web console.

 **Procedure** 

1. Click **Virtualization** → **VirtualMachines** from the side menu.
1. Select a virtual machine to open the **VirtualMachine details** page.
1. If the virtual machine is running, click **Actions** → **Stop** to power it down.
1. Click the **Snapshots** tab. The page displays a list of snapshots associated with the virtual machine.
1. Choose one of the following methods to restore a VM snapshot:
    
    
    1. For the snapshot that you want to use as the source to restore the VM, click **Restore** .
    1. Select a snapshot to open the **Snapshot Details** screen and click **Actions** → **Restore VirtualMachineSnapshot** .
    
1. In the confirmation pop-up window, click **Restore** to restore the VM to its previous configuration represented by the snapshot.


#### 9.20.11.8. Restoring a virtual machine from a snapshot in the CLI




You can restore an existing virtual machine (VM) to a previous configuration by using a VM snapshot. You can only restore from an offline VM snapshot.

 **Prerequisites** 

- Install the OpenShift CLI ( `    oc` ).
- Power down the VM you want to restore to a previous state.


 **Procedure** 

1. Create a YAML file to define a `    VirtualMachineRestore` object that specifies the name of the VM you want to restore and the name of the snapshot to be used as the source.
    
    For example:
    
    
    ```
    apiVersion: snapshot.kubevirt.io/v1alpha1    kind: VirtualMachineRestore    metadata:      name: my-vmrestore<span id="CO99-1"><!--Empty--></span><span class="callout">1</span>spec:      target:        apiGroup: kubevirt.io        kind: VirtualMachine        name: my-vm<span id="CO99-2"><!--Empty--></span><span class="callout">2</span>virtualMachineSnapshotName: my-vmsnapshot<span id="CO99-3"><!--Empty--></span><span class="callout">3</span>
    ```
    
    
1. Create the `    VirtualMachineRestore` resource. The snapshot controller updates the status fields of the `    VirtualMachineRestore` object and replaces the existing VM configuration with the snapshot content.
    
    
    ```
    $ oc create -f &lt;my-vmrestore&gt;.yaml
    ```
    
    


 **Verification** 

- Verify that the VM is restored to the previous state represented by the snapshot. The `    complete` flag must be set to `    true` .
    
    
    ```
    $ oc get vmrestore &lt;my-vmrestore&gt;
    ```
    
     **Example output** 
    
    
    ```
    apiVersion: snapshot.kubevirt.io/v1alpha1    kind: VirtualMachineRestore    metadata:    creationTimestamp: "2020-09-30T14:46:27Z"    generation: 5    name: my-vmrestore    namespace: default    ownerReferences:    - apiVersion: kubevirt.io/v1      blockOwnerDeletion: true      controller: true      kind: VirtualMachine      name: my-vm      uid: 355897f3-73a0-4ec4-83d3-3c2df9486f4f      resourceVersion: "5512"      selfLink: /apis/snapshot.kubevirt.io/v1alpha1/namespaces/default/virtualmachinerestores/my-vmrestore      uid: 71c679a8-136e-46b0-b9b5-f57175a6a041      spec:        target:          apiGroup: kubevirt.io          kind: VirtualMachine          name: my-vm      virtualMachineSnapshotName: my-vmsnapshot      status:      complete: true<span id="CO100-1"><!--Empty--></span><span class="callout">1</span>conditions:      - lastProbeTime: null      lastTransitionTime: "2020-09-30T14:46:28Z"      reason: Operation complete      status: "False"<span id="CO100-2"><!--Empty--></span><span class="callout">2</span>type: Progressing      - lastProbeTime: null      lastTransitionTime: "2020-09-30T14:46:28Z"      reason: Operation complete      status: "True"<span id="CO100-3"><!--Empty--></span><span class="callout">3</span>type: Ready      deletedDataVolumes:      - test-dv1      restoreTime: "2020-09-30T14:46:28Z"      restores:      - dataVolumeName: restore-71c679a8-136e-46b0-b9b5-f57175a6a041-datavolumedisk1      persistentVolumeClaim: restore-71c679a8-136e-46b0-b9b5-f57175a6a041-datavolumedisk1      volumeName: datavolumedisk1      volumeSnapshotName: vmsnapshot-28eedf08-5d6a-42c1-969c-2eda58e2a78d-volume-datavolumedisk1
    ```
    
    
    


#### 9.20.11.9. Deleting a virtual machine snapshot in the web console




You can delete an existing virtual machine snapshot by using the web console.

 **Procedure** 

1. Click **Virtualization** → **VirtualMachines** from the side menu.
1. Select a virtual machine to open the **VirtualMachine details** page.
1. Click the **Snapshots** tab. The page displays a list of snapshots associated with the virtual machine.
1. Click the Options menu![kebab](https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.11-Virtualization-en-US/images/f468284ec3cc9bf27e6bd2c83849ca50/kebab.png)
    of the virtual machine snapshot that you want to delete and select **Delete VirtualMachineSnapshot** .
1. In the confirmation pop-up window, click **Delete** to delete the snapshot.


#### 9.20.11.10. Deleting a virtual machine snapshot in the CLI




You can delete an existing virtual machine (VM) snapshot by deleting the appropriate `VirtualMachineSnapshot` object.

 **Prerequisites** 

- Install the OpenShift CLI ( `    oc` ).


 **Procedure** 

- Delete the `    VirtualMachineSnapshot` object. The snapshot controller deletes the `    VirtualMachineSnapshot` along with the associated `    VirtualMachineSnapshotContent` object.
    
    
    ```
    $ oc delete vmsnapshot &lt;my-vmsnapshot&gt;
    ```
    
    


 **Verification** 

- Verify that the snapshot is deleted and no longer attached to this VM:
    
    
    ```
    $ oc get vmsnapshot
    ```
    
    


#### 9.20.11.11. Additional resources




-  [CSI Volume Snapshots](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/storage/#persistent-storage-csi-snapshots) 


### 9.20.12. Moving a local virtual machine disk to a different node




Virtual machines that use local volume storage can be moved so that they run on a specific node.

You might want to move the virtual machine to a specific node for the following reasons:

- The current node has limitations to the local storage configuration.
- The new node is better optimized for the workload of that virtual machine.


To move a virtual machine that uses local storage, you must clone the underlying volume by using a data volume. After the cloning operation is complete, you can [edit the virtual machine configuration](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/virtualization/#virt-edit-vms) so that it uses the new data volume, or [add the new data volume to another virtual machine](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/virtualization/#virt-add-disk-to-vm_virt-edit-vms) .

Tip
When you enable preallocation globally, or for a single data volume, the Containerized Data Importer (CDI) preallocates disk space during cloning. Preallocation enhances write performance. For more information, see [Using preallocation for data volumes](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/virtualization/#virt-using-preallocation-for-datavolumes) .



Note
Users without the `cluster-admin` role require [additional user permissions](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/virtualization/#virt-enabling-user-permissions-to-clone-datavolumes) to clone volumes across namespaces.



#### 9.20.12.1. Cloning a local volume to another node




You can move a virtual machine disk so that it runs on a specific node by cloning the underlying persistent volume claim (PVC).

To ensure the virtual machine disk is cloned to the correct node, you must either create a new persistent volume (PV) or identify one on the correct node. Apply a unique label to the PV so that it can be referenced by the data volume.

Note
The destination PV must be the same size or larger than the source PVC. If the destination PV is smaller than the source PVC, the cloning operation fails.



 **Prerequisites** 

- The virtual machine must not be running. Power down the virtual machine before cloning the virtual machine disk.


 **Procedure** 

1. Either create a new local PV on the node, or identify a local PV already on the node:
    
    
    - Create a local PV that includes the `        nodeAffinity.nodeSelectorTerms` parameters. The following manifest creates a `        10Gi` local PV on `        node01` .
        
        
        ```
        kind: PersistentVolume        apiVersion: v1        metadata:          name: &lt;destination-pv&gt;<span id="CO101-1"><!--Empty--></span><span class="callout">1</span>annotations:        spec:          accessModes:          - ReadWriteOnce          capacity:            storage: 10Gi<span id="CO101-2"><!--Empty--></span><span class="callout">2</span>local:            path: /mnt/local-storage/local/disk1<span id="CO101-3"><!--Empty--></span><span class="callout">3</span>nodeAffinity:            required:              nodeSelectorTerms:              - matchExpressions:                - key: kubernetes.io/hostname                  operator: In                  values:                  - node01<span id="CO101-4"><!--Empty--></span><span class="callout">4</span>persistentVolumeReclaimPolicy: Delete          storageClassName: local          volumeMode: Filesystem
        ```
        
        
    - Identify a PV that already exists on the target node. You can identify the node where a PV is provisioned by viewing the `        nodeAffinity` field in its configuration:
        
        
        ```
        $ oc get pv &lt;destination-pv&gt; -o yaml
        ```
        
        The following snippet shows that the PV is on `        node01` :
        
         **Example output** 
        
        
        ```
        ...        spec:          nodeAffinity:            required:              nodeSelectorTerms:              - matchExpressions:                - key: kubernetes.io/hostname<span id="CO102-1"><!--Empty--></span><span class="callout">1</span>operator: In                  values:                  - node01<span id="CO102-2"><!--Empty--></span><span class="callout">2</span>...
        ```
        
        
        
    
1. Add a unique label to the PV:
    
    
    ```
    $ oc label pv &lt;destination-pv&gt; node=node01
    ```
    
    
1. Create a data volume manifest that references the following:
    
    
    - The PVC name and namespace of the virtual machine.
    - The label you applied to the PV in the previous step.
    - The size of the destination PV.
        
        
        ```
        apiVersion: cdi.kubevirt.io/v1beta1        kind: DataVolume        metadata:          name: &lt;clone-datavolume&gt;<span id="CO103-1"><!--Empty--></span><span class="callout">1</span>spec:          source:            pvc:              name: "&lt;source-vm-disk&gt;"<span id="CO103-2"><!--Empty--></span><span class="callout">2</span>namespace: "&lt;source-namespace&gt;"<span id="CO103-3"><!--Empty--></span><span class="callout">3</span>pvc:            accessModes:              - ReadWriteOnce            selector:              matchLabels:                node: node01<span id="CO103-4"><!--Empty--></span><span class="callout">4</span>resources:              requests:                storage: &lt;10Gi&gt;<span id="CO103-5"><!--Empty--></span><span class="callout">5</span>
        ```
        
        
    
1. Start the cloning operation by applying the data volume manifest to your cluster:
    
    
    ```
    $ oc apply -f &lt;clone-datavolume.yaml&gt;
    ```
    
    


The data volume clones the PVC of the virtual machine into the PV on the specific node.

### 9.20.13. Expanding virtual storage by adding blank disk images




You can increase your storage capacity or create new data partitions by adding blank disk images to OpenShift Virtualization.

#### 9.20.13.1. About data volumes




 `DataVolume` objects are custom resources that are provided by the Containerized Data Importer (CDI) project. Data volumes orchestrate import, clone, and upload operations that are associated with an underlying persistent volume claim (PVC). Data volumes are integrated with OpenShift Virtualization, and they prevent a virtual machine from being started before the PVC has been prepared.

#### 9.20.13.2. Creating a blank disk image with data volumes




You can create a new blank disk image in a persistent volume claim by customizing and deploying a data volume configuration file.

 **Prerequisites** 

- At least one available persistent volume.
- Install the OpenShift CLI ( `    oc` ).


 **Procedure** 

1. Edit the `    DataVolume` manifest:
    
    
    ```
    apiVersion: cdi.kubevirt.io/v1beta1    kind: DataVolume    metadata:      name: blank-image-datavolume    spec:      source:          blank: {}      pvc:        # Optional: Set the storage class or omit to accept the default        # storageClassName: "hostpath"        accessModes:          - ReadWriteOnce        resources:          requests:            storage: 500Mi
    ```
    
    
1. Create the blank disk image by running the following command:
    
    
    ```
    $ oc create -f &lt;blank-image-datavolume&gt;.yaml
    ```
    
    


#### 9.20.13.3. Additional resources




-  [Configure preallocation mode](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/virtualization/#virt-using-preallocation-for-datavolumes) to improve write performance for data volume operations.


### 9.20.14. Cloning a data volume using smart-cloning




Smart-cloning is a built-in feature of Red Hat OpenShift Data Foundation. Smart-cloning is faster and more efficient than host-assisted cloning.

You do not need to perform any action to enable smart-cloning, but you need to ensure your storage environment is compatible with smart-cloning to use this feature.

When you create a data volume with a persistent volume claim (PVC) source, you automatically initiate the cloning process. You always receive a clone of the data volume if your environment supports smart-cloning or not. However, you will only receive the performance benefits of smart cloning if your storage provider supports smart-cloning.

#### 9.20.14.1. About smart-cloning




When a data volume is smart-cloned, the following occurs:

1. A snapshot of the source persistent volume claim (PVC) is created.
1. A PVC is created from the snapshot.
1. The snapshot is deleted.


#### 9.20.14.2. Cloning a data volume




 **Prerequisites** 

For smart-cloning to occur, the following conditions are required:


- Your storage provider must support snapshots.
- The source and target PVCs must be defined to the same storage class.
- The source and target PVCs share the same **volumeMode** .
- The `    VolumeSnapshotClass` object must reference the storage class defined to both the source and target PVCs.


 **Procedure** 

To initiate cloning of a data volume:


1. Create a YAML file for a `    DataVolume` object that specifies the name of the new data volume and the name and namespace of the source PVC. In this example, because you specify the **storage** API, there is no need to specify **accessModes** or **volumeMode** . The optimal values will be calculated for you automatically.
    
    
    ```
    apiVersion: cdi.kubevirt.io/v1beta1    kind: DataVolume    metadata:      name: &lt;cloner-datavolume&gt;<span id="CO104-1"><!--Empty--></span><span class="callout">1</span>spec:      source:        pvc:          namespace: "&lt;source-namespace&gt;"<span id="CO104-2"><!--Empty--></span><span class="callout">2</span>name: "&lt;my-favorite-vm-disk&gt;"<span id="CO104-3"><!--Empty--></span><span class="callout">3</span>storage:<span id="CO104-4"><!--Empty--></span><span class="callout">4</span>resources:          requests:            storage: &lt;2Gi&gt;<span id="CO104-5"><!--Empty--></span><span class="callout">5</span>
    ```
    
    
1. Start cloning the PVC by creating the data volume:
    
    
    ```
    $ oc create -f &lt;cloner-datavolume&gt;.yaml
    ```
    
    Note
    Data volumes prevent a virtual machine from starting before the PVC is prepared, so you can create a virtual machine that references the new data volume while the PVC clones.
    
    
    
    


#### 9.20.14.3. Additional resources




-  [Cloning the persistent volume claim of a virtual machine disk into a new data volume](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/virtualization/#virt-cloning-pvc-of-vm-disk-into-new-datavolume_virt-cloning-vm-disk-into-new-datavolume) 
-  [Configure preallocation mode](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/virtualization/#virt-using-preallocation-for-datavolumes) to improve write performance for data volume operations.
-  [Customizing the storage profile](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/virtualization/#virt-customizing-storage-profile_virt-creating-data-volumes) 


### 9.20.15. Creating and using boot sources




A boot source contains a bootable operating system (OS) and all of the configuration settings for the OS, such as drivers.

You use a boot source to create virtual machine templates with specific configurations. These templates can be used to create any number of available virtual machines.

Quick Start tours are available in the OpenShift Container Platform web console to assist you in creating a custom boot source, uploading a boot source, and other tasks. Select **Quick Starts** from the **Help** menu to view the Quick Start tours.

#### 9.20.15.1. About virtual machines and boot sources




Virtual machines consist of a virtual machine definition and one or more disks that are backed by data volumes. Virtual machine templates enable you to create virtual machines using predefined virtual machine specifications.

Every virtual machine template requires a boot source, which is a fully configured virtual machine disk image including configured drivers. Each virtual machine template contains a virtual machine definition with a pointer to the boot source. Each boot source has a predefined name and namespace. For some operating systems, a boot source is automatically provided. If it is not provided, then an administrator must prepare a custom boot source.

Provided boot sources are updated automatically to the latest version of the operating system. For auto-updated boot sources, persistent volume claims (PVCs) are created with the cluster’s default storage class. If you select a different default storage class after configuration, you must delete the existing data volumes in the cluster namespace that are configured with the previous default storage class.

To use the boot sources feature, install the latest release of OpenShift Virtualization. The namespace `openshift-virtualization-os-images` enables the feature and is installed with the OpenShift Virtualization Operator. Once the boot source feature is installed, you can create boot sources, attach them to templates, and create virtual machines from the templates.

Define a boot source by using a persistent volume claim (PVC) that is populated by uploading a local file, cloning an existing PVC, importing from a registry, or by URL. Attach a boot source to a virtual machine template by using the web console. After the boot source is attached to a virtual machine template, you create any number of fully configured ready-to-use virtual machines from the template.

#### 9.20.15.2. Importing a RHEL image as a boot source




You can import a Red Hat Enterprise Linux (RHEL) image as a boot source by specifying a URL for the image.

 **Prerequisites** 

- You must have access to a web page with the operating system image. For example: Download Red Hat Enterprise Linux web page with images.


 **Procedure** 

1. In the OpenShift Container Platform console, click **Virtualization** → **Templates** from the side menu.
1. Identify the RHEL template for which you want to configure a boot source and click **Add source** .
1. In the **Add boot source to template** window, select **URL (creates PVC)** from the **Boot source type** list.
1. Click **RHEL download page** to access the Red Hat Customer Portal. A list of available installers and images is displayed on the Download Red Hat Enterprise Linux page.
1. Identify the Red Hat Enterprise Linux KVM guest image that you want to download. Right-click **Download Now** , and copy the URL for the image.
1. In the **Add boot source to template** window, paste the URL into the **Import URL** field, and click **Save and import** .


 **Verification** 

1. Verify that the template displays a green checkmark in the **Boot source** column on the **Templates** page.


You can now use this template to create RHEL virtual machines.

#### 9.20.15.3. Adding a boot source for a virtual machine template




A boot source can be configured for any virtual machine template that you want to use for creating virtual machines or custom templates. When virtual machine templates are configured with a boot source, they are labeled **Source available** on the **Templates** page. After you add a boot source to a template, you can create a new virtual machine from the template.

There are four methods for selecting and adding a boot source in the web console:

-  **Upload local file (creates PVC)** 
-  **URL (creates PVC)** 
-  **Clone (creates PVC)** 
-  **Registry (creates PVC)** 


 **Prerequisites** 

- To add a boot source, you must be logged in as a user with the `    os-images.kubevirt.io:edit` RBAC role or as an administrator. You do not need special privileges to create a virtual machine from a template with a boot source added.
- To upload a local file, the operating system image file must exist on your local machine.
- To import via URL, access to the web server with the operating system image is required. For example: the Red Hat Enterprise Linux web page with images.
- To clone an existing PVC, access to the project with a PVC is required.
- To import via registry, access to the container registry is required.


 **Procedure** 

1. In the OpenShift Container Platform console, click **Virtualization** → **Templates** from the side menu.
1. Click the options menu beside a template and select **Edit boot source** .
1. Click **Add disk** .
1. In the **Add disk** window, select **Use this disk as a boot source** .
1. Enter the disk name and select a **Source** , for example, **Blank (creates PVC)** or **Use an existing PVC** .
1. Enter a value for **Persistent Volume Claim size** to specify the PVC size that is adequate for the uncompressed image and any additional space that is required.
1. Select a **Type** , for example, **Disk** or **CD-ROM** .
1. Optional: Click **Storage class** and select the storage class that is used to create the disk. Typically, this storage class is the default storage class that is created for use by all PVCs.
    
    Note
    Provided boot sources are updated automatically to the latest version of the operating system. For auto-updated boot sources, persistent volume claims (PVCs) are created with the cluster’s default storage class. If you select a different default storage class after configuration, you must delete the existing data volumes in the cluster namespace that are configured with the previous default storage class.
    
    
    
    
1. Optional: Clear **Apply optimized StorageProfile settings** to edit the access mode or volume mode.
1. Select the appropriate method to save your boot source:
    
    
    1. Click **Save and upload** if you uploaded a local file.
    1. Click **Save and import** if you imported content from a URL or the registry.
    1. Click **Save and clone** if you cloned an existing PVC.
    


Your custom virtual machine template with a boot source is listed on the **Catalog** page. You can use this template to create a virtual machine.

#### 9.20.15.4. Creating a virtual machine from a template with an attached boot source




After you add a boot source to a template, you can create a virtual machine from the template.

 **Procedure** 

1. In the OpenShift Container Platform web console, click **Virtualization** → **Catalog** in the side menu.
1. Select the updated template and click **Quick create VirtualMachine** .


The **VirtualMachine details** is displayed with the status **Starting** .

#### 9.20.15.5. Additional resources




-  [Creating virtual machine templates](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/virtualization/#virt-create-vms) 
-  [Automatic importing and updating of pre-defined boot sources](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/virtualization/#virt-automatic-bootsource-updates) 


### 9.20.16. Hot plugging virtual disks




You can add or remove virtual disks without stopping your virtual machine (VM) or virtual machine instance (VMI).

#### 9.20.16.1. About hot plugging virtual disks




When you _hot plug_ a virtual disk, you attach a virtual disk to a virtual machine instance while the virtual machine is running.

When you _hot unplug_ a virtual disk, you detach a virtual disk from a virtual machine instance while the virtual machine is running.

Only data volumes and persistent volume claims (PVCs) can be hot plugged and hot unplugged. You cannot hot plug or hot unplug container disks.

After you hot plug a virtual disk, it remains attached until you detach it, even if you restart the virtual machine.

#### 9.20.16.2. About virtio-scsi




In OpenShift Virtualization, each virtual machine (VM) has a `virtio-scsi` controller so that hot plugged disks can use a `scsi` bus. The `virtio-scsi` controller overcomes the limitations of `virtio` while retaining its performance advantages. It is highly scalable and supports hot plugging over 4 million disks.

Regular `virtio` is not available for hot plugged disks because it is not scalable: each `virtio` disk uses one of the limited PCI Express (PCIe) slots in the VM. PCIe slots are also used by other devices and must be reserved in advance, therefore slots might not be available on demand.

#### 9.20.16.3. Hot plugging a virtual disk using the CLI




Hot plug virtual disks that you want to attach to a virtual machine instance (VMI) while a virtual machine is running.

 **Prerequisites** 

- You must have a running virtual machine to hot plug a virtual disk.
- You must have at least one data volume or persistent volume claim (PVC) available for hot plugging.


 **Procedure** 

- Hot plug a virtual disk by running the following command:
    
    
    ```
    $ virtctl addvolume &lt;virtual-machine|virtual-machine-instance&gt; --volume-name=&lt;datavolume|PVC&gt; \    [--persist] [--serial=&lt;label-name&gt;]
    ```
    
    
    - Use the optional `        --persist` flag to add the hot plugged disk to the virtual machine specification as a permanently mounted virtual disk. Stop, restart, or reboot the virtual machine to permanently mount the virtual disk. After specifying the `        --persist` flag, you can no longer hot plug or hot unplug the virtual disk. The `        --persist` flag applies to virtual machines, not virtual machine instances.
    - The optional `        --serial` flag allows you to add an alphanumeric string label of your choice. This helps you to identify the hot plugged disk in a guest virtual machine. If you do not specify this option, the label defaults to the name of the hot plugged data volume or PVC.
    


#### 9.20.16.4. Hot unplugging a virtual disk using the CLI




Hot unplug virtual disks that you want to detach from a virtual machine instance (VMI) while a virtual machine is running.

 **Prerequisites** 

- Your virtual machine must be running.
- You must have at least one data volume or persistent volume claim (PVC) available and hot plugged.


 **Procedure** 

- Hot unplug a virtual disk by running the following command:
    
    
    ```
    $ virtctl removevolume &lt;virtual-machine|virtual-machine-instance&gt; --volume-name=&lt;datavolume|PVC&gt;
    ```
    
    


#### 9.20.16.5. Hot plugging a virtual disk using the web console




Hot plug virtual disks that you want to attach to a virtual machine instance (VMI) while a virtual machine is running. When you hot plug a virtual disk, it remains attached to the VMI until you unplug it.

 **Prerequisites** 

- You must have a running virtual machine to hot plug a virtual disk.


 **Procedure** 

1. Click **Virtualization** → **VirtualMachines** from the side menu.
1. Select the running virtual machine to which you want to hot plug a virtual disk.
1. On the **VirtualMachine details** page, click the **Disks** tab.
1. Click **Add disk** .
1. In the **Add disk (hot plugged)** window, fill in the information for the virtual disk that you want to hot plug.
1. Click **Save** .


#### 9.20.16.6. Hot unplugging a virtual disk using the web console




Hot unplug virtual disks that you want to detach from a virtual machine instance (VMI) while a virtual machine is running.

 **Prerequisites** 

- Your virtual machine must be running with a hot plugged disk attached.


 **Procedure** 

1. Click **Virtualization** → **VirtualMachines** from the side menu.
1. Select the running virtual machine with the disk you want to hot unplug to open the **VirtualMachine details** page.
1. On the **Disks** tab, click the Options menu![kebab](https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.11-Virtualization-en-US/images/f468284ec3cc9bf27e6bd2c83849ca50/kebab.png)
    of the virtual disk that you want to hot unplug.
1. Click **Detach** .


### 9.20.17. Using container disks with virtual machines




You can build a virtual machine image into a container disk and store it in your container registry. You can then import the container disk into persistent storage for a virtual machine or attach it directly to the virtual machine for ephemeral storage.

Important
If you use large container disks, I/O traffic might increase, impacting worker nodes. This can lead to unavailable nodes. You can resolve this by:

-  [Pruning DeploymentConfig objects](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/building_applications/#pruning-deployments_pruning-objects) 
-  [Configuring garbage collection](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/nodes/#nodes-nodes-garbage-collection-configuring_nodes-nodes-configuring) 




#### 9.20.17.1. About container disks




A container disk is a virtual machine image that is stored as a container image in a container image registry. You can use container disks to deliver the same disk images to multiple virtual machines and to create large numbers of virtual machine clones.

A container disk can either be imported into a persistent volume claim (PVC) by using a data volume that is attached to a virtual machine, or attached directly to a virtual machine as an ephemeral `containerDisk` volume.

##### 9.20.17.1.1. Importing a container disk into a PVC by using a data volume




Use the Containerized Data Importer (CDI) to import the container disk into a PVC by using a data volume. You can then attach the data volume to a virtual machine for persistent storage.

##### 9.20.17.1.2. Attaching a container disk to a virtual machine as a `containerDisk` volume




A `containerDisk` volume is ephemeral. It is discarded when the virtual machine is stopped, restarted, or deleted. When a virtual machine with a `containerDisk` volume starts, the container image is pulled from the registry and hosted on the node that is hosting the virtual machine.

Use `containerDisk` volumes for read-only file systems such as CD-ROMs or for disposable virtual machines.

Important
Using `containerDisk` volumes for read-write file systems is not recommended because the data is temporarily written to local storage on the hosting node. This slows live migration of the virtual machine, such as in the case of node maintenance, because the data must be migrated to the destination node. Additionally, all data is lost if the node loses power or otherwise shuts down unexpectedly.



#### 9.20.17.2. Preparing a container disk for virtual machines




You must build a container disk with a virtual machine image and push it to a container registry before it can used with a virtual machine. You can then either import the container disk into a PVC using a data volume and attach it to a virtual machine, or you can attach the container disk directly to a virtual machine as an ephemeral `containerDisk` volume.

The size of a disk image inside a container disk is limited by the maximum layer size of the registry where the container disk is hosted.

Note
For [Red Hat Quay](https://access.redhat.com/documentation/en-us/red_hat_quay/) , you can change the maximum layer size by editing the YAML configuration file that is created when Red Hat Quay is first deployed.



 **Prerequisites** 

- Install `    podman` if it is not already installed.
- The virtual machine image must be either QCOW2 or RAW format.


 **Procedure** 

1. Create a Dockerfile to build the virtual machine image into a container image. The virtual machine image must be owned by QEMU, which has a UID of `    107` , and placed in the `    /disk/` directory inside the container. Permissions for the `    /disk/` directory must then be set to `    0440` .
    
    The following example uses the Red Hat Universal Base Image (UBI) to handle these configuration changes in the first stage, and uses the minimal `    scratch` image in the second stage to store the result:
    
    
    ```
    $ cat &gt; Dockerfile &lt;&lt; EOF    FROM registry.access.redhat.com/ubi8/ubi:latest AS builder    ADD --chown=107:107 &lt;vm_image&gt;.qcow2 /disk/<span id="CO105-1"><!--Empty--></span><span class="callout">1</span>RUN chmod 0440 /disk/*        FROM scratch    COPY --from=builder /disk/* /disk/    EOF
    ```
    
    
1. Build and tag the container:
    
    
    ```
    $ podman build -t &lt;registry&gt;/&lt;container_disk_name&gt;:latest .
    ```
    
    
1. Push the container image to the registry:
    
    
    ```
    $ podman push &lt;registry&gt;/&lt;container_disk_name&gt;:latest
    ```
    
    


If your container registry does not have TLS you must add it as an insecure registry before you can import container disks into persistent storage.

#### 9.20.17.3. Disabling TLS for a container registry to use as insecure registry




You can disable TLS (transport layer security) for one or more container registries by editing the `insecureRegistries` field of the `HyperConverged` custom resource.

 **Prerequisites** 

- Log in to the cluster as a user with the `    cluster-admin` role.


 **Procedure** 

- Edit the `    HyperConverged` custom resource and add a list of insecure registries to the `    spec.storageImport.insecureRegistries` field.
    
    
    ```
    apiVersion: hco.kubevirt.io/v1beta1    kind: HyperConverged    metadata:      name: kubevirt-hyperconverged      namespace: openshift-cnv    spec:      storageImport:        insecureRegistries:<span id="CO106-1"><!--Empty--></span><span class="callout">1</span>- "private-registry-example-1:5000"          - "private-registry-example-2:5000"
    ```
    
    


#### 9.20.17.4. Next steps




-  [Import the container disk into persistent storage for a virtual machine](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/virtualization/#virt-importing-virtual-machine-images-datavolumes) .
-  [Create a virtual machine](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/virtualization/#virt-create-vms) that uses a `    containerDisk` volume for ephemeral storage.


### 9.20.18. Preparing CDI scratch space




#### 9.20.18.1. About data volumes




 `DataVolume` objects are custom resources that are provided by the Containerized Data Importer (CDI) project. Data volumes orchestrate import, clone, and upload operations that are associated with an underlying persistent volume claim (PVC). Data volumes are integrated with OpenShift Virtualization, and they prevent a virtual machine from being started before the PVC has been prepared.

#### 9.20.18.2. About scratch space




The Containerized Data Importer (CDI) requires scratch space (temporary storage) to complete some operations, such as importing and uploading virtual machine images. During this process, CDI provisions a scratch space PVC equal to the size of the PVC backing the destination data volume (DV). The scratch space PVC is deleted after the operation completes or aborts.

You can define the storage class that is used to bind the scratch space PVC in the `spec.scratchSpaceStorageClass` field of the `HyperConverged` custom resource.

If the defined storage class does not match a storage class in the cluster, then the default storage class defined for the cluster is used. If there is no default storage class defined in the cluster, the storage class used to provision the original DV or PVC is used.

Note
CDI requires requesting scratch space with a `file` volume mode, regardless of the PVC backing the origin data volume. If the origin PVC is backed by `block` volume mode, you must define a storage class capable of provisioning `file` volume mode PVCs.




<span id="manual-provisioning"></span>
###### Manual provisioning


If there are no storage classes, CDI uses any PVCs in the project that match the size requirements for the image. If there are no PVCs that match these requirements, the CDI import pod remains in a **Pending** state until an appropriate PVC is made available or until a timeout function kills the pod.

#### 9.20.18.3. CDI operations that require scratch space




| Type | Reason |
| --- | --- |
| Registry imports | CDI must download the image to a scratch space and extract the layers to find the image file. The image file is then passed to QEMU-IMG for conversion to a raw disk. |
| Upload image | QEMU-IMG does not accept input from STDIN. Instead, the image to upload is saved in scratch space before it can be passed to QEMU-IMG for conversion. |
| HTTP imports of archived images | QEMU-IMG does not know how to handle the archive formats CDI supports. Instead, the image is unarchived and saved into scratch space before it is passed to QEMU-IMG. |
| HTTP imports of authenticated images | QEMU-IMG inadequately handles authentication. Instead, the image is saved to scratch space and authenticated before it is passed to QEMU-IMG. |
| HTTP imports of custom certificates | QEMU-IMG inadequately handles custom certificates of HTTPS endpoints. Instead, CDI downloads the image to scratch space before passing the file to QEMU-IMG. |


#### 9.20.18.4. Defining a storage class




You can define the storage class that the Containerized Data Importer (CDI) uses when allocating scratch space by adding the `spec.scratchSpaceStorageClass` field to the `HyperConverged` custom resource (CR).

 **Prerequisites** 

- Install the OpenShift CLI ( `    oc` ).


 **Procedure** 

1. Edit the `    HyperConverged` CR by running the following command:
    
    
    ```
    $ oc edit hco -n openshift-cnv kubevirt-hyperconverged
    ```
    
    
1. Add the `    spec.scratchSpaceStorageClass` field to the CR, setting the value to the name of a storage class that exists in the cluster:
    
    
    ```
    apiVersion: hco.kubevirt.io/v1beta1    kind: HyperConverged    metadata:      name: kubevirt-hyperconverged    spec:      scratchSpaceStorageClass: "&lt;storage_class&gt;"<span id="CO107-1"><!--Empty--></span><span class="callout">1</span>
    ```
    
    
1. Save and exit your default editor to update the `    HyperConverged` CR.


#### 9.20.18.5. CDI supported operations matrix




This matrix shows the supported CDI operations for content types against endpoints, and which of these operations requires scratch space.

| Content types | HTTP | HTTPS | HTTP basic auth | Registry | Upload |
| --- | --- | --- | --- | --- | --- |
| KubeVirt (QCOW2) | ✓ QCOW2    
✓ GZ*    
✓ XZ* | ✓ QCOW2**    
✓ GZ*    
✓ XZ* | ✓ QCOW2    
✓ GZ*    
✓ XZ* | ✓ QCOW2*    
□ GZ    
□ XZ | ✓ QCOW2*    
✓ GZ*    
✓ XZ* |
| KubeVirt (RAW) | ✓ RAW    
✓ GZ    
✓ XZ | ✓ RAW    
✓ GZ    
✓ XZ | ✓ RAW    
✓ GZ    
✓ XZ | ✓ RAW*    
□ GZ    
□ XZ | ✓ RAW*    
✓ GZ*    
✓ XZ* |


✓ Supported operation

□ Unsupported operation

* Requires scratch space

** Requires scratch space if a custom certificate authority is required

#### 9.20.18.6. Additional resources




-  [Dynamic provisioning](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/storage/#about_dynamic-provisioning) 


### 9.20.19. Re-using persistent volumes




To re-use a statically provisioned persistent volume (PV), you must first reclaim the volume. This involves deleting the PV so that the storage configuration can be re-used.

#### 9.20.19.1. About reclaiming statically provisioned persistent volumes




When you reclaim a persistent volume (PV), you unbind the PV from a persistent volume claim (PVC) and delete the PV. Depending on the underlying storage, you might need to manually delete the shared storage.

You can then re-use the PV configuration to create a PV with a different name.

Statically provisioned PVs must have a reclaim policy of `Retain` to be reclaimed. If they do not, the PV enters a failed state when the PVC is unbound from the PV.

Important
The `Recycle` reclaim policy is deprecated in OpenShift Container Platform 4.



#### 9.20.19.2. Reclaiming statically provisioned persistent volumes




Reclaim a statically provisioned persistent volume (PV) by unbinding the persistent volume claim (PVC) and deleting the PV. You might also need to manually delete the shared storage.

Reclaiming a statically provisioned PV is dependent on the underlying storage. This procedure provides a general approach that might need to be customized depending on your storage.

 **Procedure** 

1. Ensure that the reclaim policy of the PV is set to `    Retain` :
    
    
    1. Check the reclaim policy of the PV:
        
        
        ```
        $ oc get pv &lt;pv_name&gt; -o yaml | grep 'persistentVolumeReclaimPolicy'
        ```
        
        
    1. If the `        persistentVolumeReclaimPolicy` is not set to `        Retain` , edit the reclaim policy with the following command:
        
        
        ```
        $ oc patch pv &lt;pv_name&gt; -p '{"spec":{"persistentVolumeReclaimPolicy":"Retain"}}'
        ```
        
        
    
1. Ensure that no resources are using the PV:
    
    
    ```
    $ oc describe pvc &lt;pvc_name&gt; | grep 'Mounted By:'
    ```
    
    Remove any resources that use the PVC before continuing.
    
    
1. Delete the PVC to release the PV:
    
    
    ```
    $ oc delete pvc &lt;pvc_name&gt;
    ```
    
    
1. Optional: Export the PV configuration to a YAML file. If you manually remove the shared storage later in this procedure, you can refer to this configuration. You can also use `    spec` parameters in this file as the basis to create a new PV with the same storage configuration after you reclaim the PV:
    
    
    ```
    $ oc get pv &lt;pv_name&gt; -o yaml &gt; &lt;file_name&gt;.yaml
    ```
    
    
1. Delete the PV:
    
    
    ```
    $ oc delete pv &lt;pv_name&gt;
    ```
    
    
1. Optional: Depending on the storage type, you might need to remove the contents of the shared storage folder:
    
    
    ```
    $ rm -rf &lt;path_to_share_storage&gt;
    ```
    
    
1. Optional: Create a PV that uses the same storage configuration as the deleted PV. If you exported the reclaimed PV configuration earlier, you can use the `    spec` parameters of that file as the basis for a new PV manifest:
    
    Note
    To avoid possible conflict, it is good practice to give the new PV object a different name than the one that you deleted.
    
    
    
    
    ```
    $ oc create -f &lt;new_pv_name&gt;.yaml
    ```
    
    


 **Additional resources** 

-  [Configuring local storage for virtual machines](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/virtualization/#virt-configuring-local-storage-for-vms) 
- The OpenShift Container Platform Storage documentation has more information on [Persistent Storage](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html-single/storage/#understanding-persistent-storage) .


### 9.20.20. Expanding a virtual machine disk




You can enlarge the size of a virtual machine’s (VM) disk to provide a greater storage capacity by resizing the disk’s persistent volume claim (PVC).

However, you cannot reduce the size of a VM disk.

#### 9.20.20.1. Enlarging a virtual machine disk




VM disk enlargement makes extra space available to the virtual machine. However, it is the responsibility of the VM owner to decide how to consume the storage.

If the disk is a `Filesystem` PVC, the matching file expands to the remaining size while reserving some space for file system overhead.

 **Procedure** 

1. Edit the `    PersistentVolumeClaim` manifest of the VM disk that you want to expand:
    
    
    ```
    $ oc edit pvc &lt;pvc_name&gt;
    ```
    
    
1. Change the value of `    spec.resource.requests.storage` attribute to a larger size.
    
    
    ```
    apiVersion: v1    kind: PersistentVolumeClaim    metadata:       name: vm-disk-expand    spec:      accessModes:         - ReadWriteMany      resources:        requests:           storage: 3Gi<span id="CO108-1"><!--Empty--></span><span class="callout">1</span>...
    ```
    
    


#### 9.20.20.2. Additional resources




-  [Extending a basic volume in Windows](https://docs.microsoft.com/en-us/windows-server/storage/disk-management/extend-a-basic-volume) .
-  [Extending an existing file system partition without destroying data in Red Hat Enterprise Linux](https://access.redhat.com/solutions/29095) .
-  [Extending a logical volume and its file system online in Red Hat Enterprise Linux](https://access.redhat.com/solutions/24770) .


### 9.20.21. Deleting data volumes




You can manually delete a data volume by using the `oc` command-line interface.    


Note
When you delete a virtual machine, the data volume it uses is automatically deleted.



#### 9.20.21.1. About data volumes




 `DataVolume` objects are custom resources that are provided by the Containerized Data Importer (CDI) project. Data volumes orchestrate import, clone, and upload operations that are associated with an underlying persistent volume claim (PVC). Data volumes are integrated with OpenShift Virtualization, and they prevent a virtual machine from being started before the PVC has been prepared.

#### 9.20.21.2. Listing all data volumes




You can list the data volumes in your cluster by using the `oc` command-line interface.

 **Procedure** 

- List all data volumes by running the following command:
    
    
    ```
    $ oc get dvs
    ```
    
    


#### 9.20.21.3. Deleting a data volume




You can delete a data volume by using the `oc` command-line interface (CLI).

 **Prerequisites** 

- Identify the name of the data volume that you want to delete.


 **Procedure** 

- Delete the data volume by running the following command:
    
    
    ```
    $ oc delete dv &lt;datavolume_name&gt;
    ```
    
    Note
    This command only deletes objects that exist in the current project. Specify the `    -n &lt;project_name&gt;` option if the object you want to delete is in a different project or namespace.
    
    
    
    


# Chapter 10. Virtual machine templates




